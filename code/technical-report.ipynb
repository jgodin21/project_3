{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we were tasked with finding two different subreddits, culling posts from each, and using Natual Language Processing (NLP) to see how well we can classify posts as belonging to one or other subreddit.\n",
    "\n",
    "With the timing of this project coinciding with the 2020 NFL Draft, I selected the subreddits r/cowboys and r/eagles, two rival NFL franchises with dedicated fan bases \\[I myself have been a Cowboys fan since I was six years old.\\]\n",
    "\n",
    "**PRIMARY DATA SCIENCE PROBLEM**\n",
    "\n",
    "Build a machine learning classifier that maximizes the accuracy of predicting r/cowboys posts ***without using obvious keywords/tokens***, but with particular emphasis on maximizing the model's Sensitivity (in other words, trying to minimize the chances of falsely predicting a true Cowboys fan to be an Eagles fan).\n",
    "\n",
    "**SECONDARY DATA SCIENCE PROBLEM**\n",
    "\n",
    "Uncover and diagnose which words are more indicative of one or the other fan bases heading into the draft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to execute this project, I collected 2000 reddit posts using the [Pushshift API](https://github.com/pushshift/api), scraping 1000 posts from the r/cowboys subreddit and 1000 posts from the r/eagles subreddit. Since I wanted to detect r/cowboys posts, I coded each subdataset as either a 1 (r/cowboys posts) or 0 (r/eagles posts), setting up a binary classification problem.\n",
    "\n",
    "Because this project is about Natural Language Processing and not image evaluation, the two fields from the posts that I focused on for analysis were the \"title\" and \"selftext\" fields. Titles are typically like headlines and may contain anything from a few words to a couple of sentences. Selftext fields, when present, are more like paragraphs in their form. For the analysis, I concatenated the title and selftext fields into a single (sometimes lengthy) string. As a reference, about three-quarters of posts only consist of a title.\n",
    "\n",
    "At this point, the two files were merged, shuffled, and randomly split into train and test data files for modeling, with 1400 cases selected for training the machine learning classifier and 600 posts used as a holdout sample to evaluate model performance. Stratification was used to ensure that each of these files contained 50% r/cowboys posts and 50% r/eagles posts.\n",
    "\n",
    "The two datasets were cleaned by removing html and non-letter characters, converting all remaining characters to lowercase, and splitting the strings into \"tokens\", which are essentially any words or characters that are separated by spaces. These tokens were then lemmatized, which reduces multiple forms of the same word into a common base form. Stopwords were then removed in the final step prior to analysis. Stopwords are either words that have been consistently judged to provide little meaning in text analysis, or in this case, words that so obviously point to being from either subreddit as to make classification too easy. I wanted my machine learners to work hard!\n",
    "\n",
    "The analysis stream consisted of running the data through either of two types of vectorizers in order to convert the raw text data into numeric variables suitable for modeling. Count Vectorization simply sums or counts tokens used within each post (so values are integer counts from zero to theoretically infinity), while Tfidf Vectorization converts tokens to relative ratios of how many times a word occurs in a given post relative to the number of times it occurs across all posts in the data set. After vectorization, the data were run through a series of classification models, including:\n",
    "\n",
    "    - Logistic Regression Classifier\n",
    "    - Multinomial Naive Bayes Classifier\n",
    "    - Decision Tree Classifier\n",
    "    - Bagged Decision Tree Classifier\n",
    "    - Random Forests Classifier\n",
    "    - Extra Trees Classifier\n",
    "    - Gradient Boosting Classifier\n",
    "    - Support Vector Machine Classifier\n",
    "    \n",
    "Model performance was judged against the baseline accuracy rate of 50%, as well as an \"unfair\" model where custom stopwords were not removed. Again, I was looking for a model that would be relatively high in accuracy, balance Sensitivity and Specificity (as shown by a high ROC AUC score), but lean towards maximizing Sensitivity.\n",
    "\n",
    "**Results**\n",
    "\n",
    "Somewhat surprisingly, even the \"unfair\" model didn't have an easy time of it, only correctly predicting 80% of the posts (which is still a substantial improvement over the naive 50% accuracy rate baseline). The remaining models achieved accuracy ranging between 56-70% on cross-validated data. The \"winning\" model included Count Vectorized data passed through a Logistic Regression classifier with L1 (LASSO) regularization, resulting in a solution with 67% cross-validated accuracy, 72% Sensitivity, and a ROC AUC score of 72%. These represent decent performance, but not world-class classification levels.\n",
    "\n",
    "The reason for this is at least two-fold. For one thing, both subreddits consist of fans of NFL football, and therefore use a common vocabulary or terminology that may not differ enough for highly-accurate classification. And for another, I did not scrape a huge sample of posts, so some challenges may be a result of the relatively small sample size used for the analysis.\n",
    "\n",
    "Differences in word usage were revealed, however, with strong indicators often being references to various media members who cover one or the other team (such as Justin Melo or Todd Archer). Fans may have been reacting in the form of \"did you see what Todd Archer posted?\", for example. r/cowboys posts were more likely to contain references to other teams (like Tampa, Dolphins, Steelers, San Francisco). Certain vulgarities were more associated with one or the other subreddit as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2 Imports](#Imports)\n",
    "- [3 Get Data](#Get-Data)\n",
    "- [4 Data Cleaning & Preprocessing](#Data-Cleaning)\n",
    "- [5 Modeling](#Modeling)\n",
    "    - [Logistic Regression Classifier](#Logistic-Regression-Classifier)\n",
    "    - [Multinomial Naive Bayes Classifier](#Multinomial-Naive-Bayes-Classifier)\n",
    "    - [Decision Tree Classifier](#Decision-Tree-Classifier)\n",
    "    - [Random Forest/ExtraTrees Classifiers](#Random-Forests-and-Extra-Trees-Classifiers)\n",
    "    - [Boosting Classifiers](#Boosting-Classifiers)\n",
    "    - [Support Vector Machine Classifiers](#Support-Vector-Machine-Classifier)\n",
    "- [6 Conclusions](#Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:28:28.743143Z",
     "start_time": "2020-04-26T00:28:28.736676Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_roc_curve, precision_score, roc_auc_score\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, \\\n",
    "                             GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cowboys Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T17:00:01.802487Z",
     "start_time": "2020-04-17T17:00:01.800543Z"
    }
   },
   "outputs": [],
   "source": [
    "# URL for r/cowboys\n",
    "reddit_url_dal = 'https://api.pushshift.io/reddit/search/submission/?subreddit=cowboys&size=1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T17:00:04.441035Z",
     "start_time": "2020-04-17T17:00:02.343328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pull cowboys data from reddit\n",
    "r_dal = requests.get(reddit_url_dal)\n",
    "# Return json encoded data\n",
    "dal = r_dal.json()\n",
    "# Select data library from json file\n",
    "dal = dal['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:13:22.518169Z",
     "start_time": "2020-04-18T02:13:22.514864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T17:00:04.492721Z",
     "start_time": "2020-04-17T17:00:04.483623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_awardings': [],\n",
       " 'allow_live_comments': False,\n",
       " 'author': 'drewchaiinz',\n",
       " 'author_flair_css_class': 'JNW13',\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_template_id': '713e5116-1baf-11ea-8398-0ed97e74e1df',\n",
       " 'author_flair_text': 'Michael Gallup',\n",
       " 'author_flair_text_color': 'dark',\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_4uvf9n5y',\n",
       " 'author_patreon_flair': False,\n",
       " 'author_premium': False,\n",
       " 'awarders': [],\n",
       " 'can_mod_post': False,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1587105700,\n",
       " 'domain': 'i.redd.it',\n",
       " 'full_link': 'https://www.reddit.com/r/cowboys/comments/g2wo5q/fan_art_i_made_of_dak_zeke_coop_by_yung_drxw_on/',\n",
       " 'gildings': {},\n",
       " 'id': 'g2wo5q',\n",
       " 'is_crosspostable': True,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': True,\n",
       " 'is_robot_indexable': True,\n",
       " 'is_self': False,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_richtext': [],\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'text',\n",
       " 'locked': False,\n",
       " 'media_only': False,\n",
       " 'no_follow': False,\n",
       " 'num_comments': 7,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/cowboys/comments/g2wo5q/fan_art_i_made_of_dak_zeke_coop_by_yung_drxw_on/',\n",
       " 'pinned': False,\n",
       " 'post_hint': 'image',\n",
       " 'preview': {'enabled': True,\n",
       "  'images': [{'id': 'ss52zETHcbX15O4sHBxkEOs_C60MBN2g5c5m_bWlrWg',\n",
       "    'resolutions': [{'height': 101,\n",
       "      'url': 'https://preview.redd.it/owhvxqtkqbt41.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=31b846f3f83d60429c72c1caf9879f6f5c8f93fe',\n",
       "      'width': 108},\n",
       "     {'height': 202,\n",
       "      'url': 'https://preview.redd.it/owhvxqtkqbt41.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=65b47ec3f234b0ab94802ac31bc587739e159100',\n",
       "      'width': 216},\n",
       "     {'height': 299,\n",
       "      'url': 'https://preview.redd.it/owhvxqtkqbt41.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b88abe578b1a7822e9bbf000f44c9182165907f',\n",
       "      'width': 320},\n",
       "     {'height': 599,\n",
       "      'url': 'https://preview.redd.it/owhvxqtkqbt41.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b001510a7ee5fa6070a948fee9a404edddfdc23e',\n",
       "      'width': 640}],\n",
       "    'source': {'height': 637,\n",
       "     'url': 'https://preview.redd.it/owhvxqtkqbt41.jpg?auto=webp&amp;s=b5c4a01be2458306681d89c58099cc65bcb6d487',\n",
       "     'width': 680},\n",
       "    'variants': {}}]},\n",
       " 'pwls': 6,\n",
       " 'retrieved_on': 1587124630,\n",
       " 'score': 18,\n",
       " 'selftext': '',\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'cowboys',\n",
       " 'subreddit_id': 't5_2re7g',\n",
       " 'subreddit_subscribers': 131327,\n",
       " 'subreddit_type': 'public',\n",
       " 'thumbnail': 'image',\n",
       " 'thumbnail_height': 131,\n",
       " 'thumbnail_width': 140,\n",
       " 'title': 'Fan art I made of Dak, Zeke &amp; COOP by @yung__drxw on IG ⚡️ GO BOYS!',\n",
       " 'total_awards_received': 0,\n",
       " 'treatment_tags': [],\n",
       " 'url': 'https://i.redd.it/owhvxqtkqbt41.jpg',\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'wls': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T17:00:07.167199Z",
     "start_time": "2020-04-17T17:00:07.163755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dal[0]['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T17:00:07.897035Z",
     "start_time": "2020-04-17T17:00:07.893220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cowboy Fans Standards Are Higher Than Any Other Team'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dal[999]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T17:27:27.179756Z",
     "start_time": "2020-04-17T17:27:27.176790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The level of play that is needed from our players to deem them “worth it” is ridiculous. I’ve seen people calling Tank, Amari, Dak, and Byron essentially wastes this offseason while they were playing at elite levels in 2019. I really hope the wishes of these fans are met so that they can deal with the suffering of an actually awful team. PAY DAK, AMARI, and BYRON!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dal[999]['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:40:24.660548Z",
     "start_time": "2020-04-18T02:40:24.657632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1587177624.658514\n"
     ]
    }
   ],
   "source": [
    "# Save time just in case\n",
    "ts = time.time()\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:33:12.927656Z",
     "start_time": "2020-04-18T19:33:12.924229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2020, 4, 17, 22, 40, 24, 658514)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.fromtimestamp(1587177624.658514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T19:08:11.368576Z",
     "start_time": "2020-04-17T19:08:11.354003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drewchaiinz</td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StrokinCole</td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_hispanic2</td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youngDCmixa</td>\n",
       "      <td>Skins fan coming in peace</td>\n",
       "      <td>Howdy folks! Not trying to ruffle any feathers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldfishlaboratory</td>\n",
       "      <td>If anyone wants their heart broken...</td>\n",
       "      <td>2016 Divisional Playoff game vs Green Bay is o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>texastiger1025</td>\n",
       "      <td>Will training camp ever move back home to Texas?</td>\n",
       "      <td>Seriously, Oxnard is a tradition at this point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>ElizabethAnnWashingt</td>\n",
       "      <td>So according to 105.3 WFAN the deadline to vot...</td>\n",
       "      <td>for those who may not understand the reason th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>mhunt2929</td>\n",
       "      <td>Happy Birthday 88</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>kidofChrist</td>\n",
       "      <td>Only #2?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>K_Weazy</td>\n",
       "      <td>Cowboy Fans Standards Are Higher Than Any Othe...</td>\n",
       "      <td>The level of play that is needed from our play...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                              title  \\\n",
       "0             drewchaiinz  Fan art I made of Dak, Zeke &amp; COOP by @yun...   \n",
       "1             StrokinCole                     Call me the god of trade downs   \n",
       "2       classic_hispanic2               What do you guys think of this draft   \n",
       "3             youngDCmixa                          Skins fan coming in peace   \n",
       "4      goldfishlaboratory              If anyone wants their heart broken...   \n",
       "..                    ...                                                ...   \n",
       "995        texastiger1025   Will training camp ever move back home to Texas?   \n",
       "996  ElizabethAnnWashingt  So according to 105.3 WFAN the deadline to vot...   \n",
       "997             mhunt2929                                  Happy Birthday 88   \n",
       "998           kidofChrist                                           Only #2?   \n",
       "999               K_Weazy  Cowboy Fans Standards Are Higher Than Any Othe...   \n",
       "\n",
       "                                              selftext  \n",
       "0                                                       \n",
       "1                                                       \n",
       "2                                                       \n",
       "3    Howdy folks! Not trying to ruffle any feathers...  \n",
       "4    2016 Divisional Playoff game vs Green Bay is o...  \n",
       "..                                                 ...  \n",
       "995  Seriously, Oxnard is a tradition at this point...  \n",
       "996  for those who may not understand the reason th...  \n",
       "997                                                     \n",
       "998                                                     \n",
       "999  The level of play that is needed from our play...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe for dallas posts\n",
    "posts = []\n",
    "for row in range(len(dal)):\n",
    "    post = {}\n",
    "    post['author'] = dal[row]['author']\n",
    "    post['title'] = dal[row]['title']\n",
    "    try:\n",
    "        post['selftext'] = dal[row]['selftext']\n",
    "    except:\n",
    "        post['selftext'] = \"\"\n",
    "    \n",
    "\n",
    "    posts.append(post)\n",
    "    \n",
    "df_dal = pd.DataFrame(posts)\n",
    "df_dal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:15:50.002125Z",
     "start_time": "2020-04-18T02:15:49.999151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add dependent variable (1 = cowboys posts, 0 = eagles posts)\n",
    "df_dal[\"subreddit\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:15:50.866680Z",
     "start_time": "2020-04-18T02:15:50.858697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>cowboys</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drewchaiinz</td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StrokinCole</td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_hispanic2</td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youngDCmixa</td>\n",
       "      <td>Skins fan coming in peace</td>\n",
       "      <td>Howdy folks! Not trying to ruffle any feathers...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldfishlaboratory</td>\n",
       "      <td>If anyone wants their heart broken...</td>\n",
       "      <td>2016 Divisional Playoff game vs Green Bay is o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                              title  \\\n",
       "0         drewchaiinz  Fan art I made of Dak, Zeke &amp; COOP by @yun...   \n",
       "1         StrokinCole                     Call me the god of trade downs   \n",
       "2   classic_hispanic2               What do you guys think of this draft   \n",
       "3         youngDCmixa                          Skins fan coming in peace   \n",
       "4  goldfishlaboratory              If anyone wants their heart broken...   \n",
       "\n",
       "                                            selftext  cowboys  subreddit  \n",
       "0                                                           1          1  \n",
       "1                                                           1          1  \n",
       "2                                                           1          1  \n",
       "3  Howdy folks! Not trying to ruffle any feathers...        1          1  \n",
       "4  2016 Divisional Playoff game vs Green Bay is o...        1          1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Eagles Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:17:24.537750Z",
     "start_time": "2020-04-18T02:17:24.535596Z"
    }
   },
   "outputs": [],
   "source": [
    "# URL for r/eagles\n",
    "reddit_url_eagles = 'https://api.pushshift.io/reddit/search/submission/?subreddit=eagles&size=1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:17:27.314451Z",
     "start_time": "2020-04-18T02:17:26.403239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pull eagles data from reddit\n",
    "r_eagles = requests.get(reddit_url_eagles)\n",
    "# Return json encoded data\n",
    "eag = r_eagles.json()\n",
    "# Select data library from json file\n",
    "eag = eag['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:17:38.943238Z",
     "start_time": "2020-04-18T02:17:38.940140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:17:40.531275Z",
     "start_time": "2020-04-18T02:17:40.527707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3 tight end set 2020? Bears release Trey Burton'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eag[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:17:43.811021Z",
     "start_time": "2020-04-18T02:17:43.807975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tough go in Chicago but he’s a fan favourite #phillyspecial\\n\\nhttp://www.nfl.com/news/story/0ap3000001109866/article/roundup-bears-cutting-te-trey-burton-after-2-years'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eag[0]['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:17:47.499114Z",
     "start_time": "2020-04-18T02:17:47.495679Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NFL Game pass is totally free from now until May 31st. What games are people watching?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eag[999]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:17:52.243797Z",
     "start_time": "2020-04-18T02:17:52.240394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m currently enjoying the 37-9 beat down of the cowboys, where Kamu took over kicking duties and Doug kept going for 2. Probably going to check out the 2017 bears game, or the 2017 broncos game, before watching some of rookie Wentz.\\n\\nProbably going to also go back and watch Foles’ 7 TD game. Also watching the Snow Bowl, as that was my girlfriend and I’s first Birds game we watched together, and it will be fun to rewatch it together all these years later.\\n\\nWhat games would people want to rewatch?'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eag[999]['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:26:00.858139Z",
     "start_time": "2020-04-18T02:26:00.844457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MikeCloney</td>\n",
       "      <td>3 tight end set 2020? Bears release Trey Burton</td>\n",
       "      <td>Tough go in Chicago but he’s a fan favourite #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5566y</td>\n",
       "      <td>Source: The #Bears are releasing TE Trey Burto...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Crazy Thought: RB/QB Cam Newton ?</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amatom27</td>\n",
       "      <td>[Schefter] Bears releasing Trey Burton, per so...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[Rapoport] Source: The #Bears are releasing TE...</td>\n",
       "      <td>[deleted]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>LumberjackWeezy</td>\n",
       "      <td>WR market is very low right now. Good time to ...</td>\n",
       "      <td>Historically speaking, rookie WR's don't usual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>BCSinReverse</td>\n",
       "      <td>[Schefter] Former 49ers’ WR Emmanuel Sanders r...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>randomphiladelphian</td>\n",
       "      <td>Slay is still trashing his old team</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>eaglesnation11</td>\n",
       "      <td>Alshon and a 2nd for OBJ?</td>\n",
       "      <td>Apparently OBJ rumors are rampant again. We wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>whubby777</td>\n",
       "      <td>NFL Game pass is totally free from now until M...</td>\n",
       "      <td>I’m currently enjoying the 37-9 beat down of t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author                                              title  \\\n",
       "0             MikeCloney    3 tight end set 2020? Bears release Trey Burton   \n",
       "1                  5566y  Source: The #Bears are releasing TE Trey Burto...   \n",
       "2              [deleted]                  Crazy Thought: RB/QB Cam Newton ?   \n",
       "3               amatom27  [Schefter] Bears releasing Trey Burton, per so...   \n",
       "4              [deleted]  [Rapoport] Source: The #Bears are releasing TE...   \n",
       "..                   ...                                                ...   \n",
       "995      LumberjackWeezy  WR market is very low right now. Good time to ...   \n",
       "996         BCSinReverse  [Schefter] Former 49ers’ WR Emmanuel Sanders r...   \n",
       "997  randomphiladelphian                Slay is still trashing his old team   \n",
       "998       eaglesnation11                          Alshon and a 2nd for OBJ?   \n",
       "999            whubby777  NFL Game pass is totally free from now until M...   \n",
       "\n",
       "                                              selftext  \n",
       "0    Tough go in Chicago but he’s a fan favourite #...  \n",
       "1                                                       \n",
       "2                                            [deleted]  \n",
       "3                                                       \n",
       "4                                            [deleted]  \n",
       "..                                                 ...  \n",
       "995  Historically speaking, rookie WR's don't usual...  \n",
       "996                                                     \n",
       "997                                          [removed]  \n",
       "998  Apparently OBJ rumors are rampant again. We wo...  \n",
       "999  I’m currently enjoying the 37-9 beat down of t...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe for eagles posts\n",
    "posts = []\n",
    "for row in range(len(eag)):\n",
    "    post = {}\n",
    "    post['author'] = eag[row]['author']\n",
    "    post['title'] = eag[row]['title']\n",
    "    try:\n",
    "        post['selftext'] = eag[row]['selftext']\n",
    "    except:\n",
    "        post['selftext'] = \"\"\n",
    "    \n",
    "\n",
    "    posts.append(post)\n",
    "    \n",
    "df_eag = pd.DataFrame(posts)\n",
    "df_eag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:27:14.218629Z",
     "start_time": "2020-04-18T02:27:14.215725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add dependent variable (1 = cowboys posts, 0 = eagles posts)\n",
    "df_eag[\"subreddit\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:27:24.059461Z",
     "start_time": "2020-04-18T02:27:24.051804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MikeCloney</td>\n",
       "      <td>3 tight end set 2020? Bears release Trey Burton</td>\n",
       "      <td>Tough go in Chicago but he’s a fan favourite #...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5566y</td>\n",
       "      <td>Source: The #Bears are releasing TE Trey Burto...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Crazy Thought: RB/QB Cam Newton ?</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amatom27</td>\n",
       "      <td>[Schefter] Bears releasing Trey Burton, per so...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>[Rapoport] Source: The #Bears are releasing TE...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author                                              title  \\\n",
       "0  MikeCloney    3 tight end set 2020? Bears release Trey Burton   \n",
       "1       5566y  Source: The #Bears are releasing TE Trey Burto...   \n",
       "2   [deleted]                  Crazy Thought: RB/QB Cam Newton ?   \n",
       "3    amatom27  [Schefter] Bears releasing Trey Burton, per so...   \n",
       "4   [deleted]  [Rapoport] Source: The #Bears are releasing TE...   \n",
       "\n",
       "                                            selftext  subreddit  \n",
       "0  Tough go in Chicago but he’s a fan favourite #...          0  \n",
       "1                                                             0  \n",
       "2                                          [deleted]          0  \n",
       "3                                                             0  \n",
       "4                                          [deleted]          0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Cowboys and Eagles data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:30:02.257970Z",
     "start_time": "2020-04-18T02:30:02.248161Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_dal.merge(df_eag, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:30:08.168806Z",
     "start_time": "2020-04-18T02:30:08.161685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drewchaiinz</td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StrokinCole</td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_hispanic2</td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youngDCmixa</td>\n",
       "      <td>Skins fan coming in peace</td>\n",
       "      <td>Howdy folks! Not trying to ruffle any feathers...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldfishlaboratory</td>\n",
       "      <td>If anyone wants their heart broken...</td>\n",
       "      <td>2016 Divisional Playoff game vs Green Bay is o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                              title  \\\n",
       "0         drewchaiinz  Fan art I made of Dak, Zeke &amp; COOP by @yun...   \n",
       "1         StrokinCole                     Call me the god of trade downs   \n",
       "2   classic_hispanic2               What do you guys think of this draft   \n",
       "3         youngDCmixa                          Skins fan coming in peace   \n",
       "4  goldfishlaboratory              If anyone wants their heart broken...   \n",
       "\n",
       "                                            selftext  subreddit  \n",
       "0                                                             1  \n",
       "1                                                             1  \n",
       "2                                                             1  \n",
       "3  Howdy folks! Not trying to ruffle any feathers...          1  \n",
       "4  2016 Divisional Playoff game vs Green Bay is o...          1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:30:17.564313Z",
     "start_time": "2020-04-18T02:30:17.557539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>LumberjackWeezy</td>\n",
       "      <td>WR market is very low right now. Good time to ...</td>\n",
       "      <td>Historically speaking, rookie WR's don't usual...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>BCSinReverse</td>\n",
       "      <td>[Schefter] Former 49ers’ WR Emmanuel Sanders r...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>randomphiladelphian</td>\n",
       "      <td>Slay is still trashing his old team</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>eaglesnation11</td>\n",
       "      <td>Alshon and a 2nd for OBJ?</td>\n",
       "      <td>Apparently OBJ rumors are rampant again. We wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>whubby777</td>\n",
       "      <td>NFL Game pass is totally free from now until M...</td>\n",
       "      <td>I’m currently enjoying the 37-9 beat down of t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                              title  \\\n",
       "1995      LumberjackWeezy  WR market is very low right now. Good time to ...   \n",
       "1996         BCSinReverse  [Schefter] Former 49ers’ WR Emmanuel Sanders r...   \n",
       "1997  randomphiladelphian                Slay is still trashing his old team   \n",
       "1998       eaglesnation11                          Alshon and a 2nd for OBJ?   \n",
       "1999            whubby777  NFL Game pass is totally free from now until M...   \n",
       "\n",
       "                                               selftext  subreddit  \n",
       "1995  Historically speaking, rookie WR's don't usual...          0  \n",
       "1996                                                             0  \n",
       "1997                                          [removed]          0  \n",
       "1998  Apparently OBJ rumors are rampant again. We wo...          0  \n",
       "1999  I’m currently enjoying the 37-9 beat down of t...          0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:30:32.633051Z",
     "start_time": "2020-04-18T02:30:32.629070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author       object\n",
       "title        object\n",
       "selftext     object\n",
       "subreddit     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:30:52.844493Z",
     "start_time": "2020-04-18T02:30:52.839477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1000\n",
       "0    1000\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:33:26.592203Z",
     "start_time": "2020-04-18T02:33:26.588984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace [removed] and [deleted] with empty strings in selftext\n",
    "df[\"selftext\"].replace([\"[removed]\", \"[deleted]\"], \"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:35:06.500279Z",
     "start_time": "2020-04-18T02:35:06.293461Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate title and selftext to create final independent variable input\n",
    "df[\"title_selftext\"] = df[\"title\"] + \" \" + df[\"selftext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:35:12.224591Z",
     "start_time": "2020-04-18T02:35:12.216130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drewchaiinz</td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StrokinCole</td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_hispanic2</td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youngDCmixa</td>\n",
       "      <td>Skins fan coming in peace</td>\n",
       "      <td>Howdy folks! Not trying to ruffle any feathers...</td>\n",
       "      <td>1</td>\n",
       "      <td>Skins fan coming in peace Howdy folks! Not try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldfishlaboratory</td>\n",
       "      <td>If anyone wants their heart broken...</td>\n",
       "      <td>2016 Divisional Playoff game vs Green Bay is o...</td>\n",
       "      <td>1</td>\n",
       "      <td>If anyone wants their heart broken... 2016 Div...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                              title  \\\n",
       "0         drewchaiinz  Fan art I made of Dak, Zeke &amp; COOP by @yun...   \n",
       "1         StrokinCole                     Call me the god of trade downs   \n",
       "2   classic_hispanic2               What do you guys think of this draft   \n",
       "3         youngDCmixa                          Skins fan coming in peace   \n",
       "4  goldfishlaboratory              If anyone wants their heart broken...   \n",
       "\n",
       "                                            selftext  subreddit  \\\n",
       "0                                                             1   \n",
       "1                                                             1   \n",
       "2                                                             1   \n",
       "3  Howdy folks! Not trying to ruffle any feathers...          1   \n",
       "4  2016 Divisional Playoff game vs Green Bay is o...          1   \n",
       "\n",
       "                                      title_selftext  \n",
       "0  Fan art I made of Dak, Zeke &amp; COOP by @yun...  \n",
       "1                    Call me the god of trade downs   \n",
       "2              What do you guys think of this draft   \n",
       "3  Skins fan coming in peace Howdy folks! Not try...  \n",
       "4  If anyone wants their heart broken... 2016 Div...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:37:19.640926Z",
     "start_time": "2020-04-18T02:37:19.633048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>LumberjackWeezy</td>\n",
       "      <td>WR market is very low right now. Good time to ...</td>\n",
       "      <td>Historically speaking, rookie WR's don't usual...</td>\n",
       "      <td>0</td>\n",
       "      <td>WR market is very low right now. Good time to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>BCSinReverse</td>\n",
       "      <td>[Schefter] Former 49ers’ WR Emmanuel Sanders r...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[Schefter] Former 49ers’ WR Emmanuel Sanders r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>randomphiladelphian</td>\n",
       "      <td>Slay is still trashing his old team</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Slay is still trashing his old team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>eaglesnation11</td>\n",
       "      <td>Alshon and a 2nd for OBJ?</td>\n",
       "      <td>Apparently OBJ rumors are rampant again. We wo...</td>\n",
       "      <td>0</td>\n",
       "      <td>Alshon and a 2nd for OBJ? Apparently OBJ rumor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>whubby777</td>\n",
       "      <td>NFL Game pass is totally free from now until M...</td>\n",
       "      <td>I’m currently enjoying the 37-9 beat down of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>NFL Game pass is totally free from now until M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                              title  \\\n",
       "1995      LumberjackWeezy  WR market is very low right now. Good time to ...   \n",
       "1996         BCSinReverse  [Schefter] Former 49ers’ WR Emmanuel Sanders r...   \n",
       "1997  randomphiladelphian                Slay is still trashing his old team   \n",
       "1998       eaglesnation11                          Alshon and a 2nd for OBJ?   \n",
       "1999            whubby777  NFL Game pass is totally free from now until M...   \n",
       "\n",
       "                                               selftext  subreddit  \\\n",
       "1995  Historically speaking, rookie WR's don't usual...          0   \n",
       "1996                                                             0   \n",
       "1997                                                             0   \n",
       "1998  Apparently OBJ rumors are rampant again. We wo...          0   \n",
       "1999  I’m currently enjoying the 37-9 beat down of t...          0   \n",
       "\n",
       "                                         title_selftext  \n",
       "1995  WR market is very low right now. Good time to ...  \n",
       "1996  [Schefter] Former 49ers’ WR Emmanuel Sanders r...  \n",
       "1997               Slay is still trashing his old team   \n",
       "1998  Alshon and a 2nd for OBJ? Apparently OBJ rumor...  \n",
       "1999  NFL Game pass is totally free from now until M...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:53:00.006310Z",
     "start_time": "2020-04-18T02:53:00.002504Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[[\"author\", \"title\", \"selftext\", \"title_selftext\", \"subreddit\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:53:09.018002Z",
     "start_time": "2020-04-18T02:53:09.010026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drewchaiinz</td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "      <td></td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StrokinCole</td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "      <td></td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_hispanic2</td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "      <td></td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youngDCmixa</td>\n",
       "      <td>Skins fan coming in peace</td>\n",
       "      <td>Howdy folks! Not trying to ruffle any feathers...</td>\n",
       "      <td>Skins fan coming in peace Howdy folks! Not try...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldfishlaboratory</td>\n",
       "      <td>If anyone wants their heart broken...</td>\n",
       "      <td>2016 Divisional Playoff game vs Green Bay is o...</td>\n",
       "      <td>If anyone wants their heart broken... 2016 Div...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                              title  \\\n",
       "0         drewchaiinz  Fan art I made of Dak, Zeke &amp; COOP by @yun...   \n",
       "1         StrokinCole                     Call me the god of trade downs   \n",
       "2   classic_hispanic2               What do you guys think of this draft   \n",
       "3         youngDCmixa                          Skins fan coming in peace   \n",
       "4  goldfishlaboratory              If anyone wants their heart broken...   \n",
       "\n",
       "                                            selftext  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  Howdy folks! Not trying to ruffle any feathers...   \n",
       "4  2016 Divisional Playoff game vs Green Bay is o...   \n",
       "\n",
       "                                      title_selftext  subreddit  \n",
       "0  Fan art I made of Dak, Zeke &amp; COOP by @yun...          1  \n",
       "1                    Call me the god of trade downs           1  \n",
       "2              What do you guys think of this draft           1  \n",
       "3  Skins fan coming in peace Howdy folks! Not try...          1  \n",
       "4  If anyone wants their heart broken... 2016 Div...          1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:53:14.510545Z",
     "start_time": "2020-04-18T02:53:14.482317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save raw data to csv\n",
    "df.to_csv(\"../data/raw_data_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T02:53:53.136419Z",
     "start_time": "2020-04-18T02:53:53.114384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title_selftext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drewchaiinz</td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fan art I made of Dak, Zeke &amp;amp; COOP by @yun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StrokinCole</td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Call me the god of trade downs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>classic_hispanic2</td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What do you guys think of this draft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>youngDCmixa</td>\n",
       "      <td>Skins fan coming in peace</td>\n",
       "      <td>Howdy folks! Not trying to ruffle any feathers...</td>\n",
       "      <td>Skins fan coming in peace Howdy folks! Not try...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldfishlaboratory</td>\n",
       "      <td>If anyone wants their heart broken...</td>\n",
       "      <td>2016 Divisional Playoff game vs Green Bay is o...</td>\n",
       "      <td>If anyone wants their heart broken... 2016 Div...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>LumberjackWeezy</td>\n",
       "      <td>WR market is very low right now. Good time to ...</td>\n",
       "      <td>Historically speaking, rookie WR's don't usual...</td>\n",
       "      <td>WR market is very low right now. Good time to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>BCSinReverse</td>\n",
       "      <td>[Schefter] Former 49ers’ WR Emmanuel Sanders r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Schefter] Former 49ers’ WR Emmanuel Sanders r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>randomphiladelphian</td>\n",
       "      <td>Slay is still trashing his old team</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Slay is still trashing his old team</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>eaglesnation11</td>\n",
       "      <td>Alshon and a 2nd for OBJ?</td>\n",
       "      <td>Apparently OBJ rumors are rampant again. We wo...</td>\n",
       "      <td>Alshon and a 2nd for OBJ? Apparently OBJ rumor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>whubby777</td>\n",
       "      <td>NFL Game pass is totally free from now until M...</td>\n",
       "      <td>I’m currently enjoying the 37-9 beat down of t...</td>\n",
       "      <td>NFL Game pass is totally free from now until M...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                              title  \\\n",
       "0             drewchaiinz  Fan art I made of Dak, Zeke &amp; COOP by @yun...   \n",
       "1             StrokinCole                     Call me the god of trade downs   \n",
       "2       classic_hispanic2               What do you guys think of this draft   \n",
       "3             youngDCmixa                          Skins fan coming in peace   \n",
       "4      goldfishlaboratory              If anyone wants their heart broken...   \n",
       "...                   ...                                                ...   \n",
       "1995      LumberjackWeezy  WR market is very low right now. Good time to ...   \n",
       "1996         BCSinReverse  [Schefter] Former 49ers’ WR Emmanuel Sanders r...   \n",
       "1997  randomphiladelphian                Slay is still trashing his old team   \n",
       "1998       eaglesnation11                          Alshon and a 2nd for OBJ?   \n",
       "1999            whubby777  NFL Game pass is totally free from now until M...   \n",
       "\n",
       "                                               selftext  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3     Howdy folks! Not trying to ruffle any feathers...   \n",
       "4     2016 Divisional Playoff game vs Green Bay is o...   \n",
       "...                                                 ...   \n",
       "1995  Historically speaking, rookie WR's don't usual...   \n",
       "1996                                                NaN   \n",
       "1997                                                NaN   \n",
       "1998  Apparently OBJ rumors are rampant again. We wo...   \n",
       "1999  I’m currently enjoying the 37-9 beat down of t...   \n",
       "\n",
       "                                         title_selftext  subreddit  \n",
       "0     Fan art I made of Dak, Zeke &amp; COOP by @yun...          1  \n",
       "1                       Call me the god of trade downs           1  \n",
       "2                 What do you guys think of this draft           1  \n",
       "3     Skins fan coming in peace Howdy folks! Not try...          1  \n",
       "4     If anyone wants their heart broken... 2016 Div...          1  \n",
       "...                                                 ...        ...  \n",
       "1995  WR market is very low right now. Good time to ...          0  \n",
       "1996  [Schefter] Former 49ers’ WR Emmanuel Sanders r...          0  \n",
       "1997               Slay is still trashing his old team           0  \n",
       "1998  Alshon and a 2nd for OBJ? Apparently OBJ rumor...          0  \n",
       "1999  NFL Game pass is totally free from now until M...          0  \n",
       "\n",
       "[2000 rows x 5 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in file to make sure it was saved correctly\n",
    "pd.read_csv(\"../data/raw_data_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "***Note that almost all of the code in Section 3 borrows heavily from Matt Brems's NLP lecture code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:29:24.748674Z",
     "start_time": "2020-04-26T00:29:24.730009Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw_data_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:29:25.367832Z",
     "start_time": "2020-04-26T00:29:25.356788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author               0\n",
       "title                0\n",
       "selftext          1530\n",
       "title_selftext       0\n",
       "subreddit            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:29:27.998214Z",
     "start_time": "2020-04-26T00:29:27.994727Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'title', 'selftext', 'title_selftext', 'subreddit'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:29:31.403909Z",
     "start_time": "2020-04-26T00:29:31.396315Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['author', 'title_selftext']],\n",
    "                                                    df['subreddit'],\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42, \n",
    "                                                    stratify=df['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:29:33.706929Z",
     "start_time": "2020-04-26T00:29:33.698171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title_selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>Time-Ambassador</td>\n",
       "      <td>What about Greg Ward? This sub has been adaman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>Conety</td>\n",
       "      <td>And this is why we didn’t pay him. Asking wayy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Badlands32</td>\n",
       "      <td>Defense is suddenly bottom of the league in a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>PersonBehindAScreen</td>\n",
       "      <td>[Rodriguez] DeMarcus Lawrence Biggest Winner f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>teo1315</td>\n",
       "      <td>Cowboys need TE help, would you prefer the tea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author                                     title_selftext\n",
       "1593      Time-Ambassador  What about Greg Ward? This sub has been adaman...\n",
       "785                Conety  And this is why we didn’t pay him. Asking wayy...\n",
       "729            Badlands32  Defense is suddenly bottom of the league in a ...\n",
       "403   PersonBehindAScreen  [Rodriguez] DeMarcus Lawrence Biggest Winner f...\n",
       "890               teo1315  Cowboys need TE help, would you prefer the tea..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T17:08:09.829415Z",
     "start_time": "2020-04-21T17:08:09.821752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    700\n",
       "0    700\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T17:08:11.930324Z",
     "start_time": "2020-04-21T17:08:11.925965Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    300\n",
       "0    300\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove HTML code artifacts\n",
    "There actually might not be any in here, but better safe than sorry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T01:08:36.943156Z",
     "start_time": "2020-04-20T01:08:36.936144Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014 Demarco Murray So because gamepass is now free, I’ve been rewatching a lot of 2014 highlights just because of how great and enjoyable that season was and I have to say.....man was Demarco a beast that year. I think people on this sub forget him too much at times because of the injury issues and how soon Zeke came after him, but I’ll take his 2014 season (minus all those fumbles) up against any season Zeke has had thus far in his career (although I’d still take Zeke as a player because of the consistency).\\n\\nAlso, that offense looked damn near unstoppable at times. Everyone was at their peak from romo to the offensive line to dez, Murray, Beasley and the other skill guys (even Dunbar had a solid role on that team on third downs). Hell, even the body catcher was good as a #2 WR. \\n\\nDamn, I miss that team. If only the defense was a bit better at that point in time (or the refs could call a catch correctly). We would’ve gone to Seattle and beaten them again.\\n\\nWould we have beaten the Pats? Probably not because our defense couldn’t stop Brady, but damn I would’ve loved to see Romo and that offense go up against Bellichek.'"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"title_selftext\"][515]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T01:08:38.795259Z",
     "start_time": "2020-04-20T01:08:38.790812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014 Demarco Murray So because gamepass is now free, I’ve been rewatching a lot of 2014 highlights just because of how great and enjoyable that season was and I have to say.....man was Demarco a beast that year. I think people on this sub forget him too much at times because of the injury issues and how soon Zeke came after him, but I’ll take his 2014 season (minus all those fumbles) up against any season Zeke has had thus far in his career (although I’d still take Zeke as a player because of the consistency).\n",
      "\n",
      "Also, that offense looked damn near unstoppable at times. Everyone was at their peak from romo to the offensive line to dez, Murray, Beasley and the other skill guys (even Dunbar had a solid role on that team on third downs). Hell, even the body catcher was good as a #2 WR. \n",
      "\n",
      "Damn, I miss that team. If only the defense was a bit better at that point in time (or the refs could call a catch correctly). We would’ve gone to Seattle and beaten them again.\n",
      "\n",
      "Would we have beaten the Pats? Probably not because our defense couldn’t stop Brady, but damn I would’ve loved to see Romo and that offense go up against Bellichek.\n",
      "\n",
      "2014 Demarco Murray So because gamepass is now free, I’ve been rewatching a lot of 2014 highlights just because of how great and enjoyable that season was and I have to say.....man was Demarco a beast that year. I think people on this sub forget him too much at times because of the injury issues and how soon Zeke came after him, but I’ll take his 2014 season (minus all those fumbles) up against any season Zeke has had thus far in his career (although I’d still take Zeke as a player because of the consistency).\n",
      "\n",
      "Also, that offense looked damn near unstoppable at times. Everyone was at their peak from romo to the offensive line to dez, Murray, Beasley and the other skill guys (even Dunbar had a solid role on that team on third downs). Hell, even the body catcher was good as a #2 WR. \n",
      "\n",
      "Damn, I miss that team. If only the defense was a bit better at that point in time (or the refs could call a catch correctly). We would’ve gone to Seattle and beaten them again.\n",
      "\n",
      "Would we have beaten the Pats? Probably not because our defense couldn’t stop Brady, but damn I would’ve loved to see Romo and that offense go up against Bellichek.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BeautifulSoup object on a single reddit entry     \n",
    "example1 = BeautifulSoup(X_train['title_selftext'][515])\n",
    "\n",
    "# Print the raw reddit post and then the output of get_text(), for \n",
    "# comparison\n",
    "print(X_train['title_selftext'][515])\n",
    "print()\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-letters (non-numbers?)\n",
    "May need to reconsider as player numbers may come in to play here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T01:12:47.188350Z",
     "start_time": "2020-04-20T01:12:47.185251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use regular expressions to do a find-and-replace\n",
    "# In this case, the # symbol may be important to keep because #2 WR is a thing in football\n",
    "okchars_only = re.sub(\"[^a-zA-Z0-9\\#]\",           # The pattern to search for - keeps letters, numbers and #\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      example1.get_text())   # The text to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T01:12:48.375551Z",
     "start_time": "2020-04-20T01:12:48.372285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014 Demarco Murray So because gamepass is now free  I ve been rewatching a lot of 2014 highlights just because of how great and enjoyable that season was and I have to say     man was Demarco a beast that year  I think people on this sub forget him too much at times because of the injury issues and how soon Zeke came after him  but I ll take his 2014 season  minus all those fumbles  up against any season Zeke has had thus far in his career  although I d still take Zeke as a player because of the consistency    Also  that offense looked damn near unstoppable at times  Everyone was at their peak from romo to the offensive line to dez  Murray  Beasley and the other skill guys  even Dunbar had a solid role on that team on third downs   Hell  even the body catcher was good as a #2 WR    Damn  I miss that team  If only the defense was a bit better at that point in time  or the refs could call a catch correctly   We would ve gone to Seattle and beaten them again   Would we have beaten the Pats  Probably not because our defense couldn t stop Brady  but damn I would ve loved to see Romo and that offense go up against Bellichek '"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okchars_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T18:01:50.420392Z",
     "start_time": "2020-04-18T18:01:50.417626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert letters_only to lower case.\n",
    "lower_case = okchars_only.lower()\n",
    "\n",
    "# Split lower_case up at each space.\n",
    "words = lower_case.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T18:01:56.913449Z",
     "start_time": "2020-04-18T18:01:56.910356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014 demarco murray so because gamepass is now free  i ve been rewatching a lot of 2014 highlights just because of how great and enjoyable that season was and i have to say     man was demarco a beast that year  i think people on this sub forget him too much at times because of the injury issues and how soon zeke came after him  but i ll take his 2014 season  minus all those fumbles  up against any season zeke has had thus far in his career  although i d still take zeke as a player because of the consistency    also  that offense looked damn near unstoppable at times  everyone was at their peak from romo to the offensive line to dez  murray  beasley and the other skill guys  even dunbar had a solid role on that team on third downs   hell  even the body catcher was good as a #2 wr    damn  i miss that team  if only the defense was a bit better at that point in time  or the refs could call a catch correctly   we would ve gone to seattle and beaten them again   would we have beaten the pats  probably not because our defense couldn t stop brady  but damn i would ve loved to see romo and that offense go up against bellichek '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T18:02:10.677651Z",
     "start_time": "2020-04-18T18:02:10.671173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014',\n",
       " 'demarco',\n",
       " 'murray',\n",
       " 'so',\n",
       " 'because',\n",
       " 'gamepass',\n",
       " 'is',\n",
       " 'now',\n",
       " 'free',\n",
       " 'i',\n",
       " 've',\n",
       " 'been',\n",
       " 'rewatching',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " '2014',\n",
       " 'highlights',\n",
       " 'just',\n",
       " 'because',\n",
       " 'of',\n",
       " 'how',\n",
       " 'great',\n",
       " 'and',\n",
       " 'enjoyable',\n",
       " 'that',\n",
       " 'season',\n",
       " 'was',\n",
       " 'and',\n",
       " 'i',\n",
       " 'have',\n",
       " 'to',\n",
       " 'say',\n",
       " 'man',\n",
       " 'was',\n",
       " 'demarco',\n",
       " 'a',\n",
       " 'beast',\n",
       " 'that',\n",
       " 'year',\n",
       " 'i',\n",
       " 'think',\n",
       " 'people',\n",
       " 'on',\n",
       " 'this',\n",
       " 'sub',\n",
       " 'forget',\n",
       " 'him',\n",
       " 'too',\n",
       " 'much',\n",
       " 'at',\n",
       " 'times',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'injury',\n",
       " 'issues',\n",
       " 'and',\n",
       " 'how',\n",
       " 'soon',\n",
       " 'zeke',\n",
       " 'came',\n",
       " 'after',\n",
       " 'him',\n",
       " 'but',\n",
       " 'i',\n",
       " 'll',\n",
       " 'take',\n",
       " 'his',\n",
       " '2014',\n",
       " 'season',\n",
       " 'minus',\n",
       " 'all',\n",
       " 'those',\n",
       " 'fumbles',\n",
       " 'up',\n",
       " 'against',\n",
       " 'any',\n",
       " 'season',\n",
       " 'zeke',\n",
       " 'has',\n",
       " 'had',\n",
       " 'thus',\n",
       " 'far',\n",
       " 'in',\n",
       " 'his',\n",
       " 'career',\n",
       " 'although',\n",
       " 'i',\n",
       " 'd',\n",
       " 'still',\n",
       " 'take',\n",
       " 'zeke',\n",
       " 'as',\n",
       " 'a',\n",
       " 'player',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'consistency',\n",
       " 'also',\n",
       " 'that',\n",
       " 'offense',\n",
       " 'looked',\n",
       " 'damn',\n",
       " 'near',\n",
       " 'unstoppable',\n",
       " 'at',\n",
       " 'times',\n",
       " 'everyone',\n",
       " 'was',\n",
       " 'at',\n",
       " 'their',\n",
       " 'peak',\n",
       " 'from',\n",
       " 'romo',\n",
       " 'to',\n",
       " 'the',\n",
       " 'offensive',\n",
       " 'line',\n",
       " 'to',\n",
       " 'dez',\n",
       " 'murray',\n",
       " 'beasley',\n",
       " 'and',\n",
       " 'the',\n",
       " 'other',\n",
       " 'skill',\n",
       " 'guys',\n",
       " 'even',\n",
       " 'dunbar',\n",
       " 'had',\n",
       " 'a',\n",
       " 'solid',\n",
       " 'role',\n",
       " 'on',\n",
       " 'that',\n",
       " 'team',\n",
       " 'on',\n",
       " 'third',\n",
       " 'downs',\n",
       " 'hell',\n",
       " 'even',\n",
       " 'the',\n",
       " 'body',\n",
       " 'catcher',\n",
       " 'was',\n",
       " 'good',\n",
       " 'as',\n",
       " 'a',\n",
       " '#2',\n",
       " 'wr',\n",
       " 'damn',\n",
       " 'i',\n",
       " 'miss',\n",
       " 'that',\n",
       " 'team',\n",
       " 'if',\n",
       " 'only',\n",
       " 'the',\n",
       " 'defense',\n",
       " 'was',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'better',\n",
       " 'at',\n",
       " 'that',\n",
       " 'point',\n",
       " 'in',\n",
       " 'time',\n",
       " 'or',\n",
       " 'the',\n",
       " 'refs',\n",
       " 'could',\n",
       " 'call',\n",
       " 'a',\n",
       " 'catch',\n",
       " 'correctly',\n",
       " 'we',\n",
       " 'would',\n",
       " 've',\n",
       " 'gone',\n",
       " 'to',\n",
       " 'seattle',\n",
       " 'and',\n",
       " 'beaten',\n",
       " 'them',\n",
       " 'again',\n",
       " 'would',\n",
       " 'we',\n",
       " 'have',\n",
       " 'beaten',\n",
       " 'the',\n",
       " 'pats',\n",
       " 'probably',\n",
       " 'not',\n",
       " 'because',\n",
       " 'our',\n",
       " 'defense',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'stop',\n",
       " 'brady',\n",
       " 'but',\n",
       " 'damn',\n",
       " 'i',\n",
       " 'would',\n",
       " 've',\n",
       " 'loved',\n",
       " 'to',\n",
       " 'see',\n",
       " 'romo',\n",
       " 'and',\n",
       " 'that',\n",
       " 'offense',\n",
       " 'go',\n",
       " 'up',\n",
       " 'against',\n",
       " 'bellichek']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T18:06:52.236723Z",
     "start_time": "2020-04-18T18:06:50.915687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014', '2014'),\n",
       " ('demarco', 'demarco'),\n",
       " ('murray', 'murray'),\n",
       " ('so', 'so'),\n",
       " ('because', 'because'),\n",
       " ('gamepass', 'gamepass'),\n",
       " ('is', 'is'),\n",
       " ('now', 'now'),\n",
       " ('free', 'free'),\n",
       " ('i', 'i'),\n",
       " ('ve', 've'),\n",
       " ('been', 'been'),\n",
       " ('rewatching', 'rewatching'),\n",
       " ('a', 'a'),\n",
       " ('lot', 'lot'),\n",
       " ('of', 'of'),\n",
       " ('2014', '2014'),\n",
       " ('highlights', 'highlight'),\n",
       " ('just', 'just'),\n",
       " ('because', 'because'),\n",
       " ('of', 'of'),\n",
       " ('how', 'how'),\n",
       " ('great', 'great'),\n",
       " ('and', 'and'),\n",
       " ('enjoyable', 'enjoyable'),\n",
       " ('that', 'that'),\n",
       " ('season', 'season'),\n",
       " ('was', 'wa'),\n",
       " ('and', 'and'),\n",
       " ('i', 'i'),\n",
       " ('have', 'have'),\n",
       " ('to', 'to'),\n",
       " ('say', 'say'),\n",
       " ('man', 'man'),\n",
       " ('was', 'wa'),\n",
       " ('demarco', 'demarco'),\n",
       " ('a', 'a'),\n",
       " ('beast', 'beast'),\n",
       " ('that', 'that'),\n",
       " ('year', 'year'),\n",
       " ('i', 'i'),\n",
       " ('think', 'think'),\n",
       " ('people', 'people'),\n",
       " ('on', 'on'),\n",
       " ('this', 'this'),\n",
       " ('sub', 'sub'),\n",
       " ('forget', 'forget'),\n",
       " ('him', 'him'),\n",
       " ('too', 'too'),\n",
       " ('much', 'much'),\n",
       " ('at', 'at'),\n",
       " ('times', 'time'),\n",
       " ('because', 'because'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('injury', 'injury'),\n",
       " ('issues', 'issue'),\n",
       " ('and', 'and'),\n",
       " ('how', 'how'),\n",
       " ('soon', 'soon'),\n",
       " ('zeke', 'zeke'),\n",
       " ('came', 'came'),\n",
       " ('after', 'after'),\n",
       " ('him', 'him'),\n",
       " ('but', 'but'),\n",
       " ('i', 'i'),\n",
       " ('ll', 'll'),\n",
       " ('take', 'take'),\n",
       " ('his', 'his'),\n",
       " ('2014', '2014'),\n",
       " ('season', 'season'),\n",
       " ('minus', 'minus'),\n",
       " ('all', 'all'),\n",
       " ('those', 'those'),\n",
       " ('fumbles', 'fumble'),\n",
       " ('up', 'up'),\n",
       " ('against', 'against'),\n",
       " ('any', 'any'),\n",
       " ('season', 'season'),\n",
       " ('zeke', 'zeke'),\n",
       " ('has', 'ha'),\n",
       " ('had', 'had'),\n",
       " ('thus', 'thus'),\n",
       " ('far', 'far'),\n",
       " ('in', 'in'),\n",
       " ('his', 'his'),\n",
       " ('career', 'career'),\n",
       " ('although', 'although'),\n",
       " ('i', 'i'),\n",
       " ('d', 'd'),\n",
       " ('still', 'still'),\n",
       " ('take', 'take'),\n",
       " ('zeke', 'zeke'),\n",
       " ('as', 'a'),\n",
       " ('a', 'a'),\n",
       " ('player', 'player'),\n",
       " ('because', 'because'),\n",
       " ('of', 'of'),\n",
       " ('the', 'the'),\n",
       " ('consistency', 'consistency'),\n",
       " ('also', 'also'),\n",
       " ('that', 'that'),\n",
       " ('offense', 'offense'),\n",
       " ('looked', 'looked'),\n",
       " ('damn', 'damn'),\n",
       " ('near', 'near'),\n",
       " ('unstoppable', 'unstoppable'),\n",
       " ('at', 'at'),\n",
       " ('times', 'time'),\n",
       " ('everyone', 'everyone'),\n",
       " ('was', 'wa'),\n",
       " ('at', 'at'),\n",
       " ('their', 'their'),\n",
       " ('peak', 'peak'),\n",
       " ('from', 'from'),\n",
       " ('romo', 'romo'),\n",
       " ('to', 'to'),\n",
       " ('the', 'the'),\n",
       " ('offensive', 'offensive'),\n",
       " ('line', 'line'),\n",
       " ('to', 'to'),\n",
       " ('dez', 'dez'),\n",
       " ('murray', 'murray'),\n",
       " ('beasley', 'beasley'),\n",
       " ('and', 'and'),\n",
       " ('the', 'the'),\n",
       " ('other', 'other'),\n",
       " ('skill', 'skill'),\n",
       " ('guys', 'guy'),\n",
       " ('even', 'even'),\n",
       " ('dunbar', 'dunbar'),\n",
       " ('had', 'had'),\n",
       " ('a', 'a'),\n",
       " ('solid', 'solid'),\n",
       " ('role', 'role'),\n",
       " ('on', 'on'),\n",
       " ('that', 'that'),\n",
       " ('team', 'team'),\n",
       " ('on', 'on'),\n",
       " ('third', 'third'),\n",
       " ('downs', 'down'),\n",
       " ('hell', 'hell'),\n",
       " ('even', 'even'),\n",
       " ('the', 'the'),\n",
       " ('body', 'body'),\n",
       " ('catcher', 'catcher'),\n",
       " ('was', 'wa'),\n",
       " ('good', 'good'),\n",
       " ('as', 'a'),\n",
       " ('a', 'a'),\n",
       " ('#2', '#2'),\n",
       " ('wr', 'wr'),\n",
       " ('damn', 'damn'),\n",
       " ('i', 'i'),\n",
       " ('miss', 'miss'),\n",
       " ('that', 'that'),\n",
       " ('team', 'team'),\n",
       " ('if', 'if'),\n",
       " ('only', 'only'),\n",
       " ('the', 'the'),\n",
       " ('defense', 'defense'),\n",
       " ('was', 'wa'),\n",
       " ('a', 'a'),\n",
       " ('bit', 'bit'),\n",
       " ('better', 'better'),\n",
       " ('at', 'at'),\n",
       " ('that', 'that'),\n",
       " ('point', 'point'),\n",
       " ('in', 'in'),\n",
       " ('time', 'time'),\n",
       " ('or', 'or'),\n",
       " ('the', 'the'),\n",
       " ('refs', 'ref'),\n",
       " ('could', 'could'),\n",
       " ('call', 'call'),\n",
       " ('a', 'a'),\n",
       " ('catch', 'catch'),\n",
       " ('correctly', 'correctly'),\n",
       " ('we', 'we'),\n",
       " ('would', 'would'),\n",
       " ('ve', 've'),\n",
       " ('gone', 'gone'),\n",
       " ('to', 'to'),\n",
       " ('seattle', 'seattle'),\n",
       " ('and', 'and'),\n",
       " ('beaten', 'beaten'),\n",
       " ('them', 'them'),\n",
       " ('again', 'again'),\n",
       " ('would', 'would'),\n",
       " ('we', 'we'),\n",
       " ('have', 'have'),\n",
       " ('beaten', 'beaten'),\n",
       " ('the', 'the'),\n",
       " ('pats', 'pat'),\n",
       " ('probably', 'probably'),\n",
       " ('not', 'not'),\n",
       " ('because', 'because'),\n",
       " ('our', 'our'),\n",
       " ('defense', 'defense'),\n",
       " ('couldn', 'couldn'),\n",
       " ('t', 't'),\n",
       " ('stop', 'stop'),\n",
       " ('brady', 'brady'),\n",
       " ('but', 'but'),\n",
       " ('damn', 'damn'),\n",
       " ('i', 'i'),\n",
       " ('would', 'would'),\n",
       " ('ve', 've'),\n",
       " ('loved', 'loved'),\n",
       " ('to', 'to'),\n",
       " ('see', 'see'),\n",
       " ('romo', 'romo'),\n",
       " ('and', 'and'),\n",
       " ('that', 'that'),\n",
       " ('offense', 'offense'),\n",
       " ('go', 'go'),\n",
       " ('up', 'up'),\n",
       " ('against', 'against'),\n",
       " ('bellichek', 'bellichek')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider lemmatizing tokenized characters\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokens_lem = [lemmatizer.lemmatize(i) for i in words]\n",
    "# Compare original and lemmatized tokens\n",
    "list(zip(words, tokens_lem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T17:14:01.118033Z",
     "start_time": "2020-04-20T17:14:01.112584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Print English stopwords.\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:03:44.811623Z",
     "start_time": "2020-04-26T03:03:44.797769Z"
    }
   },
   "outputs": [],
   "source": [
    "my_stop_words = stopwords.words(\"english\") + [\n",
    "    \"dallas\",\n",
    "    \"cowboys\",\n",
    "    \"cowboy\",\n",
    "    \"philadelphia\",\n",
    "    \"eagles\",\n",
    "    \"philly\",\n",
    "    \"eagle\",\n",
    "    \"phi\",\n",
    "    \"dal\",\n",
    "    \"dallascowboys\",\n",
    "    \"boys\",\n",
    "    \"birds\",\n",
    "    \"texas\",\n",
    "    \"app\",\n",
    "    \"http\",\n",
    "    \"https\",\n",
    "    \"etc\",\n",
    "    \"imgur\",\n",
    "    \"www\",\n",
    "    \"com\",\n",
    "    \"png\",\n",
    "    \"auto\",\n",
    "    \"width\",\n",
    "    \"reddit\",\n",
    "    \"x200b\",\n",
    "    \"2qgz89t\",\n",
    "    \"2z11chhwhq99x5sashj6fb5xi6wir5t\",\n",
    "    \"zeke\",\n",
    "    \"dak\",\n",
    "    \"prescott\",\n",
    "    \"romo\",\n",
    "    \"tony\",\n",
    "    \"carson\",\n",
    "    \"wentz\",\n",
    "    \"dez\",\n",
    "    \"bryant\",\n",
    "    \"demarco\",\n",
    "    \"murray\",\n",
    "    \"cole\",\n",
    "    \"beasley\",\n",
    "    \"aldon\",\n",
    "    \"smith\",\n",
    "    \"amari\",\n",
    "    \"cooper\",\n",
    "    \"alshon\",\n",
    "    \"jeffery\",\n",
    "    \"anthony\",\n",
    "    \"brown\", \n",
    "    \"chidobe\",\n",
    "    \"awuzie\",\n",
    "    \"avonte\",\n",
    "    \"maddox\",\n",
    "    \"blake\",\n",
    "    \"jarwin\",\n",
    "    \"brian\",\n",
    "    \"dawkins\",\n",
    "    \"byron\",\n",
    "    \"jones\",\n",
    "    \"cam\",\n",
    "    \"fleming\",\n",
    "    \"haha\",\n",
    "    \"ha\", \n",
    "    \"ha-ha\",\n",
    "    \"clinton\",\n",
    "    \"clinton-dix\",\n",
    "    \"dix\",\n",
    "    \"rush\",\n",
    "    \"darius\",\n",
    "    \"slay\",\n",
    "    \"david\",\n",
    "    \"irving\",\n",
    "    \"demarcus\",\n",
    "    \"lawrence\",\n",
    "    \"robinson\",\n",
    "    \"desean\",\n",
    "    \"jackson\",\n",
    "    \"donovan\",\n",
    "    \"mcnabb\",\n",
    "    \"dontari\",\n",
    "    \"poe\",\n",
    "    \"dorsett\",\n",
    "    \"ezekiel\",\n",
    "    \"elliott\",\n",
    "    \"jake\",\n",
    "    \"zach\",\n",
    "    \"ertz\",\n",
    "    \"leighton\",\n",
    "    \"van\",\n",
    "    \"der\",\n",
    "    \"esch\",\n",
    "    \"fletcher\",\n",
    "    \"cox\",\n",
    "    \"nick\",\n",
    "    \"foles\",\n",
    "    \"kai\",\n",
    "    \"forbath\",\n",
    "    \"travis\",\n",
    "    \"frederick\",\n",
    "    \"jason\",\n",
    "    \"garrett\",\n",
    "    \"witten\",\n",
    "    \"kelce\",\n",
    "    \"peters\",\n",
    "    \"gerald\",\n",
    "    \"mccoy\",\n",
    "    \"howie\",\n",
    "    \"roseman\",\n",
    "    \"jeff\",\n",
    "    \"heath\",\n",
    "    \"tim\",\n",
    "    \"jernigan\",\n",
    "    \"jerry\",\n",
    "    \"jim\",\n",
    "    \"schwartz\",\n",
    "    \"joe\",\n",
    "    \"looney\",\n",
    "    \"jordan\",\n",
    "    \"matthews\",\n",
    "    \"jp\",\n",
    "    \"ladouceur\",\n",
    "    \"lane\",\n",
    "    \"johnson\",\n",
    "    \"lesean\",\n",
    "    \"mccoy\",\n",
    "    \"maliek\",\n",
    "    \"collins\",\n",
    "    \"mike\",\n",
    "    \"mccarthy\",\n",
    "    \"michael\",\n",
    "    \"gallup\",\n",
    "    \"miles\",\n",
    "    \"sanders\",\n",
    "    \"nelson\",\n",
    "    \"agholor\",\n",
    "    \"nickell\",\n",
    "    \"robey\",\n",
    "    \"coleman\",\n",
    "    \"robert\",\n",
    "    \"quinn\",\n",
    "    \"randall\",\n",
    "    \"cobb\",\n",
    "    \"randy\",\n",
    "    \"gregory\",\n",
    "    \"sean\",\n",
    "    \"lee\",\n",
    "    \"roger\",\n",
    "    \"staubach\",\n",
    "    \"stephen\",\n",
    "    \"tank\",\n",
    "    \"troy\",\n",
    "    \"aikman\",\n",
    "    \"tyron\",\n",
    "    \"smith\",\n",
    "    \"vick\",\n",
    "    \"whiteside\",\n",
    "    \"zack\",\n",
    "    \"martin\",\n",
    "    \"zuerlein\",\n",
    "    \"andre\",\n",
    "    \"dillard\",\n",
    "    \"boston\",\n",
    "    \"scott\",\n",
    "    \"brandon\",\n",
    "    \"graham\",\n",
    "    \"cameron\",\n",
    "    \"chido\",\n",
    "    \"cunningham\",\n",
    "    \"curtis\",\n",
    "    \"samuel\",\n",
    "    \"darian\",\n",
    "    \"thompson\",\n",
    "    \"jeremy\",\n",
    "    \"maclin\",\n",
    "    \"kerry\",\n",
    "    \"hyder\",\n",
    "    \"rasul\",\n",
    "    \"douglas\",\n",
    "    \"tavon\",\n",
    "    \"austin\",\n",
    "    \"timmy\",\n",
    "    \"malcolm\",\n",
    "    \"jenkins\",\n",
    "    \"jalen\",\n",
    "    \"mills\",\n",
    "    \"greg\",\n",
    "    \"javon\",\n",
    "    \"hargrave\",\n",
    "    \"kellen\",\n",
    "    \"moore\",\n",
    "    \"dlaw\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:03:47.285652Z",
     "start_time": "2020-04-26T03:03:47.277119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'dallas',\n",
       " 'cowboys',\n",
       " 'cowboy',\n",
       " 'philadelphia',\n",
       " 'eagles',\n",
       " 'philly',\n",
       " 'eagle',\n",
       " 'phi',\n",
       " 'dal',\n",
       " 'dallascowboys',\n",
       " 'boys',\n",
       " 'birds',\n",
       " 'texas',\n",
       " 'app',\n",
       " 'http',\n",
       " 'https',\n",
       " 'etc',\n",
       " 'imgur',\n",
       " 'www',\n",
       " 'com',\n",
       " 'png',\n",
       " 'auto',\n",
       " 'width',\n",
       " 'reddit',\n",
       " 'x200b',\n",
       " '2qgz89t',\n",
       " '2z11chhwhq99x5sashj6fb5xi6wir5t',\n",
       " 'zeke',\n",
       " 'dak',\n",
       " 'prescott',\n",
       " 'romo',\n",
       " 'tony',\n",
       " 'carson',\n",
       " 'wentz',\n",
       " 'dez',\n",
       " 'bryant',\n",
       " 'demarco',\n",
       " 'murray',\n",
       " 'cole',\n",
       " 'beasley',\n",
       " 'aldon',\n",
       " 'smith',\n",
       " 'amari',\n",
       " 'cooper',\n",
       " 'alshon',\n",
       " 'jeffery',\n",
       " 'anthony',\n",
       " 'brown',\n",
       " 'chidobe',\n",
       " 'awuzie',\n",
       " 'avonte',\n",
       " 'maddox',\n",
       " 'blake',\n",
       " 'jarwin',\n",
       " 'brian',\n",
       " 'dawkins',\n",
       " 'byron',\n",
       " 'jones',\n",
       " 'cam',\n",
       " 'fleming',\n",
       " 'haha',\n",
       " 'ha',\n",
       " 'ha-ha',\n",
       " 'clinton',\n",
       " 'clinton-dix',\n",
       " 'dix',\n",
       " 'rush',\n",
       " 'darius',\n",
       " 'slay',\n",
       " 'david',\n",
       " 'irving',\n",
       " 'demarcus',\n",
       " 'lawrence',\n",
       " 'robinson',\n",
       " 'desean',\n",
       " 'jackson',\n",
       " 'donovan',\n",
       " 'mcnabb',\n",
       " 'dontari',\n",
       " 'poe',\n",
       " 'dorsett',\n",
       " 'ezekiel',\n",
       " 'elliott',\n",
       " 'jake',\n",
       " 'zach',\n",
       " 'ertz',\n",
       " 'leighton',\n",
       " 'van',\n",
       " 'der',\n",
       " 'esch',\n",
       " 'fletcher',\n",
       " 'cox',\n",
       " 'nick',\n",
       " 'foles',\n",
       " 'kai',\n",
       " 'forbath',\n",
       " 'travis',\n",
       " 'frederick',\n",
       " 'jason',\n",
       " 'garrett',\n",
       " 'witten',\n",
       " 'kelce',\n",
       " 'peters',\n",
       " 'gerald',\n",
       " 'mccoy',\n",
       " 'howie',\n",
       " 'roseman',\n",
       " 'jeff',\n",
       " 'heath',\n",
       " 'tim',\n",
       " 'jernigan',\n",
       " 'jerry',\n",
       " 'jim',\n",
       " 'schwartz',\n",
       " 'joe',\n",
       " 'looney',\n",
       " 'jordan',\n",
       " 'matthews',\n",
       " 'jp',\n",
       " 'ladouceur',\n",
       " 'lane',\n",
       " 'johnson',\n",
       " 'lesean',\n",
       " 'mccoy',\n",
       " 'maliek',\n",
       " 'collins',\n",
       " 'mike',\n",
       " 'mccarthy',\n",
       " 'michael',\n",
       " 'gallup',\n",
       " 'miles',\n",
       " 'sanders',\n",
       " 'nelson',\n",
       " 'agholor',\n",
       " 'nickell',\n",
       " 'robey',\n",
       " 'coleman',\n",
       " 'robert',\n",
       " 'quinn',\n",
       " 'randall',\n",
       " 'cobb',\n",
       " 'randy',\n",
       " 'gregory',\n",
       " 'sean',\n",
       " 'lee',\n",
       " 'roger',\n",
       " 'staubach',\n",
       " 'stephen',\n",
       " 'tank',\n",
       " 'troy',\n",
       " 'aikman',\n",
       " 'tyron',\n",
       " 'smith',\n",
       " 'vick',\n",
       " 'whiteside',\n",
       " 'zack',\n",
       " 'martin',\n",
       " 'zuerlein',\n",
       " 'andre',\n",
       " 'dillard',\n",
       " 'boston',\n",
       " 'scott',\n",
       " 'brandon',\n",
       " 'graham',\n",
       " 'cameron',\n",
       " 'chido',\n",
       " 'cunningham',\n",
       " 'curtis',\n",
       " 'samuel',\n",
       " 'darian',\n",
       " 'thompson',\n",
       " 'jeremy',\n",
       " 'maclin',\n",
       " 'kerry',\n",
       " 'hyder',\n",
       " 'rasul',\n",
       " 'douglas',\n",
       " 'tavon',\n",
       " 'austin',\n",
       " 'timmy',\n",
       " 'malcolm',\n",
       " 'jenkins',\n",
       " 'jalen',\n",
       " 'mills',\n",
       " 'greg',\n",
       " 'javon',\n",
       " 'hargrave',\n",
       " 'kellen',\n",
       " 'moore',\n",
       " 'dlaw']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T04:22:01.716579Z",
     "start_time": "2020-04-19T04:22:01.713795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from \"words.\"\n",
    "words = [w for w in words if w not in my_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T04:22:01.723695Z",
     "start_time": "2020-04-19T04:22:01.719491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2014',\n",
       " 'gamepass',\n",
       " 'free',\n",
       " 'rewatching',\n",
       " 'lot',\n",
       " '2014',\n",
       " 'highlights',\n",
       " 'great',\n",
       " 'enjoyable',\n",
       " 'season',\n",
       " 'say',\n",
       " 'man',\n",
       " 'beast',\n",
       " 'year',\n",
       " 'think',\n",
       " 'people',\n",
       " 'sub',\n",
       " 'forget',\n",
       " 'much',\n",
       " 'times',\n",
       " 'injury',\n",
       " 'issues',\n",
       " 'soon',\n",
       " 'came',\n",
       " 'take',\n",
       " '2014',\n",
       " 'season',\n",
       " 'minus',\n",
       " 'fumbles',\n",
       " 'season',\n",
       " 'thus',\n",
       " 'far',\n",
       " 'career',\n",
       " 'although',\n",
       " 'still',\n",
       " 'take',\n",
       " 'player',\n",
       " 'consistency',\n",
       " 'also',\n",
       " 'offense',\n",
       " 'looked',\n",
       " 'damn',\n",
       " 'near',\n",
       " 'unstoppable',\n",
       " 'times',\n",
       " 'everyone',\n",
       " 'peak',\n",
       " 'offensive',\n",
       " 'line',\n",
       " 'skill',\n",
       " 'guys',\n",
       " 'even',\n",
       " 'dunbar',\n",
       " 'solid',\n",
       " 'role',\n",
       " 'team',\n",
       " 'third',\n",
       " 'downs',\n",
       " 'hell',\n",
       " 'even',\n",
       " 'body',\n",
       " 'catcher',\n",
       " 'good',\n",
       " '#2',\n",
       " 'wr',\n",
       " 'damn',\n",
       " 'miss',\n",
       " 'team',\n",
       " 'defense',\n",
       " 'bit',\n",
       " 'better',\n",
       " 'point',\n",
       " 'time',\n",
       " 'refs',\n",
       " 'could',\n",
       " 'call',\n",
       " 'catch',\n",
       " 'correctly',\n",
       " 'would',\n",
       " 'gone',\n",
       " 'seattle',\n",
       " 'beaten',\n",
       " 'would',\n",
       " 'beaten',\n",
       " 'pats',\n",
       " 'probably',\n",
       " 'defense',\n",
       " 'stop',\n",
       " 'brady',\n",
       " 'damn',\n",
       " 'would',\n",
       " 'loved',\n",
       " 'see',\n",
       " 'offense',\n",
       " 'go',\n",
       " 'bellichek']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T16:00:20.966782Z",
     "start_time": "2020-04-18T16:00:20.964828Z"
    }
   },
   "source": [
    "## Combine into single function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function without lemmatizer included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:31:56.662619Z",
     "start_time": "2020-04-26T00:31:56.658284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from Matt Brems's NLP lecture code\n",
    "# This version does NOT include a lemmatizer\n",
    "def reddit_to_words(raw_post, stop_words):\n",
    "    # Function to convert a raw reddit post to a string of words\n",
    "    # The input is a single string (a raw reddit post), and \n",
    "    # the output is a single string (a preprocessed reddit post)\n",
    "    \n",
    "    # 1. Remove HTML. There may not be any in these posts, but just in case.\n",
    "    post_text = BeautifulSoup(raw_post).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters/non-numbers, but keep # character\n",
    "    okchars_only = re.sub(\"[^a-zA-Z0-9\\#]\", \" \", post_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = okchars_only.lower().split()\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stopwords to a set.\n",
    "    stops = set(stop_words)\n",
    "    \n",
    "    # 5. Remove stopwords.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function without lemmatizer, also excludes numbers and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:31:58.968768Z",
     "start_time": "2020-04-26T00:31:58.964217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from Matt Brems's NLP lecture code\n",
    "# This version does NOT include a lemmatizer\n",
    "def reddit_to_words_nonum(raw_post, stop_words):\n",
    "    # Function to convert a raw reddit post to a string of words\n",
    "    # The input is a single string (a raw reddit post), and \n",
    "    # the output is a single string (a preprocessed reddit post)\n",
    "    \n",
    "    # 1. Remove HTML. There may not be any in these posts, but just in case.\n",
    "    post_text = BeautifulSoup(raw_post).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters/non-numbers, but keep # character\n",
    "    okchars_only = re.sub(\"[^a-zA-Z]\", \" \", post_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = okchars_only.lower().split()\n",
    "    \n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stopwords to a set.\n",
    "    stops = set(stop_words)\n",
    "    \n",
    "    # 5. Remove stopwords.\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function with lemmatizer that also removes numbers and # sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:32:02.166836Z",
     "start_time": "2020-04-26T00:32:02.161914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from Matt Brems's NLP lecture code\n",
    "# This version INCLUDES a lemmatizer\n",
    "def reddit_to_words_wlem(raw_post, stop_words):\n",
    "    # Function to convert a raw reddit post to a string of words\n",
    "    # The input is a single string (a raw reddit post), and \n",
    "    # the output is a single string (a preprocessed reddit post)\n",
    "    \n",
    "    # 1. Remove HTML. There may not be any in these posts, but just in case.\n",
    "    post_text = BeautifulSoup(raw_post).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters/non-numbers, but keep # character\n",
    "    okchars_only = re.sub(\"[^a-zA-Z]\", \" \", post_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = okchars_only.lower().split()\n",
    "    \n",
    "    # 4. Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lem = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "    # 5. In Python, searching a set is much faster than searching\n",
    "    # a list, so convert the stopwords to a set.\n",
    "    stops = set(stop_words)\n",
    "    \n",
    "    # 6. Remove stopwords.\n",
    "    meaningful_words = [w for w in tokens_lem if not w in stops]\n",
    "    \n",
    "    # 7. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T00:32:05.270197Z",
     "start_time": "2020-04-26T00:32:05.267151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2000 posts in the dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of posts in the dataframe.\n",
    "total_posts = df.shape[0]\n",
    "print(f'There are {total_posts} posts in the dataframe.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try un-lemmatized version first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T17:52:46.432713Z",
     "start_time": "2020-04-19T17:52:45.939183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set reddit posts...\n",
      "Review 100 of 2000.\n",
      "Review 200 of 2000.\n",
      "Review 300 of 2000.\n",
      "Review 400 of 2000.\n",
      "Review 500 of 2000.\n",
      "Review 600 of 2000.\n",
      "Review 700 of 2000.\n",
      "Review 800 of 2000.\n",
      "Review 900 of 2000.\n",
      "Review 1000 of 2000.\n",
      "Review 1100 of 2000.\n",
      "Review 1200 of 2000.\n",
      "Review 1300 of 2000.\n",
      "Review 1400 of 2000.\n",
      "Cleaning and parsing the testing set reddit posts...\n",
      "Review 1500 of 2000.\n",
      "Review 1600 of 2000.\n",
      "Review 1700 of 2000.\n",
      "Review 1800 of 2000.\n",
      "Review 1900 of 2000.\n",
      "Review 2000 of 2000.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean posts.\n",
    "clean_train_posts = []\n",
    "clean_test_posts = []\n",
    "\n",
    "print(\"Cleaning and parsing the training set reddit posts...\")\n",
    "\n",
    "# Instantiate counter.\n",
    "j = 0\n",
    "\n",
    "# For every post in our training set...\n",
    "for train_post in X_train['title_selftext']:\n",
    "    \n",
    "    # Convert post to words, then append to clean_train_posts.\n",
    "    clean_train_posts.append(reddit_to_words_nonum(train_post, my_stop_words))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message.\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {total_posts}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "print(\"Cleaning and parsing the testing set reddit posts...\")\n",
    "\n",
    "# For every post in our testing set...\n",
    "for test_post in X_test['title_selftext']:\n",
    "    \n",
    "    # Convert post to words, then append to clean_train_posts.\n",
    "    clean_test_posts.append(reddit_to_words_nonum(test_post, my_stop_words))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message.\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {total_posts}.')\n",
    "        \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-19T17:52:51.963782Z",
     "start_time": "2020-04-19T17:52:51.960849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'case drafting reagor reagor talented explosive collegiate playmaker hope target first round trade back scenario reagor blessed incredible athletic traits truly special athlete reaches top speed incredibly quickly change direction instant potential rac superstar aggressive ball carrier attacks defenders strong compliment power finesse moves studying reagor games saw player extreme boom bust potential player raw athletic gifts incredible ball carrier vision also significant flaws game although talented receiver belong second tier jefferson mims opposed drafting first wide receiver reagor athletic traits project favourable scheme fit yet believe reagor would wasted pick natural hands catcher double catches straight drops passes consistently drops concentration drops others occur already thinking running secured catch drops passes three levels naturally track ball shoulder uses pads help make shoulder catches instead relying hands shows lack concentration fighting contact traffic reagor hands give confidence projects well reliable target coaching help solve issues flaws every level screen fade scares makes believe changed worth sharing reagor horrible qb play maybe worst prospect draft awful qb play really complicates evaluation clearly moments reagor open high school standards qb see tcu top qb evaluation could completely different reagor reaction qb play concerns saw multiple plays adjusting towel ball snapped run play making step behind teammates complained poor throws early game best player tcu team put consistent effort play number called building team want player always give best thoughts'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_posts[250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try lemmatized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:04:16.171056Z",
     "start_time": "2020-04-26T03:04:15.385265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set reddit posts...\n",
      "Review 100 of 2000.\n",
      "Review 200 of 2000.\n",
      "Review 300 of 2000.\n",
      "Review 400 of 2000.\n",
      "Review 500 of 2000.\n",
      "Review 600 of 2000.\n",
      "Review 700 of 2000.\n",
      "Review 800 of 2000.\n",
      "Review 900 of 2000.\n",
      "Review 1000 of 2000.\n",
      "Review 1100 of 2000.\n",
      "Review 1200 of 2000.\n",
      "Review 1300 of 2000.\n",
      "Review 1400 of 2000.\n",
      "Cleaning and parsing the testing set reddit posts...\n",
      "Review 1500 of 2000.\n",
      "Review 1600 of 2000.\n",
      "Review 1700 of 2000.\n",
      "Review 1800 of 2000.\n",
      "Review 1900 of 2000.\n",
      "Review 2000 of 2000.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean posts.\n",
    "clean_train_posts_lem = []\n",
    "clean_test_posts_lem = []\n",
    "\n",
    "print(\"Cleaning and parsing the training set reddit posts...\")\n",
    "\n",
    "# Instantiate counter.\n",
    "j = 0\n",
    "\n",
    "# For every post in our training set...\n",
    "for train_post in X_train['title_selftext']:\n",
    "    \n",
    "    # Convert post to words, then append to clean_train_posts.\n",
    "    clean_train_posts_lem.append(reddit_to_words_wlem(train_post, my_stop_words))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message.\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {total_posts}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "print(\"Cleaning and parsing the testing set reddit posts...\")\n",
    "\n",
    "# For every post in our testing set...\n",
    "for test_post in X_test['title_selftext']:\n",
    "    \n",
    "    # Convert post to words, then append to clean_train_posts.\n",
    "    clean_test_posts_lem.append(reddit_to_words_wlem(test_post, my_stop_words))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message.\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {total_posts}.')\n",
    "        \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T01:20:07.324481Z",
     "start_time": "2020-04-20T01:20:07.321049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'case drafting reagor reagor talented explosive collegiate playmaker hope target first round trade back scenario reagor blessed incredible athletic trait truly special athlete reach top speed incredibly quickly change direction instant potential rac superstar aggressive ball carrier attack defender strong compliment power finesse move studying reagor game saw player extreme boom bust potential player raw athletic gift incredible ball carrier vision also significant flaw game although talented receiver doe belong second tier jefferson mims opposed drafting first wide receiver reagor athletic trait project favourable scheme fit yet believe reagor would wasted pick natural hand catcher double catch straight drop pass consistently drop concentration drop others occur already thinking running secured catch drop pass three level naturally track ball shoulder us pad help make shoulder catch instead relying hand show lack concentration fighting contact traffic reagor hand give confidence project well reliable target coaching help solve issue flaw every level screen fade scare make believe changed worth sharing reagor horrible qb play maybe worst prospect draft awful qb play really complicates evaluation clearly moment reagor open high school standard qb see tcu top qb evaluation could completely different reagor reaction qb play concern saw multiple play wa adjusting towel ball wa snapped run play making step behind teammate complained poor throw early game wa best player tcu team put consistent effort play number called wa building team want player always give best thought'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_posts_lem[250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First try using count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:04:25.819692Z",
     "start_time": "2020-04-26T03:04:25.815982Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_cvec = Pipeline([\n",
    "    ('cvec', CountVectorizer(min_df=2, stop_words=my_stop_words)),\n",
    "    ('ss', StandardScaler(with_mean=False)),\n",
    "    ('lr', LogisticRegression(penalty='l1', solver='saga', random_state=42, max_iter=5000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:04:26.692969Z",
     "start_time": "2020-04-26T03:04:26.689880Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_cvec_params = {\n",
    "    'cvec__max_features': [750, 1000, 1250],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "#     'cvec__min_df': [1, 2, 5],\n",
    "    'lr__C': [.1, .35, .5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:06:02.452259Z",
     "start_time": "2020-04-26T03:04:40.861766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   13.1s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=2,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=['i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    '...\n",
       "                                                           max_iter=5000,\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l1',\n",
       "                                                           random_state=42,\n",
       "                                                           solver='saga',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'cvec__max_features': [750, 1000, 1250],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'lr__C': [0.1, 0.35, 0.5]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate grid search\n",
    "gs = GridSearchCV(pipe_cvec, \n",
    "                  param_grid=pipe_cvec_params, \n",
    "                  cv=5, \n",
    "                  n_jobs=-1,\n",
    "                  verbose=1)\n",
    "\n",
    "# Fit grid search to training data\n",
    "gs.fit(clean_train_posts_lem, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:07:13.207685Z",
     "start_time": "2020-04-26T03:07:13.203574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 1000, 'cvec__ngram_range': (1, 1), 'lr__C': 0.35}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:07:21.463924Z",
     "start_time": "2020-04-26T03:07:21.415535Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score 0.6564285714285715\n",
      "Best train score 0.8957142857142857\n",
      "Best test score 0.6533333333333333\n"
     ]
    }
   ],
   "source": [
    "# Best score\n",
    "print(f'Best CV score', gs.best_score_)\n",
    "print(f'Best train score', gs.score(clean_train_posts_lem, y_train))\n",
    "print(f'Best test score', gs.score(clean_test_posts_lem, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:07:41.517315Z",
     "start_time": "2020-04-26T03:07:41.514111Z"
    }
   },
   "outputs": [],
   "source": [
    "c_vectorizer = CountVectorizer(max_features=1000, ngram_range=(1,1), min_df=2, stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:07:43.171993Z",
     "start_time": "2020-04-26T03:07:43.121513Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_features_c = c_vectorizer.fit_transform(clean_train_posts_lem)\n",
    "\n",
    "test_data_features_c = c_vectorizer.transform(clean_test_posts_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:07:44.202267Z",
     "start_time": "2020-04-26T03:07:44.199363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_features_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:07:44.991464Z",
     "start_time": "2020-04-26T03:07:44.988837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_features_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:07:46.169143Z",
     "start_time": "2020-04-26T03:07:46.165209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aav', 'ability', 'able', 'absolute', 'absolutely', 'according', 'act', 'actually', 'adam', 'adamant', 'add', 'adding', 'address', 'age', 'agency', 'agent', 'ago', 'agree', 'agreed', 'agreement', 'ahead', 'aiyuk', 'aj', 'alabama', 'allowed', 'almost', 'along', 'already', 'also', 'although', 'always', 'amazing', 'among', 'amount', 'anderson', 'andrew', 'another', 'anybody', 'anyone', 'anything', 'anywhere', 'appreciate', 'april', 'arcega', 'archer', 'arizona', 'around', 'article', 'ask', 'asking', 'athletic', 'attack', 'attempt', 'authentic', 'available', 'average', 'away', 'awesome', 'back', 'backup', 'bad', 'ball', 'barnett', 'base', 'based', 'basically', 'baun', 'baylor', 'bear', 'beat', 'became', 'become', 'becomes', 'begin', 'beginning', 'behind', 'believe', 'best', 'better', 'big', 'biggest', 'bill', 'bird', 'birthday', 'bit', 'blocker', 'blocking', 'blogging', 'board', 'boise', 'bonus', 'bored', 'bottom', 'boundary', 'bowl', 'box', 'boy', 'bpa', 'brady', 'brandin', 'break', 'breakdown', 'breaking', 'bring', 'bringing', 'broke', 'bronco', 'brook', 'brought', 'bucs', 'build', 'burrow', 'burton', 'call', 'called', 'came', 'camp', 'cap', 'card', 'career', 'carolina', 'case', 'catch', 'cb', 'cba', 'cbs', 'ceedee', 'center', 'chaisson', 'chance', 'change', 'changing', 'charger', 'chart', 'chase', 'cheap', 'cheaper', 'check', 'chief', 'chinn', 'choice', 'choose', 'chris', 'city', 'cj', 'class', 'claypool', 'clear', 'clemson', 'cleveland', 'clock', 'close', 'clowney', 'co', 'coach', 'college', 'color', 'colt', 'combine', 'come', 'comeback', 'coming', 'comment', 'community', 'comp', 'comparison', 'competitive', 'completely', 'concern', 'consensus', 'consider', 'considering', 'continue', 'contract', 'conversation', 'cook', 'cool', 'coop', 'core', 'corner', 'cornerback', 'coronavirus', 'cost', 'could', 'couple', 'course', 'cover', 'coverage', 'covid', 'crazy', 'create', 'credit', 'curious', 'current', 'currently', 'cut', 'damn', 'damon', 'davis', 'day', 'db', 'de', 'deadline', 'deal', 'decade', 'decent', 'decided', 'decision', 'deep', 'defender', 'defense', 'defensive', 'definitely', 'delpit', 'denver', 'denzel', 'depth', 'derrick', 'despite', 'detail', 'develop', 'devin', 'difference', 'different', 'disagree', 'discus', 'discussion', 'distancing', 'dj', 'dl', 'doe', 'dolphin', 'done', 'double', 'draft', 'drafted', 'drafting', 'drive', 'drop', 'dt', 'dude', 'due', 'duo', 'early', 'easily', 'easy', 'edge', 'edit', 'edward', 'either', 'elite', 'else', 'emmanuel', 'end', 'ended', 'enjoy', 'enough', 'entire', 'epenesa', 'er', 'era', 'eric', 'especially', 'espn', 'even', 'ever', 'every', 'everyone', 'everything', 'exciting', 'expect', 'expected', 'experience', 'explosive', 'extended', 'extension', 'eye', 'fa', 'face', 'facetime', 'fact', 'fair', 'falcon', 'fall', 'family', 'fan', 'fantastic', 'far', 'favorite', 'fb', 'feel', 'feeling', 'felt', 'field', 'figured', 'fill', 'film', 'finally', 'find', 'finding', 'fine', 'finished', 'fire', 'first', 'fisher', 'fit', 'five', 'florida', 'fly', 'focus', 'football', 'form', 'former', 'forward', 'found', 'four', 'franchise', 'free', 'friday', 'friend', 'front', 'fuck', 'full', 'fully', 'fulton', 'fumble', 'fun', 'future', 'game', 'gamepass', 'garafolo', 'gave', 'general', 'gerry', 'get', 'getting', 'giant', 'gift', 'give', 'given', 'giving', 'gm', 'go', 'god', 'goedert', 'going', 'golden', 'gone', 'gonna', 'good', 'google', 'got', 'grade', 'grant', 'great', 'greatest', 'green', 'group', 'guaranteed', 'guess', 'guy', 'half', 'hall', 'hamler', 'hand', 'happen', 'happens', 'happy', 'hard', 'harris', 'harrison', 'hate', 'head', 'health', 'healthy', 'hear', 'heard', 'hearing', 'heart', 'held', 'hell', 'helmet', 'help', 'henderson', 'henry', 'herbert', 'hey', 'hi', 'higgins', 'high', 'higher', 'highest', 'highlight', 'hill', 'history', 'hit', 'hold', 'holding', 'hole', 'home', 'hope', 'hopefully', 'hoping', 'hopkins', 'hour', 'howard', 'however', 'huge', 'hurt', 'id', 'idea', 'ideal', 'im', 'important', 'improve', 'include', 'included', 'including', 'incredible', 'info', 'injury', 'inside', 'instagram', 'instead', 'interception', 'interest', 'interested', 'interesting', 'interview', 'issue', 'jag', 'jaguar', 'james', 'jefferson', 'jersey', 'jet', 'jeudy', 'jimmy', 'jj', 'jjaw', 'john', 'josh', 'jourdan', 'jr', 'jump', 'justin', 'keep', 'keeping', 'kelly', 'kenneth', 'key', 'kid', 'kind', 'kinda', 'kinlaw', 'knack', 'know', 'kristian', 'la', 'lack', 'lamb', 'land', 'last', 'late', 'later', 'latest', 'lavon', 'lb', 'le', 'leader', 'league', 'learn', 'least', 'leave', 'leaving', 'left', 'let', 'letter', 'level', 'leverage', 'lewis', 'life', 'like', 'likely', 'limited', 'line', 'linebacker', 'lineman', 'link', 'lion', 'list', 'listening', 'literally', 'little', 'live', 'logan', 'lol', 'long', 'longer', 'look', 'looked', 'looking', 'losing', 'lost', 'lot', 'love', 'low', 'lsu', 'lt', 'luck', 'lucky', 'lve', 'machine', 'madden', 'made', 'make', 'making', 'man', 'many', 'march', 'marcus', 'market', 'massive', 'match', 'matter', 'max', 'may', 'maybe', 'mckinney', 'mcleod', 'meadowlands', 'mean', 'medium', 'meeting', 'melo', 'member', 'meme', 'mention', 'message', 'met', 'mid', 'middle', 'might', 'mil', 'mile', 'mill', 'million', 'mims', 'mind', 'miracle', 'miss', 'missed', 'missing', 'mock', 'moment', 'monday', 'money', 'month', 'move', 'moving', 'much', 'multiple', 'must', 'name', 'nate', 'nation', 'nd', 'need', 'needed', 'negotiation', 'never', 'new', 'news', 'next', 'nfc', 'nfl', 'ngakoue', 'night', 'note', 'nothing', 'number', 'obj', 'obviously', 'offense', 'offensive', 'offer', 'office', 'official', 'offseason', 'ok', 'oklahoma', 'okudah', 'old', 'one', 'open', 'opinion', 'opportunity', 'option', 'order', 'ot', 'outside', 'overall', 'paid', 'panther', 'park', 'part', 'party', 'pas', 'pass', 'passer', 'passing', 'past', 'patrick', 'pay', 'paying', 'people', 'per', 'perfect', 'person', 'personal', 'personally', 'peter', 'physical', 'pick', 'pierre', 'place', 'plan', 'play', 'played', 'player', 'playing', 'playmaker', 'playoff', 'please', 'plus', 'po', 'podcast', 'point', 'poll', 'poor', 'position', 'possible', 'post', 'posted', 'potential', 'potentially', 'power', 'pre', 'prediction', 'prefer', 'present', 'press', 'pretty', 'price', 'pro', 'probably', 'problem', 'process', 'production', 'productive', 'program', 'project', 'projected', 'prospect', 'proven', 'provide', 'pull', 'put', 'qb', 'qbs', 'quarantine', 'quarterback', 'queen', 'question', 'quick', 'quite', 'raider', 'ram', 'ran', 'range', 'rank', 'ranked', 'rapoport', 'rather', 'rating', 'raven', 'raw', 'rb', 'rd', 'reach', 'read', 'reagor', 'real', 'really', 'reason', 'rec', 'receive', 'receiver', 'receiving', 'recently', 'record', 'redskin', 'regular', 'related', 'release', 'released', 'releasing', 'reliable', 'remaining', 'remember', 'removed', 'replace', 'replacement', 'report', 'resource', 'result', 'retired', 'retirement', 'retires', 'retiring', 'return', 'rex', 'ridgeway', 'right', 'ring', 'rival', 'road', 'role', 'rookie', 'room', 'roster', 'round', 'rounder', 'route', 'rt', 'ruggs', 'rule', 'rumor', 'run', 'runner', 'running', 'rusher', 'rushing', 'ryan', 'sack', 'safe', 'safety', 'said', 'salary', 'san', 'sander', 'save', 'saw', 'say', 'saying', 'scenario', 'schefter', 'scheme', 'school', 'score', 'scouting', 'seahawks', 'season', 'seattle', 'second', 'secondary', 'section', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'select', 'selection', 'sense', 'seriously', 'set', 'sf', 'share', 'sharing', 'shenault', 'shirt', 'short', 'show', 'showed', 'shown', 'si', 'side', 'sign', 'signed', 'signing', 'similar', 'simmons', 'since', 'single', 'sit', 'situation', 'size', 'skill', 'slater', 'slot', 'small', 'snap', 'social', 'solid', 'someone', 'something', 'sorry', 'source', 'space', 'special', 'speed', 'sport', 'spot', 'spreadsheet', 'st', 'stand', 'star', 'start', 'started', 'starter', 'starting', 'state', 'stats', 'stay', 'steal', 'steelers', 'step', 'still', 'stint', 'stop', 'story', 'straight', 'strength', 'stretch', 'strong', 'sub', 'subreddit', 'success', 'suck', 'super', 'superbowl', 'support', 'sure', 'surprise', 'system', 'table', 'tackle', 'tag', 'tagged', 'take', 'taken', 'taking', 'talent', 'talented', 'talk', 'talked', 'talking', 'tampa', 'tape', 'target', 'taylor', 'tcu', 'td', 'tds', 'te', 'team', 'tech', 'tee', 'tell', 'tennessee', 'term', 'texan', 'th', 'thank', 'thanks', 'thing', 'think', 'thinking', 'third', 'thomas', 'though', 'thought', 'thread', 'threat', 'three', 'throw', 'throwing', 'tier', 'tight', 'time', 'title', 'today', 'together', 'told', 'tom', 'ton', 'tonight', 'took', 'top', 'total', 'touchdown', 'tough', 'towards', 'trade', 'traded', 'trading', 'training', 'transition', 'trey', 'tried', 'trigger', 'true', 'try', 'trying', 'trysten', 'tua', 'turn', 'turned', 'twitter', 'two', 'type', 'understand', 'uniform', 'upcoming', 'update', 'use', 'used', 'using', 'usp', 'utah', 'utm', 'value', 'vega', 'versatile', 'veteran', 'via', 'video', 'view', 'viking', 'virtual', 'visit', 'vote', 'wa', 'wait', 'walk', 'walker', 'wallpaper', 'want', 'wanted', 'ward', 'washington', 'watch', 'watched', 'watching', 'watkins', 'way', 'weapon', 'wear', 'week', 'welcome', 'well', 'went', 'whatever', 'whole', 'wide', 'williams', 'willing', 'wilson', 'win', 'winfield', 'wing', 'winning', 'wish', 'without', 'wondering', 'wood', 'work', 'working', 'world', 'worst', 'worth', 'would', 'wr', 'wrong', 'wrs', 'xavier', 'xfl', 'yannick', 'yard', 'yeah', 'year', 'yes', 'yesterday', 'yet', 'young', 'youtube', 'yr', 'zone']\n"
     ]
    }
   ],
   "source": [
    "c_vocab = c_vectorizer.get_feature_names()\n",
    "print(c_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:07:50.497096Z",
     "start_time": "2020-04-26T03:07:50.491902Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scale data for use in regularized logistic regression model\n",
    "ss = StandardScaler(with_mean=False)\n",
    "\n",
    "X_train_sc_c = ss.fit_transform(train_data_features_c)\n",
    "X_test_sc_c = ss.transform(test_data_features_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:09:03.783753Z",
     "start_time": "2020-04-26T03:08:51.734115Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 3098 epochs took 12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   11.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.35, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=5000,\n",
       "                   multi_class='auto', n_jobs=-1, penalty='l1', random_state=42,\n",
       "                   solver='saga', tol=0.0001, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate logistic regression model.\n",
    "lr_c = LogisticRegression(penalty='l1', \n",
    "                          solver = 'saga', \n",
    "                          C=0.35, \n",
    "                          random_state=42, \n",
    "                          max_iter=5000,\n",
    "                          n_jobs=-1,\n",
    "                          verbose=1)\n",
    "\n",
    "# Fit model to training data.\n",
    "lr_c.fit(X_train_sc_c, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:10:15.792134Z",
     "start_time": "2020-04-26T03:09:24.917273Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    9.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    9.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    9.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   11.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6649999999999999\n",
      "0.8957142857142857\n",
      "0.6533333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   11.5s finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on cross-validated data\n",
    "c_cv_mean = cross_val_score(lr_c, X_train_sc_c, y_train, cv=5).mean()\n",
    "print(c_cv_mean)\n",
    "\n",
    "# Evaluate model on training data.\n",
    "train_acc_c = lr_c.score(X_train_sc_c, y_train)\n",
    "print(train_acc_c)\n",
    "\n",
    "# Evaluate model on testing data.\n",
    "test_acc_c = lr_c.score(X_test_sc_c, y_test)\n",
    "print(test_acc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:10:22.632829Z",
     "start_time": "2020-04-26T03:10:22.629644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict test data\n",
    "y_pred_c = lr_c.predict(X_test_sc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:10:23.944287Z",
     "start_time": "2020-04-26T03:10:23.939704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99816394e-01, 1.83605956e-04],\n",
       "       [8.90471879e-01, 1.09528121e-01],\n",
       "       [1.26393787e-01, 8.73606213e-01],\n",
       "       ...,\n",
       "       [6.90692697e-06, 9.99993093e-01],\n",
       "       [9.77582493e-01, 2.24175068e-02],\n",
       "       [6.55762097e-01, 3.44237903e-01]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict test data probabilities\n",
    "y_probs_c = lr_c.predict_proba(X_test_sc_c)\n",
    "y_probs_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:10:25.618738Z",
     "start_time": "2020-04-26T03:10:25.612615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176 124]\n",
      " [ 84 216]]\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_c).ravel()\n",
    "print(confusion_matrix(y_test, y_pred_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:10:28.995375Z",
     "start_time": "2020-04-26T03:10:28.988323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute sensitivity & specificity\n",
    "sensitivity_c = tp / (tp + fn)\n",
    "specificity_c = tn / (tn + fp)\n",
    "\n",
    "# Compute precision\n",
    "precision_c = precision_score(y_test, y_pred_c)\n",
    "\n",
    "# Compute roc_auc score\n",
    "roc_auc_c = roc_auc_score(y_test, y_probs_c[:, 1])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:10:30.017313Z",
     "start_time": "2020-04-26T03:10:30.011746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Val Accuracy: 0.665\n",
      "Train Accuracy: 0.8957\n",
      "Test Accuracy: 0.6533\n",
      "Test Sensitivity: 0.72\n",
      "Test Specificity: 0.5867\n",
      "Test Precision: 0.6353\n",
      "ROC AUC: 0.7223\n"
     ]
    }
   ],
   "source": [
    "# Model score summary\n",
    "print(f'Cross-Val Accuracy:', round(c_cv_mean, 4))\n",
    "print(f'Train Accuracy:', round(train_acc_c, 4))\n",
    "print(f'Test Accuracy:', round(test_acc_c, 4))\n",
    "print(f'Test Sensitivity:', round(sensitivity_c, 4))\n",
    "print(f'Test Specificity:', round(specificity_c, 4))\n",
    "print(f'Test Precision:', round(precision_c, 4))\n",
    "print(f'ROC AUC:', round(roc_auc_c, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:12:29.077900Z",
     "start_time": "2020-04-26T03:12:29.062183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00, -5.16216508e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  6.35734251e-02,  5.49362422e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  5.32000703e-03,\n",
       "         8.57277892e-02,  0.00000000e+00,  1.48386131e-01,\n",
       "         2.69308671e-01,  0.00000000e+00,  1.27103234e-01,\n",
       "         1.93403104e-02,  1.60352677e-02,  0.00000000e+00,\n",
       "        -2.59525842e-01,  2.69489552e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.83598420e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.26602218e-01,\n",
       "         9.20422703e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  9.06507465e-02,  1.62704116e-01,\n",
       "         9.66333752e-02,  0.00000000e+00,  1.57188791e-02,\n",
       "        -1.73240297e-01, -1.30749419e-01,  1.56637799e-01,\n",
       "         5.12570163e-03,  0.00000000e+00, -1.49435427e-02,\n",
       "        -2.97722543e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.36068571e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.04385225e-02,  0.00000000e+00, -2.48780497e-01,\n",
       "        -1.53975536e-01,  3.23677753e-01,  0.00000000e+00,\n",
       "         1.78932643e-01,  0.00000000e+00, -9.68550411e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  4.44452382e-02,\n",
       "         0.00000000e+00, -1.15663298e-02, -4.77952961e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  4.19294591e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.25697330e-02,  2.14650586e-02, -1.40644316e-01,\n",
       "         7.42976466e-02, -1.02768190e-01,  0.00000000e+00,\n",
       "         8.40394306e-03, -3.24229636e-01,  0.00000000e+00,\n",
       "         8.52340633e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         9.95065866e-02,  2.11933813e-03,  1.20919603e-01,\n",
       "        -3.97372988e-03, -8.10718061e-03,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.66561480e-01,\n",
       "         1.45435308e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -9.00209326e-02,  0.00000000e+00, -2.23330750e-01,\n",
       "         5.62415395e-02,  1.85057746e-01, -2.58657662e-02,\n",
       "        -1.23425382e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -7.21344822e-02,  5.56828529e-02, -5.89998732e-02,\n",
       "         0.00000000e+00, -9.01470483e-02,  2.97372720e-03,\n",
       "         0.00000000e+00, -1.17130503e-01,  6.49543364e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.58556562e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  6.23337698e-02,\n",
       "         2.24627395e-01,  1.36928173e-01,  1.98690391e-01,\n",
       "         0.00000000e+00, -4.62994431e-02,  6.79190639e-02,\n",
       "         0.00000000e+00,  7.71177072e-02,  6.37264907e-02,\n",
       "        -4.40707465e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.30047968e-02, -1.22593839e-01,\n",
       "         9.60051175e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         7.38553573e-03,  0.00000000e+00,  6.77826030e-02,\n",
       "        -1.09908796e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  7.12084534e-02, -6.04587198e-03,\n",
       "         1.38643727e-02,  0.00000000e+00, -1.28606645e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -3.11418187e-02,  4.34054207e-03, -7.14714322e-02,\n",
       "        -3.23241944e-01, -1.33313280e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.82704169e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.09769885e-02,  0.00000000e+00,\n",
       "        -3.87945191e-02,  0.00000000e+00,  2.62797558e-01,\n",
       "        -1.83713080e-02, -9.49803900e-02,  2.22210907e-01,\n",
       "         7.83395621e-02,  3.25205350e-04,  0.00000000e+00,\n",
       "         2.07516948e-01, -2.22822935e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.99862527e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -5.25266190e-02,\n",
       "        -4.92882725e-02,  0.00000000e+00, -1.21587005e-01,\n",
       "        -3.80413725e-02,  0.00000000e+00, -6.43520187e-03,\n",
       "         3.35980383e-02,  5.97514936e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.29436281e-01,  1.31170471e-01,\n",
       "         6.82487546e-02, -3.61354192e-02,  3.22045726e-01,\n",
       "        -5.49207896e-02,  0.00000000e+00, -1.68800108e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  7.98072110e-02,\n",
       "         4.11133663e-02,  0.00000000e+00, -2.41158219e-01,\n",
       "        -1.11978564e-01,  1.72594199e-01,  0.00000000e+00,\n",
       "         6.95550420e-03, -1.77986644e-02,  7.99536185e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -5.27032478e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.11777254e-02,\n",
       "         2.57256237e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.79963931e-01, -2.72079868e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  7.16584377e-02, -1.61005328e-01,\n",
       "        -2.02747516e-01,  2.65320769e-01, -6.96948333e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.59757912e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.04430089e-02,\n",
       "         1.33606573e-01,  0.00000000e+00,  1.41740868e-02,\n",
       "         3.54880974e-03,  0.00000000e+00,  1.73155678e-01,\n",
       "         0.00000000e+00,  2.89543528e-02,  1.53942433e-02,\n",
       "        -1.52130856e-01, -5.80684075e-02,  6.82760626e-02,\n",
       "         0.00000000e+00, -2.15536891e-01,  4.66922885e-02,\n",
       "         7.18996006e-02,  0.00000000e+00, -1.43472309e-01,\n",
       "         1.18181159e-01, -8.92273607e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  4.30579950e-03,\n",
       "         0.00000000e+00,  7.14839744e-02,  0.00000000e+00,\n",
       "         4.65634134e-02,  0.00000000e+00, -1.49395954e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -5.20393795e-03,  3.54589991e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.57930034e-02,\n",
       "        -7.59706470e-02,  0.00000000e+00,  3.28678823e-02,\n",
       "        -9.41640184e-02,  0.00000000e+00, -1.21208291e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.73895501e-01,  8.08766486e-02,\n",
       "         0.00000000e+00, -7.51123947e-03, -2.56881238e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -9.52996833e-03,\n",
       "         0.00000000e+00, -8.29950287e-02, -2.71395239e-02,\n",
       "         1.65870869e-02,  0.00000000e+00,  1.32588833e-01,\n",
       "         3.10365098e-02, -1.40432821e-02,  0.00000000e+00,\n",
       "        -1.87136243e-01,  0.00000000e+00,  1.23484837e-01,\n",
       "         0.00000000e+00,  1.02019562e-02, -2.68770312e-03,\n",
       "         0.00000000e+00,  4.23433534e-02,  1.41977707e-01,\n",
       "         2.42640952e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.69021997e-02,  4.00364576e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  6.42343780e-02, -4.18666441e-02,\n",
       "         2.55940302e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  8.08813703e-03,  0.00000000e+00,\n",
       "        -2.85618854e-01,  0.00000000e+00, -7.78841918e-02,\n",
       "         8.81355783e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "        -4.47715239e-02,  7.81118961e-02, -2.03235500e-01,\n",
       "         2.33416265e-01,  0.00000000e+00,  8.91377139e-02,\n",
       "         0.00000000e+00,  1.90818834e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.94137758e-01,\n",
       "         0.00000000e+00,  1.05233178e-01, -5.28214328e-02,\n",
       "         0.00000000e+00,  8.10732715e-02, -1.13294242e-01,\n",
       "         0.00000000e+00, -1.41342147e-01, -1.57861980e-01,\n",
       "         1.01771480e-01,  0.00000000e+00,  3.69042360e-02,\n",
       "         0.00000000e+00, -5.47911338e-02,  8.91406761e-02,\n",
       "        -8.16587214e-02,  0.00000000e+00,  2.33657172e-02,\n",
       "         3.23972670e-04,  0.00000000e+00,  0.00000000e+00,\n",
       "         5.45226526e-02, -6.75795917e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -7.82848428e-02, -4.63384743e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.99212475e-01,  1.44416686e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -6.79378551e-02,  9.82657780e-02,\n",
       "         8.42822099e-02,  0.00000000e+00, -1.05750442e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.95623822e-02,\n",
       "         1.51046547e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         3.48129742e-03, -1.47791335e-01,  0.00000000e+00,\n",
       "         0.00000000e+00, -3.46930124e-02, -1.45811703e-01,\n",
       "        -3.93003025e-02,  0.00000000e+00, -6.02915353e-02,\n",
       "         0.00000000e+00,  6.47140680e-02,  6.80714626e-02,\n",
       "        -3.70734659e-02, -1.61940242e-01,  0.00000000e+00,\n",
       "         0.00000000e+00, -3.98959705e-01, -2.48611804e-01,\n",
       "         0.00000000e+00, -6.62556379e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.21833159e-02,  1.68892768e-01,\n",
       "         0.00000000e+00,  4.31543658e-02, -2.48236803e-01,\n",
       "         0.00000000e+00, -7.73867206e-02, -1.83338240e-02,\n",
       "         1.88998540e-01, -1.47733617e-01, -2.05787047e-01,\n",
       "         1.77706080e-01, -1.25371588e-01, -2.51330139e-02,\n",
       "         0.00000000e+00,  1.86399459e-01,  0.00000000e+00,\n",
       "         9.36459689e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.85222822e-01,  0.00000000e+00,\n",
       "        -4.07168078e-03, -4.85443674e-02, -8.78813253e-02,\n",
       "         2.84088821e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.41587379e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.29793712e-01,  0.00000000e+00, -1.13471142e-01,\n",
       "         2.64343467e-02,  1.07367569e-01,  1.62090193e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.28440595e-01,\n",
       "         1.27525088e-01,  2.45791015e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.77241776e-02,  5.28621102e-03,\n",
       "         0.00000000e+00,  1.10664702e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -7.43725362e-02,\n",
       "         1.48096033e-01,  0.00000000e+00, -1.53127762e-01,\n",
       "         0.00000000e+00, -7.00798609e-02,  1.38777102e-01,\n",
       "         0.00000000e+00,  2.66823332e-01, -1.23974838e-01,\n",
       "        -9.65723325e-03,  2.25937689e-01,  2.10689364e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.97112265e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -6.57379192e-02,\n",
       "        -1.16133338e-01,  0.00000000e+00,  1.43424932e-01,\n",
       "        -1.97168079e-01,  2.14008600e-02,  0.00000000e+00,\n",
       "        -7.28347527e-02, -9.19284970e-02,  0.00000000e+00,\n",
       "         1.08954573e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  4.84097618e-02,\n",
       "        -8.09460844e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.57895709e-01, -1.39874115e-01,\n",
       "         3.14764785e-02,  2.03773974e-01, -1.47838321e-01,\n",
       "        -3.82794034e-01,  0.00000000e+00,  3.44041491e-01,\n",
       "        -3.85133244e-02, -1.50430040e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -6.64665422e-02,\n",
       "         0.00000000e+00, -2.07748658e-01, -9.96594115e-02,\n",
       "         0.00000000e+00, -1.06612776e-01,  7.39630015e-02,\n",
       "        -1.39874115e-01, -3.24008268e-02,  0.00000000e+00,\n",
       "        -2.50012264e-01,  1.63652039e-01, -5.04159974e-02,\n",
       "        -4.64954930e-01,  2.72184034e-01, -1.06922061e-01,\n",
       "         0.00000000e+00, -9.36270524e-03,  3.63327447e-02,\n",
       "         0.00000000e+00,  1.72599244e-01,  0.00000000e+00,\n",
       "        -1.33181008e-01, -1.18991903e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.08363187e-02,\n",
       "         5.60387991e-02,  1.28767301e-01,  0.00000000e+00,\n",
       "         5.53440606e-03,  0.00000000e+00,  1.56644296e-01,\n",
       "         0.00000000e+00,  5.56010851e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -5.40241270e-02, -2.62899280e-01,\n",
       "         0.00000000e+00,  1.98392500e-02,  2.13359776e-01,\n",
       "         3.69357048e-02,  1.34518079e-01,  2.28172108e-02,\n",
       "         1.44294130e-01, -1.24408504e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -8.27223866e-02, -1.60676956e-01,\n",
       "         5.99589797e-02, -7.30039877e-02,  0.00000000e+00,\n",
       "        -1.62829383e-01,  0.00000000e+00,  6.71537301e-02,\n",
       "         0.00000000e+00, -3.93807882e-01,  0.00000000e+00,\n",
       "         2.19512457e-01,  0.00000000e+00,  1.56449183e-01,\n",
       "         5.51517623e-03,  8.74315909e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.71041373e-01,  0.00000000e+00,\n",
       "        -1.58424612e-01, -2.38150674e-01,  0.00000000e+00,\n",
       "        -7.44023900e-02,  1.34925993e-01,  0.00000000e+00,\n",
       "        -2.77244171e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -9.33355788e-02,  8.91062435e-02, -8.13656031e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         7.23570954e-02, -9.59979881e-03,  0.00000000e+00,\n",
       "         3.02663923e-02,  5.87104279e-02,  0.00000000e+00,\n",
       "         8.17948018e-02,  0.00000000e+00, -3.33584509e-01,\n",
       "         1.37268405e-01,  0.00000000e+00, -1.07237870e-01,\n",
       "         0.00000000e+00,  1.52162079e-01, -1.45258297e-01,\n",
       "        -1.71177902e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.37053611e-02,\n",
       "         0.00000000e+00,  1.28525082e-01, -1.19915846e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  6.46289418e-02,\n",
       "        -1.39507362e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.76020044e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  5.91717691e-03,\n",
       "        -4.50431116e-02,  2.23277430e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -5.91657025e-02,  0.00000000e+00,\n",
       "        -2.02307718e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.90867886e-02,  0.00000000e+00,  4.14685884e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  3.25624011e-02,  1.87969778e-02,\n",
       "         1.02525954e-01, -1.31759692e-01,  0.00000000e+00,\n",
       "         0.00000000e+00, -6.48038966e-02,  0.00000000e+00,\n",
       "        -2.58318937e-01,  0.00000000e+00,  9.76713398e-03,\n",
       "         4.27600218e-02, -1.60774081e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -9.28686081e-02,\n",
       "         0.00000000e+00, -8.94682578e-02,  4.16496708e-02,\n",
       "        -7.76384833e-02,  0.00000000e+00,  1.24720722e-01,\n",
       "         8.29698376e-02,  0.00000000e+00, -1.64554088e-02,\n",
       "        -2.10381780e-01, -1.26582538e-01,  0.00000000e+00,\n",
       "         8.45584986e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.10706304e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         4.13059311e-02,  2.17670244e-01,  0.00000000e+00,\n",
       "        -3.51397611e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.93770315e-01,  0.00000000e+00,  4.97623882e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.17941535e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.75001972e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.01930605e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.56469523e-01,  1.44574120e-01,  3.60538701e-02,\n",
       "         0.00000000e+00, -5.31551028e-02,  0.00000000e+00,\n",
       "         4.19721632e-02,  2.78880573e-01, -5.27210853e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.92429801e-02,\n",
       "         0.00000000e+00,  1.50249413e-02,  1.41179221e-01,\n",
       "        -1.06544853e-01, -3.25674571e-02, -3.83052752e-01,\n",
       "         0.00000000e+00, -9.86213441e-02,  5.05066467e-03,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -3.20049672e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.44061621e-02, -2.02328792e-01,\n",
       "         7.31643445e-02,  2.61129011e-02,  5.10381390e-02,\n",
       "         1.81023791e-02, -1.49220881e-02, -4.72688691e-02,\n",
       "         0.00000000e+00,  1.66890732e-01,  3.22103435e-02,\n",
       "        -4.98516018e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.55606305e-01, -2.85842838e-02,  1.85816570e-01,\n",
       "         2.75468364e-01,  9.57075606e-02,  1.79558611e-01,\n",
       "         4.22928136e-02,  0.00000000e+00,  3.36090778e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.17553643e-01, -1.21670953e-01,  1.54001265e-01,\n",
       "         0.00000000e+00,  4.63817988e-02,  0.00000000e+00,\n",
       "         1.57191890e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.23408727e-01, -3.17804660e-01,  1.04068394e-01,\n",
       "         7.59305994e-02,  0.00000000e+00, -2.64314416e-01,\n",
       "        -1.44867234e-01, -1.28422745e-01,  0.00000000e+00,\n",
       "        -3.92371231e-02, -1.55027379e-01,  1.15498241e-01,\n",
       "         1.42404544e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.35837379e-01,  7.25669825e-02,  0.00000000e+00,\n",
       "         8.36781577e-02,  0.00000000e+00,  2.12611919e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.22123816e-01,  0.00000000e+00,  1.48068548e-02,\n",
       "        -2.00730338e-01, -1.25545797e-02,  2.07755973e-01,\n",
       "         2.64334678e-01,  0.00000000e+00, -6.07646322e-02,\n",
       "        -1.69504448e-01,  0.00000000e+00, -9.51406723e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         4.12777965e-01,  1.00983258e-01,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.22373837e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         9.19280843e-02,  2.24340111e-01,  0.00000000e+00,\n",
       "         7.78042067e-02, -8.33127164e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -7.43758402e-02,  4.57030890e-02,\n",
       "         5.37205548e-02,  0.00000000e+00, -2.80475412e-01,\n",
       "         1.13952887e-01,  6.04260533e-02,  1.54141360e-01,\n",
       "        -5.90871887e-02, -1.29979189e-01,  1.63675330e-01,\n",
       "         0.00000000e+00,  3.40804062e-02,  0.00000000e+00,\n",
       "         5.62341928e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.43055796e-01,  6.61465951e-02,  9.35396132e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.22307643e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.18676230e-01,\n",
       "         1.95463590e-01,  4.16108766e-02, -1.45510265e-02,\n",
       "         0.00000000e+00,  3.73506304e-02,  8.33752892e-02,\n",
       "         0.00000000e+00, -8.15986198e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -8.13308392e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.96306402e-01, -5.05182729e-02, -9.77141601e-02,\n",
       "         0.00000000e+00, -9.01470483e-02, -2.06388728e-02,\n",
       "        -3.63589442e-02,  0.00000000e+00, -1.61704799e-02,\n",
       "        -2.24900463e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.41388915e-01,\n",
       "        -2.54490344e-01,  0.00000000e+00, -1.69209638e-01,\n",
       "        -9.39098607e-02, -7.28458761e-02,  2.89497977e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -3.99072836e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.59323060e-01, -9.79859650e-03, -3.19694679e-02,\n",
       "        -1.90163931e-01, -2.51483475e-02, -2.57186541e-01,\n",
       "         1.44360035e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.96839478e-01,  1.46355429e-01,\n",
       "        -5.92640436e-02,  0.00000000e+00,  1.17076038e-01,\n",
       "        -5.41068724e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         5.12302130e-02,  4.99776350e-02,  0.00000000e+00,\n",
       "        -2.95124534e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.60755088e-01, -6.69469220e-02, -2.61451324e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.95930753e-01, -5.88868180e-03,  0.00000000e+00,\n",
       "        -1.55192460e-01,  0.00000000e+00,  1.01318461e-01,\n",
       "        -5.01507584e-02,  0.00000000e+00,  1.98090072e-01,\n",
       "         1.75429170e-01, -2.45656145e-01,  0.00000000e+00,\n",
       "        -1.05793147e-01,  0.00000000e+00, -3.50357869e-02,\n",
       "        -8.01805735e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.57526929e-01,  0.00000000e+00, -3.77505632e-01,\n",
       "         0.00000000e+00,  1.59317176e-01, -2.67611007e-01,\n",
       "         0.00000000e+00,  3.00460479e-02,  0.00000000e+00,\n",
       "        -2.75536261e-02,  1.50073164e-02,  0.00000000e+00,\n",
       "        -1.57664019e-01,  0.00000000e+00,  1.70809374e-01,\n",
       "        -2.14740263e-02]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get model coefficients\n",
    "lr_c.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:12:35.881222Z",
     "start_time": "2020-04-26T03:12:35.878261Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/31029340/\n",
    "#how-to-adjust-scaled-scikit-learn-logicistic-regression-coeffs-to-score-a-non-sc/38836670\n",
    "\n",
    "lr_c_coefficients = np.exp(np.true_divide(lr_c.coef_,  ss.scale_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:12:37.187472Z",
     "start_time": "2020-04-26T03:12:37.173380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 6.93547062e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.81719452e+00,\n",
       "        1.53809285e+00, 1.00000000e+00, 1.00000000e+00, 1.05695567e+00,\n",
       "        2.91916757e+00, 1.00000000e+00, 2.18024866e+00, 3.56855610e+00,\n",
       "        1.00000000e+00, 3.76286095e+00, 1.27378801e+00, 1.25525909e+00,\n",
       "        1.00000000e+00, 3.88786290e-02, 1.49520740e+01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.26884915e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.74325656e+00, 3.16349234e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.10888823e+00, 2.13603830e+00, 1.99226103e+00,\n",
       "        1.00000000e+00, 1.19486555e+00, 1.86953114e-01, 1.76468061e-01,\n",
       "        7.98902842e+00, 1.07036662e+00, 1.00000000e+00, 8.50533697e-01,\n",
       "        6.88992714e-01, 1.00000000e+00, 1.00000000e+00, 2.29347718e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.08823258e+00, 1.00000000e+00,\n",
       "        1.75869576e-01, 1.45640356e-01, 2.20398936e+00, 1.00000000e+00,\n",
       "        3.17466375e+00, 1.00000000e+00, 2.98092824e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.87785070e+00, 1.00000000e+00, 8.48756386e-01,\n",
       "        6.54638325e-01, 1.00000000e+00, 1.00000000e+00, 1.30020239e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.24474045e+00,\n",
       "        1.13101997e+00, 7.22033953e-01, 1.45492933e+00, 6.00890623e-01,\n",
       "        1.00000000e+00, 1.11794561e+00, 8.72547833e-02, 1.00000000e+00,\n",
       "        1.85404044e+00, 1.00000000e+00, 1.00000000e+00, 4.58713590e+00,\n",
       "        1.00824481e+00, 4.96390640e+00, 9.40982989e-01, 8.98026536e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.56187394e-01,\n",
       "        3.30845674e+00, 1.00000000e+00, 1.00000000e+00, 2.52069965e-01,\n",
       "        1.00000000e+00, 8.89697089e-02, 1.79746051e+00, 4.13822180e+00,\n",
       "        6.73036651e-01, 1.51161432e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        4.25085782e-01, 1.78358917e+00, 4.05280684e-01, 1.00000000e+00,\n",
       "        3.43692369e-01, 1.03278442e+00, 1.00000000e+00, 2.81130512e-01,\n",
       "        1.70213074e+00, 1.00000000e+00, 1.00000000e+00, 3.80439772e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.32143863e+00, 1.41478691e+00,\n",
       "        3.61002397e+00, 6.09632996e+00, 1.00000000e+00, 6.17499383e-01,\n",
       "        1.30798894e+00, 1.00000000e+00, 1.97795959e+00, 2.65257507e+00,\n",
       "        7.18660062e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 7.50151750e-01,\n",
       "        1.75856076e-01, 3.57395421e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.06560399e+00, 1.00000000e+00, 1.47945865e+00, 3.32576024e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.43478246e+00,\n",
       "        9.49268948e-01, 1.23644098e+00, 1.00000000e+00, 3.30003534e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 8.69604573e-01,\n",
       "        1.05927503e+00, 6.37436481e-01, 2.15569818e-01, 3.35314091e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.32732662e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.96478849e-01, 1.00000000e+00, 5.52186177e-01,\n",
       "        1.00000000e+00, 2.30732729e+00, 7.83701617e-01, 3.24198042e-01,\n",
       "        1.01428913e+01, 2.53210806e+00, 1.00499069e+00, 1.00000000e+00,\n",
       "        4.74303608e+00, 7.76955977e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.01302631e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        6.10074930e-01, 5.98170163e-01, 1.00000000e+00, 3.41959373e-01,\n",
       "        6.99484553e-01, 1.00000000e+00, 9.41397104e-01, 1.26892512e+00,\n",
       "        1.72143982e+00, 1.00000000e+00, 1.00000000e+00, 4.42947377e-01,\n",
       "        6.42171764e+00, 1.50873182e+00, 6.64771029e-01, 1.95723033e+00,\n",
       "        4.82575205e-01, 1.00000000e+00, 3.97205416e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.98968637e+00, 1.27590627e+00, 1.00000000e+00, 1.10925797e-01,\n",
       "        5.58238292e-01, 8.64365716e+00, 1.00000000e+00, 1.09093003e+00,\n",
       "        7.76978365e-01, 2.58421255e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        5.17571883e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.13734431e+00, 1.45645880e+01,\n",
       "        1.00000000e+00, 1.00000000e+00, 8.34068275e-01, 7.23912185e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.77540244e+00, 8.50356972e-02, 2.24377381e-01,\n",
       "        3.36333573e+01, 4.18091552e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        9.63100540e+00, 1.00000000e+00, 1.00000000e+00, 1.16002177e+00,\n",
       "        6.64738647e+00, 1.00000000e+00, 1.20688611e+00, 1.04101291e+00,\n",
       "        1.00000000e+00, 1.16456896e+01, 1.00000000e+00, 1.50757731e+00,\n",
       "        1.20029206e+00, 2.88269954e-01, 5.98027542e-01, 1.63887242e+00,\n",
       "        1.00000000e+00, 2.38139961e-01, 1.39521499e+00, 1.41763884e+00,\n",
       "        1.00000000e+00, 3.62282275e-01, 3.42301292e+00, 3.27438476e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.05878690e+00, 1.00000000e+00,\n",
       "        2.44596916e+00, 1.00000000e+00, 1.78939994e+00, 1.00000000e+00,\n",
       "        3.26307700e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        9.71988420e-01, 1.34582998e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.16558222e+00, 4.38622431e-01, 1.00000000e+00,\n",
       "        1.22235606e+00, 2.86720540e-01, 1.00000000e+00, 2.37533837e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00444695e+01, 1.68657763e+00, 1.00000000e+00, 9.18445158e-01,\n",
       "        1.28219635e-01, 1.00000000e+00, 1.00000000e+00, 9.23278168e-01,\n",
       "        1.00000000e+00, 3.32514618e-01, 7.44974855e-01, 1.12273119e+00,\n",
       "        1.00000000e+00, 1.44836404e+00, 1.42057756e+00, 8.53126119e-01,\n",
       "        1.00000000e+00, 1.08372983e-01, 1.00000000e+00, 3.80507808e+00,\n",
       "        1.00000000e+00, 1.12861141e+00, 9.77707254e-01, 1.00000000e+00,\n",
       "        1.06109267e+00, 5.89571822e+00, 1.35473231e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.92108119e-01, 1.16926685e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.86465351e+00, 7.15068479e-01, 1.04935587e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.10635797e+00, 1.00000000e+00,\n",
       "        1.79134051e-01, 1.00000000e+00, 6.61859095e-01, 1.02718192e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 8.24274191e-01, 2.65415512e+00,\n",
       "        1.20159492e-01, 1.25793785e+01, 1.00000000e+00, 3.26273145e+00,\n",
       "        1.00000000e+00, 1.25728438e+01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.60982545e-01, 1.00000000e+00, 2.61044799e+00,\n",
       "        6.99079543e-01, 1.00000000e+00, 3.45933711e+00, 3.95456755e-01,\n",
       "        1.00000000e+00, 1.34809066e-01, 3.05367745e-01, 3.01181806e+00,\n",
       "        1.00000000e+00, 1.68744303e+00, 1.00000000e+00, 5.52345716e-01,\n",
       "        2.88208470e+00, 6.00177502e-01, 1.00000000e+00, 1.11305773e+00,\n",
       "        1.00259859e+00, 1.00000000e+00, 1.00000000e+00, 2.30397918e+00,\n",
       "        6.24468022e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 5.18575972e-01, 7.23448041e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.13913236e-02, 2.16749221e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 6.83291982e-01, 2.19909432e+00,\n",
       "        2.59436565e+00, 1.00000000e+00, 5.55043469e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.03953628e-01, 5.12743576e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.03329451e+00, 1.73305141e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.47068311e-01, 2.05617040e-01, 6.52905541e-01,\n",
       "        1.00000000e+00, 6.09370534e-01, 1.00000000e+00, 1.74812688e+00,\n",
       "        2.24414467e+00, 7.26445852e-01, 1.00667943e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.28280777e-01, 3.21927796e-01, 1.00000000e+00,\n",
       "        7.67667606e-01, 1.00000000e+00, 1.00000000e+00, 8.22194455e-01,\n",
       "        9.37307044e+00, 1.00000000e+00, 1.49817851e+00, 1.10753504e-01,\n",
       "        1.00000000e+00, 6.29255769e-01, 9.11626273e-01, 9.40639645e+00,\n",
       "        1.41213756e-01, 1.17339051e-01, 8.22739368e+00, 2.08731243e-01,\n",
       "        8.05204642e-01, 1.00000000e+00, 3.20487055e+00, 1.00000000e+00,\n",
       "        1.32488745e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        2.76347211e-01, 1.00000000e+00, 9.89666707e-01, 7.68650575e-01,\n",
       "        3.69592923e-01, 1.49596346e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        2.49452436e+00, 1.00000000e+00, 1.00000000e+00, 2.14539318e-01,\n",
       "        1.00000000e+00, 3.32647063e-01, 1.34809870e+00, 2.00005079e+00,\n",
       "        1.06844648e+00, 1.00000000e+00, 1.00000000e+00, 5.48406534e+00,\n",
       "        2.60289094e+00, 9.43271656e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.36833779e+00, 1.01947671e+00, 1.00000000e+00, 2.66118479e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 3.72812637e-01,\n",
       "        8.16331132e+00, 1.00000000e+00, 1.31138339e-01, 1.00000000e+00,\n",
       "        3.42053248e-01, 2.07119317e+00, 1.00000000e+00, 2.51487463e+00,\n",
       "        2.11986638e-01, 9.52676591e-01, 5.44588173e+00, 6.16231514e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 7.31654187e-02, 1.00000000e+00,\n",
       "        1.00000000e+00, 4.18061683e-01, 2.84568988e-01, 1.00000000e+00,\n",
       "        4.72149541e+00, 1.28880481e-01, 1.09269220e+00, 1.00000000e+00,\n",
       "        5.90044923e-01, 6.15745418e-01, 1.00000000e+00, 3.10247499e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.90073100e+00, 3.41677252e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.71034786e-01, 1.17513082e-01, 1.16737828e+00,\n",
       "        9.09377849e+00, 1.87414546e-01, 1.57253254e-02, 1.00000000e+00,\n",
       "        6.77521299e+00, 5.79246339e-01, 1.18512059e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 6.96472514e-01, 1.00000000e+00,\n",
       "        2.46402880e-01, 5.16295221e-01, 1.00000000e+00, 6.71081746e-01,\n",
       "        2.31107480e+00, 1.17513082e-01, 7.75523572e-01, 1.00000000e+00,\n",
       "        3.62671957e-02, 1.47289429e+00, 6.78505831e-01, 4.02979861e-03,\n",
       "        5.68923670e+00, 2.97896118e-01, 1.00000000e+00, 9.20356732e-01,\n",
       "        1.16687133e+00, 1.00000000e+00, 1.15541792e+01, 1.00000000e+00,\n",
       "        2.50587053e-01, 8.53966617e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.44151851e+00, 1.42975301e+00, 1.59423425e+00,\n",
       "        1.00000000e+00, 1.02521637e+00, 1.00000000e+00, 1.37690176e+00,\n",
       "        1.00000000e+00, 1.74618725e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.29523871e-01, 1.60659030e-01, 1.00000000e+00, 1.12222497e+00,\n",
       "        4.68988144e+00, 1.31329016e+00, 4.57943407e+00, 1.31074554e+00,\n",
       "        2.39226527e+00, 1.71389419e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 4.89694267e-01,\n",
       "        1.18640721e-01, 1.30312281e+00, 4.01133674e-01, 1.00000000e+00,\n",
       "        3.13507785e-01, 1.00000000e+00, 1.49693110e+00, 1.00000000e+00,\n",
       "        1.59780649e-01, 1.00000000e+00, 9.83157188e+00, 1.00000000e+00,\n",
       "        7.06440899e+00, 1.04505288e+00, 2.01313390e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 4.99864663e+00, 1.00000000e+00, 4.65845759e-01,\n",
       "        4.05765135e-01, 1.00000000e+00, 3.48244967e-01, 4.95366402e+00,\n",
       "        1.00000000e+00, 8.02691488e-02, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.92631653e-01, 2.07935500e+00, 5.04651596e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.67335415e+00, 9.51820287e-01,\n",
       "        1.00000000e+00, 1.20904645e+00, 2.00624424e+00, 1.00000000e+00,\n",
       "        1.69206921e+00, 1.00000000e+00, 2.13081778e-01, 5.55876808e+00,\n",
       "        1.00000000e+00, 5.29262311e-01, 1.00000000e+00, 6.71218260e+00,\n",
       "        5.45894961e-01, 1.17436304e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 8.49985527e-01, 1.00000000e+00,\n",
       "        2.22115361e+00, 3.35881979e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        2.07723538e+00, 1.91195403e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.21283540e+01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.07279176e+00, 5.28028878e-01, 1.15048227e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 4.95309869e-01, 1.00000000e+00,\n",
       "        9.07880663e-02, 1.00000000e+00, 1.00000000e+00, 7.72723230e-01,\n",
       "        1.00000000e+00, 1.59947839e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.30720026e+00, 1.18650863e+00,\n",
       "        2.36026923e+00, 1.54426136e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.24983716e-01, 1.00000000e+00, 8.83018850e-02, 1.00000000e+00,\n",
       "        1.05494253e+00, 1.16040833e+00, 4.05432425e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 4.91240953e-01, 1.00000000e+00,\n",
       "        4.53569808e-01, 1.54378914e+00, 4.93439209e-01, 1.00000000e+00,\n",
       "        6.74793213e+00, 2.29750347e+00, 1.00000000e+00, 8.36519540e-01,\n",
       "        2.12919115e-01, 3.70401900e-01, 1.00000000e+00, 3.07041756e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.32205494e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.88196216e+00, 1.05712292e+01, 1.00000000e+00,\n",
       "        7.02791079e-01, 1.00000000e+00, 1.00000000e+00, 2.39609983e+00,\n",
       "        1.00000000e+00, 2.14205821e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 4.27399237e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.13460984e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 4.34021023e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 4.34960099e+00, 2.95579793e+00, 1.53438080e+00,\n",
       "        1.00000000e+00, 4.94012904e-01, 1.00000000e+00, 1.26353904e+00,\n",
       "        1.49565494e+01, 4.73568017e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.16319493e+00, 1.00000000e+00, 1.22058668e+00, 2.48029486e+00,\n",
       "        3.88393341e-01, 6.91520538e-01, 3.69347922e-02, 1.00000000e+00,\n",
       "        3.27739433e-01, 1.08038346e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.66281502e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.33617962e+00, 9.07653794e-02, 1.57504785e+00,\n",
       "        1.18407375e+00, 2.06184157e+00, 1.22723244e+00, 8.94019805e-01,\n",
       "        5.34137143e-01, 1.00000000e+00, 7.23700072e+00, 1.33000104e+00,\n",
       "        7.06567267e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.85639107e-01, 7.82241007e-01,\n",
       "        8.20363971e+00, 3.41247391e+00, 1.91458164e+00, 1.60498561e+00,\n",
       "        1.82141125e+00, 1.00000000e+00, 1.14426733e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 3.03565091e-01, 3.60035029e-01,\n",
       "        7.71441776e+00, 1.00000000e+00, 1.50715348e+00, 1.00000000e+00,\n",
       "        4.59406701e+00, 1.00000000e+00, 1.00000000e+00, 2.13054401e+00,\n",
       "        3.65515030e-02, 1.53866746e+00, 1.45872314e+00, 1.00000000e+00,\n",
       "        3.50682822e-01, 4.75839459e-01, 4.87076405e-01, 1.00000000e+00,\n",
       "        8.61173400e-01, 1.43736118e-01, 2.46817545e+00, 1.90457970e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.40361937e+00, 1.50426376e+00,\n",
       "        1.00000000e+00, 1.90361723e+00, 1.00000000e+00, 1.42521499e+01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.23358652e+01, 1.00000000e+00,\n",
       "        1.14420823e+00, 2.26088455e-01, 8.25152845e-01, 6.27066055e+00,\n",
       "        3.33431729e+01, 1.00000000e+00, 5.02918339e-01, 9.04308019e-02,\n",
       "        1.00000000e+00, 4.21110292e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.56945306e+00, 3.13008939e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 2.73436585e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 3.15897573e+00, 1.65017969e+01,\n",
       "        1.00000000e+00, 1.54073443e+00, 4.20738774e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.73258163e-01, 1.19431480e+00, 1.08590921e+00,\n",
       "        1.00000000e+00, 1.68980995e-01, 2.55023843e+00, 2.22698653e+00,\n",
       "        2.37831626e+00, 7.07253496e-01, 6.69419781e-01, 4.65427014e+00,\n",
       "        1.00000000e+00, 1.18856799e+00, 1.00000000e+00, 1.84046581e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.37914907e-01, 1.28723594e+00,\n",
       "        1.99265611e+00, 1.00000000e+00, 1.00000000e+00, 3.41300890e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 4.03187631e-01, 1.67483446e+00,\n",
       "        1.54094974e+00, 9.22793102e-01, 1.00000000e+00, 1.64135184e+00,\n",
       "        2.56788750e+00, 1.00000000e+00, 3.97330393e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 4.42466231e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.84103573e-01, 7.96290061e-01,\n",
       "        3.47340515e-01, 1.00000000e+00, 3.43692369e-01, 7.46313012e-01,\n",
       "        5.97211807e-01, 1.00000000e+00, 8.49999568e-01, 8.44361283e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.29777372e-01, 4.36099508e-01, 1.00000000e+00, 1.05942778e-01,\n",
       "        3.76542414e-01, 3.56015344e-01, 2.03748084e+01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 9.06623247e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        6.07699085e+00, 8.84613288e-01, 8.12704754e-01, 3.17926624e-01,\n",
       "        8.09655459e-01, 7.58914305e-02, 5.54007820e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.17025555e+01, 9.39729773e+00,\n",
       "        5.39892517e-01, 1.00000000e+00, 1.91525280e+00, 8.03304820e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.89841011e+00, 1.44538935e+00,\n",
       "        1.00000000e+00, 2.86222134e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        4.05535992e-01, 4.32716857e-01, 4.10576381e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 3.24715120e-01, 9.40527964e-01,\n",
       "        1.00000000e+00, 2.61986921e-01, 1.00000000e+00, 2.04831041e+00,\n",
       "        6.15654961e-01, 1.00000000e+00, 5.80274073e+00, 4.54543318e+00,\n",
       "        1.13316275e-01, 1.00000000e+00, 5.13910184e-01, 1.00000000e+00,\n",
       "        6.59655511e-01, 4.03754943e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        6.54013787e-01, 1.00000000e+00, 1.54425438e-01, 1.00000000e+00,\n",
       "        6.05089621e+00, 1.22786769e-01, 1.00000000e+00, 1.26454391e+00,\n",
       "        1.00000000e+00, 7.08699083e-01, 1.20656643e+00, 1.00000000e+00,\n",
       "        4.00295714e-01, 1.00000000e+00, 3.80055742e+00, 7.37528408e-01]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_c_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:12:46.618154Z",
     "start_time": "2020-04-26T03:12:46.614440Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names_c = np.array(c_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:13:10.296304Z",
     "start_time": "2020-04-26T03:13:10.292662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:13:11.221358Z",
     "start_time": "2020-04-26T03:13:11.217526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_c_coefficients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:13:13.039433Z",
     "start_time": "2020-04-26T03:13:13.036132Z"
    }
   },
   "outputs": [],
   "source": [
    "df_lr_c_coef = pd.DataFrame(lr_c_coefficients[0], index=feature_names_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:14:09.958698Z",
     "start_time": "2020-04-26T03:14:09.952967Z"
    }
   },
   "outputs": [],
   "source": [
    "df_lr_c_coef.to_csv(\"../data/final cvec lasso coefs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model with no custom stopwords for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:29:43.056860Z",
     "start_time": "2020-04-26T03:29:41.967811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the training set reddit posts...\n",
      "Review 100 of 2000.\n",
      "Review 200 of 2000.\n",
      "Review 300 of 2000.\n",
      "Review 400 of 2000.\n",
      "Review 500 of 2000.\n",
      "Review 600 of 2000.\n",
      "Review 700 of 2000.\n",
      "Review 800 of 2000.\n",
      "Review 900 of 2000.\n",
      "Review 1000 of 2000.\n",
      "Review 1100 of 2000.\n",
      "Review 1200 of 2000.\n",
      "Review 1300 of 2000.\n",
      "Review 1400 of 2000.\n",
      "Cleaning and parsing the testing set reddit posts...\n",
      "Review 1500 of 2000.\n",
      "Review 1600 of 2000.\n",
      "Review 1700 of 2000.\n",
      "Review 1800 of 2000.\n",
      "Review 1900 of 2000.\n",
      "Review 2000 of 2000.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold the clean posts.\n",
    "clean_train_posts_orig_lem = []\n",
    "clean_test_posts_orig_lem = []\n",
    "\n",
    "print(\"Cleaning and parsing the training set reddit posts...\")\n",
    "\n",
    "# Instantiate counter.\n",
    "j = 0\n",
    "\n",
    "# For every post in our training set...\n",
    "for train_post in X_train['title_selftext']:\n",
    "    \n",
    "    # Convert post to words, then append to clean_train_posts.\n",
    "    clean_train_posts_orig_lem.append(reddit_to_words_wlem(train_post, stopwords.words(\"english\")))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message.\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {total_posts}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "print(\"Cleaning and parsing the testing set reddit posts...\")\n",
    "\n",
    "# For every post in our testing set...\n",
    "for test_post in X_test['title_selftext']:\n",
    "    \n",
    "    # Convert post to words, then append to clean_train_posts.\n",
    "    clean_test_posts_orig_lem.append(reddit_to_words_wlem(test_post, stopwords.words(\"english\")))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message.\n",
    "    if (j + 1) % 100 == 0:\n",
    "        print(f'Review {j + 1} of {total_posts}.')\n",
    "        \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:29:47.845402Z",
     "start_time": "2020-04-26T03:29:47.782572Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 1000)\n",
      "(600, 1000)\n",
      "['aav', 'ability', 'able', 'absolute', 'absolutely', 'according', 'act', 'actually', 'adam', 'adamant', 'add', 'adding', 'address', 'age', 'agency', 'agent', 'agholor', 'ago', 'agree', 'agreed', 'agreement', 'ahead', 'aiyuk', 'aj', 'alabama', 'aldon', 'allowed', 'almost', 'along', 'already', 'alshon', 'also', 'although', 'always', 'amari', 'amazing', 'among', 'amount', 'anderson', 'andrew', 'another', 'anthony', 'anybody', 'anyone', 'anything', 'anywhere', 'app', 'appreciate', 'april', 'arcega', 'archer', 'arizona', 'around', 'article', 'ask', 'asking', 'athletic', 'attack', 'attempt', 'austin', 'authentic', 'available', 'average', 'away', 'awesome', 'back', 'backup', 'bad', 'ball', 'barnett', 'base', 'based', 'basically', 'baun', 'baylor', 'bear', 'beat', 'became', 'become', 'becomes', 'begin', 'beginning', 'behind', 'believe', 'best', 'better', 'big', 'biggest', 'bill', 'bird', 'birthday', 'bit', 'blake', 'blocker', 'board', 'bored', 'boston', 'boundary', 'bowl', 'box', 'boy', 'brady', 'brandon', 'break', 'breakdown', 'breaking', 'bring', 'bronco', 'brook', 'brought', 'brown', 'bryant', 'bucs', 'burrow', 'byron', 'call', 'called', 'cam', 'came', 'camp', 'cap', 'card', 'career', 'carolina', 'carson', 'case', 'catch', 'cb', 'cba', 'cbs', 'ceedee', 'center', 'chaisson', 'chance', 'change', 'charger', 'chart', 'chase', 'check', 'chief', 'choose', 'chris', 'city', 'cj', 'class', 'claypool', 'clear', 'clemson', 'cleveland', 'clinton', 'clock', 'clowney', 'coach', 'cobb', 'coleman', 'college', 'collins', 'colt', 'com', 'combine', 'come', 'comeback', 'coming', 'comment', 'community', 'comp', 'comparison', 'competitive', 'completely', 'consensus', 'consider', 'continue', 'contract', 'conversation', 'cook', 'cool', 'coop', 'cooper', 'corner', 'cornerback', 'coronavirus', 'could', 'couple', 'course', 'coverage', 'covid', 'cowboy', 'cox', 'crazy', 'create', 'curious', 'current', 'currently', 'cut', 'dak', 'dallas', 'damn', 'damon', 'darius', 'david', 'day', 'de', 'deal', 'decade', 'decided', 'decision', 'deep', 'defender', 'defense', 'defensive', 'definitely', 'delpit', 'demarcus', 'denzel', 'depth', 'desean', 'detail', 'devin', 'dez', 'different', 'discus', 'discussion', 'distancing', 'dix', 'dl', 'doe', 'dolphin', 'done', 'donovan', 'dontari', 'douglas', 'draft', 'drafted', 'drafting', 'drive', 'drop', 'dt', 'due', 'eagle', 'early', 'easy', 'edge', 'edit', 'either', 'elite', 'elliott', 'else', 'end', 'ended', 'enjoy', 'enough', 'epenesa', 'era', 'eric', 'ertz', 'especially', 'espn', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'exciting', 'expect', 'expected', 'experience', 'explosive', 'extended', 'extension', 'eye', 'fa', 'face', 'fact', 'fall', 'family', 'fan', 'far', 'favorite', 'fb', 'feel', 'feeling', 'felt', 'field', 'figured', 'fill', 'film', 'finally', 'find', 'finding', 'fire', 'first', 'fisher', 'fit', 'five', 'florida', 'fly', 'foles', 'football', 'form', 'former', 'forward', 'found', 'four', 'franchise', 'frederick', 'free', 'friday', 'friend', 'front', 'fuck', 'full', 'fully', 'fulton', 'fumble', 'fun', 'future', 'gallup', 'game', 'garafolo', 'gave', 'general', 'gerald', 'gerry', 'get', 'getting', 'giant', 'gift', 'give', 'given', 'giving', 'gm', 'go', 'god', 'goedert', 'going', 'golden', 'gone', 'gonna', 'good', 'google', 'got', 'grade', 'graham', 'grant', 'great', 'greatest', 'green', 'greg', 'gregory', 'group', 'guaranteed', 'guess', 'guy', 'ha', 'haha', 'half', 'hall', 'hamler', 'hand', 'happen', 'happens', 'happy', 'hard', 'harris', 'hate', 'head', 'health', 'healthy', 'hear', 'heard', 'hearing', 'held', 'hell', 'helmet', 'help', 'henderson', 'henry', 'hey', 'higgins', 'high', 'higher', 'highest', 'highlight', 'hill', 'history', 'hit', 'hold', 'hole', 'home', 'hope', 'hopefully', 'hoping', 'hopkins', 'hour', 'howard', 'however', 'howie', 'http', 'huge', 'hurt', 'id', 'idea', 'ideal', 'im', 'imgur', 'important', 'improve', 'include', 'included', 'including', 'incredible', 'injury', 'inside', 'instagram', 'instead', 'interception', 'interest', 'interested', 'interesting', 'interview', 'issue', 'jackson', 'jag', 'jaguar', 'jalen', 'james', 'jarwin', 'jason', 'jeff', 'jefferson', 'jeffery', 'jenkins', 'jeremy', 'jernigan', 'jerry', 'jersey', 'jet', 'jeudy', 'jimmy', 'jj', 'joe', 'johnson', 'jones', 'jordan', 'josh', 'jourdan', 'jr', 'jump', 'justin', 'kai', 'keep', 'keeping', 'kelce', 'kenneth', 'key', 'kid', 'kind', 'kinda', 'kinlaw', 'knack', 'know', 'kristian', 'la', 'lack', 'lamb', 'last', 'late', 'later', 'latest', 'lavon', 'lawrence', 'lb', 'le', 'leader', 'league', 'learn', 'least', 'leave', 'leaving', 'lee', 'left', 'lesean', 'let', 'letter', 'level', 'lewis', 'life', 'like', 'likely', 'limited', 'line', 'linebacker', 'lineman', 'link', 'lion', 'list', 'listening', 'literally', 'little', 'live', 'long', 'look', 'looked', 'looking', 'losing', 'lost', 'lot', 'love', 'low', 'lsu', 'lt', 'luck', 'lucky', 'lve', 'machine', 'maclin', 'madden', 'maddox', 'made', 'make', 'making', 'malcolm', 'man', 'many', 'march', 'marcus', 'market', 'massive', 'match', 'matter', 'max', 'may', 'maybe', 'mccarthy', 'mccoy', 'mckinney', 'mcleod', 'mean', 'medium', 'meeting', 'melo', 'member', 'meme', 'mention', 'message', 'met', 'michael', 'mid', 'middle', 'might', 'mike', 'mil', 'mile', 'mill', 'million', 'mims', 'mind', 'miss', 'missed', 'missing', 'mock', 'moment', 'monday', 'money', 'month', 'moore', 'move', 'moving', 'much', 'multiple', 'murray', 'must', 'name', 'nate', 'nation', 'nd', 'need', 'needed', 'negotiation', 'nelson', 'never', 'new', 'news', 'next', 'nfc', 'nfl', 'ngakoue', 'nick', 'nickell', 'night', 'note', 'nothing', 'number', 'obj', 'obviously', 'offense', 'offensive', 'offer', 'office', 'official', 'offseason', 'ok', 'oklahoma', 'okudah', 'old', 'one', 'open', 'opinion', 'opportunity', 'option', 'order', 'ot', 'outside', 'overall', 'paid', 'panther', 'park', 'part', 'party', 'pas', 'pass', 'passer', 'passing', 'past', 'patrick', 'pay', 'paying', 'people', 'per', 'perfect', 'person', 'personal', 'personally', 'peter', 'philadelphia', 'philly', 'physical', 'pick', 'pierre', 'place', 'plan', 'play', 'played', 'player', 'playing', 'playmaker', 'playoff', 'please', 'plus', 'po', 'podcast', 'poe', 'point', 'poll', 'poor', 'position', 'possible', 'post', 'posted', 'potential', 'potentially', 'power', 'pre', 'prediction', 'prefer', 'prescott', 'present', 'press', 'pretty', 'price', 'pro', 'probably', 'problem', 'process', 'production', 'productive', 'program', 'project', 'projected', 'prospect', 'proven', 'provide', 'pull', 'put', 'qb', 'qbs', 'quarantine', 'quarterback', 'queen', 'question', 'quick', 'quinn', 'quite', 'raider', 'ram', 'ran', 'randall', 'randy', 'rank', 'ranked', 'rapoport', 'rasul', 'rather', 'rating', 'rb', 'rd', 'reach', 'read', 'reagor', 'real', 'really', 'reason', 'rec', 'receive', 'receiver', 'receiving', 'recently', 'record', 'reddit', 'redskin', 'regular', 'released', 'releasing', 'reliable', 'remaining', 'remember', 'removed', 'replace', 'replacement', 'report', 'result', 'retired', 'retires', 'retiring', 'return', 'rex', 'ridgeway', 'right', 'robert', 'robey', 'robinson', 'role', 'romo', 'rookie', 'room', 'roseman', 'roster', 'round', 'rounder', 'route', 'ruggs', 'rule', 'rumor', 'run', 'running', 'rush', 'rusher', 'rushing', 'ryan', 'sack', 'safe', 'safety', 'said', 'salary', 'sander', 'save', 'saw', 'say', 'saying', 'scenario', 'schefter', 'scheme', 'school', 'score', 'scott', 'scouting', 'season', 'second', 'secondary', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'selection', 'sense', 'seriously', 'set', 'sf', 'share', 'sharing', 'shenault', 'shirt', 'short', 'show', 'showed', 'shown', 'side', 'sign', 'signed', 'signing', 'simmons', 'since', 'single', 'sit', 'situation', 'size', 'skill', 'slater', 'slay', 'slot', 'small', 'smith', 'snap', 'social', 'solid', 'someone', 'something', 'sorry', 'source', 'space', 'special', 'speed', 'sport', 'spot', 'spreadsheet', 'st', 'stand', 'star', 'start', 'started', 'starter', 'starting', 'state', 'stats', 'stay', 'step', 'still', 'stint', 'stop', 'story', 'strength', 'strong', 'sub', 'success', 'suck', 'super', 'superbowl', 'sure', 'surprise', 'system', 'table', 'tackle', 'tag', 'take', 'taken', 'taking', 'talent', 'talented', 'talk', 'talking', 'target', 'td', 'te', 'team', 'tee', 'tell', 'term', 'texan', 'texas', 'th', 'thank', 'thanks', 'thing', 'think', 'thinking', 'third', 'thomas', 'though', 'thought', 'thread', 'three', 'throw', 'tier', 'tight', 'time', 'today', 'together', 'told', 'tom', 'ton', 'tonight', 'tony', 'took', 'top', 'total', 'touchdown', 'tough', 'towards', 'trade', 'traded', 'trading', 'training', 'transition', 'travis', 'tried', 'trigger', 'true', 'try', 'trying', 'tua', 'turn', 'turned', 'twitter', 'two', 'type', 'understand', 'uniform', 'upcoming', 'update', 'use', 'used', 'using', 'usp', 'utm', 'value', 'vega', 'versatile', 'veteran', 'via', 'video', 'view', 'viking', 'virtual', 'visit', 'vote', 'wa', 'wait', 'walk', 'wallpaper', 'want', 'wanted', 'ward', 'washington', 'watch', 'watched', 'watching', 'watkins', 'way', 'weapon', 'wear', 'week', 'welcome', 'well', 'went', 'wentz', 'whatever', 'whiteside', 'whole', 'wide', 'williams', 'willing', 'wilson', 'win', 'winfield', 'wing', 'winning', 'wish', 'without', 'witten', 'wondering', 'wood', 'work', 'working', 'world', 'worst', 'worth', 'would', 'wr', 'wrong', 'wrs', 'www', 'xavier', 'xfl', 'yannick', 'yard', 'yeah', 'year', 'yes', 'yesterday', 'yet', 'young', 'youtube', 'yr', 'zach', 'zack', 'zeke', 'zone']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate count vectorizer\n",
    "c_vectorizer = CountVectorizer(max_features=1000, ngram_range=(1,1), min_df=2, \n",
    "                               stop_words=stopwords.words(\"english\"))\n",
    "\n",
    "# Fit count vectorizer & transform\n",
    "train_data_features_orig_c = c_vectorizer.fit_transform(clean_train_posts_orig_lem)\n",
    "\n",
    "test_data_features_orig_c = c_vectorizer.transform(clean_test_posts_orig_lem)\n",
    "\n",
    "print(train_data_features_orig_c.shape)\n",
    "\n",
    "print(test_data_features_orig_c.shape)\n",
    "\n",
    "c_orig_vocab = c_vectorizer.get_feature_names()\n",
    "print(c_orig_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:30:55.804314Z",
     "start_time": "2020-04-26T03:30:02.844520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[248  52]\n",
      " [ 66 234]]\n",
      "Cross-Val Accuracy: 0.8157\n",
      "Train Accuracy: 0.9479\n",
      "Test Accuracy: 0.8033\n",
      "Test Sensitivity: 0.78\n",
      "Test Specificity: 0.8267\n",
      "Test Precision: 0.8182\n",
      "ROC AUC: 0.8958\n"
     ]
    }
   ],
   "source": [
    "# Scale data for use in regularized logistic regression model\n",
    "ss = StandardScaler(with_mean=False)\n",
    "\n",
    "X_train_sc_orig_c = ss.fit_transform(train_data_features_orig_c)\n",
    "X_test_sc_orig_c = ss.transform(test_data_features_orig_c)\n",
    "\n",
    "# Instantiate logistic regression model.\n",
    "lr_orig_c = LogisticRegression(penalty='l1', solver = 'saga', C=0.35, random_state=42, max_iter=5000)\n",
    "\n",
    "# Fit model to training data.\n",
    "lr_orig_c.fit(X_train_sc_orig_c, y_train)\n",
    "\n",
    "# Evaluate model on cross-validated data\n",
    "c_orig_cv_mean = cross_val_score(lr_orig_c, X_train_sc_orig_c, y_train, cv=5).mean()\n",
    "\n",
    "# Evaluate model on training data.\n",
    "train_acc_orig_c = lr_orig_c.score(X_train_sc_orig_c, y_train)\n",
    "\n",
    "# Evaluate model on testing data.\n",
    "test_acc_orig_c = lr_orig_c.score(X_test_sc_orig_c, y_test)\n",
    "\n",
    "# Predict test data\n",
    "y_pred_orig_c = lr_orig_c.predict(X_test_sc_orig_c)\n",
    "\n",
    "# Predict test data probabilities\n",
    "y_probs_orig_c = lr_orig_c.predict_proba(X_test_sc_orig_c)\n",
    "\n",
    "# Create confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_orig_c).ravel()\n",
    "print(confusion_matrix(y_test, y_pred_orig_c))\n",
    "\n",
    "# Compute sensitivity & specificity\n",
    "sensitivity_orig_c = tp / (tp + fn)\n",
    "specificity_orig_c = tn / (tn + fp)\n",
    "\n",
    "# Compute precision\n",
    "precision_orig_c = precision_score(y_test, y_pred_orig_c)\n",
    "\n",
    "# Compute roc_auc score\n",
    "roc_auc_orig_c = roc_auc_score(y_test, y_probs_orig_c[:, 1])\n",
    "\n",
    "# Model score summary\n",
    "print(f'Cross-Val Accuracy:', round(c_orig_cv_mean, 4))\n",
    "print(f'Train Accuracy:', round(train_acc_orig_c, 4))\n",
    "print(f'Test Accuracy:', round(test_acc_orig_c, 4))\n",
    "print(f'Test Sensitivity:', round(sensitivity_orig_c, 4))\n",
    "print(f'Test Specificity:', round(specificity_orig_c, 4))\n",
    "print(f'Test Precision:', round(precision_orig_c, 4))\n",
    "print(f'ROC AUC:', round(roc_auc_orig_c, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:32:12.418051Z",
     "start_time": "2020-04-26T03:32:12.406148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -3.96693588e-02,  0.00000000e+00,  5.57561342e-02,\n",
       "         0.00000000e+00, -2.93448906e-03,  0.00000000e+00,\n",
       "         5.34799590e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         5.91217244e-02, -3.39454783e-01,  0.00000000e+00,\n",
       "         1.73570254e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.70059455e-01,  1.74930764e-01,\n",
       "         0.00000000e+00,  1.92792125e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  6.84116805e-03,\n",
       "        -2.00324821e-01, -2.85722128e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  3.19910746e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.86106980e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.33102625e-01,\n",
       "         5.25150826e-02,  1.40700226e-01,  4.97450238e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.70286550e-03,\n",
       "        -1.88093072e-01,  0.00000000e+00,  1.55439922e-01,\n",
       "         4.73064510e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.46345806e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -9.53230081e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         4.86160304e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.66512896e-01, -2.78364909e-01,  2.46548839e-01,\n",
       "         0.00000000e+00,  1.05372652e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.00371937e-02,\n",
       "        -1.62363864e-01,  3.59269384e-03,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.85662330e-03,  1.43720659e-02,  1.18708069e-01,\n",
       "        -1.04349813e-01,  4.72173564e-02, -1.66703560e-01,\n",
       "        -8.35151759e-02,  6.87646281e-02, -2.84302122e-01,\n",
       "        -5.93992805e-05,  5.65699043e-02,  2.49403484e-03,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.28638724e-01,\n",
       "        -5.76150811e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "        -8.94315566e-02,  1.53668697e-01,  7.17952683e-02,\n",
       "        -9.96326079e-03,  0.00000000e+00, -1.49575070e-01,\n",
       "        -6.01852237e-02,  0.00000000e+00, -1.09056265e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.40596662e-01,\n",
       "         2.93200893e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.35138446e-01,  1.72310112e-02,  0.00000000e+00,\n",
       "         3.70917724e-02, -1.50100321e-01,  4.35452711e-03,\n",
       "         0.00000000e+00,  0.00000000e+00,  7.60477742e-03,\n",
       "         0.00000000e+00, -3.12773692e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.92371304e-03,  1.31672479e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.39049613e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -9.77172976e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.90302952e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         4.79847812e-03,  0.00000000e+00,  4.53621365e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  9.33207493e-02,\n",
       "         0.00000000e+00, -4.18575948e-02,  0.00000000e+00,\n",
       "         1.92489060e-01, -2.00473001e-01, -6.44178201e-02,\n",
       "         6.69213965e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.56167625e-02,\n",
       "        -1.30976001e-01, -2.01602122e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.77906038e-01,  1.28358311e-01,  0.00000000e+00,\n",
       "         1.99556296e-01,  0.00000000e+00,  1.19522746e-01,\n",
       "        -7.75477589e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -6.49513166e-02,\n",
       "         1.44278109e+00, -2.05006377e-01, -6.80306794e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  9.15222925e-03,  1.02763086e+00,\n",
       "         5.44230122e-01,  5.77727985e-02,  0.00000000e+00,\n",
       "         4.16692107e-02,  1.23007497e-01, -1.80651799e-01,\n",
       "         1.87713627e-01,  1.37525456e-01,  0.00000000e+00,\n",
       "        -8.89583530e-02, -2.64585389e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.24387912e-01, -9.00121667e-02, -1.88636815e-01,\n",
       "         1.73284868e-02,  0.00000000e+00,  3.82377647e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.74396537e-03,  9.33207493e-02,  3.72909395e-02,\n",
       "         0.00000000e+00,  1.47436302e-01,  0.00000000e+00,\n",
       "        -1.65802191e-01,  0.00000000e+00, -1.67947496e-01,\n",
       "        -7.10254812e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.37355939e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.92339907e+00, -1.81021585e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -7.95598123e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         5.83129014e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -8.14215389e-02, -2.38131491e-01,\n",
       "        -1.76298354e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.74536622e-01,  0.00000000e+00,\n",
       "         1.44391798e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.23557601e-01, -1.22483684e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -8.94108846e-02,  0.00000000e+00,  4.17854939e-02,\n",
       "         8.86396448e-02,  0.00000000e+00, -3.48342796e-02,\n",
       "         3.19636058e-04,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.67189056e-02, -7.47728929e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.01806555e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.29422636e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.45491539e-01,\n",
       "        -1.17640900e-01,  0.00000000e+00, -3.18766335e-02,\n",
       "         0.00000000e+00, -4.11454117e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -5.63932634e-02,  1.25624860e-02,\n",
       "         0.00000000e+00,  5.86409610e-02, -5.44402834e-03,\n",
       "         0.00000000e+00, -1.75013303e-01, -3.75464064e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -3.00942477e-02,  3.54524698e-01,\n",
       "         0.00000000e+00, -5.06629546e-03,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.71801867e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.19554159e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         5.18065502e-02,  0.00000000e+00, -4.28654881e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -7.67777303e-02,  1.23716561e-01, -1.32557101e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.56587708e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -7.60269222e-02,  0.00000000e+00,  8.82194139e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.87894894e-01,\n",
       "         0.00000000e+00, -2.88338777e-02,  2.90852655e-02,\n",
       "         1.03387072e-01,  0.00000000e+00,  1.58321872e-03,\n",
       "         0.00000000e+00,  0.00000000e+00, -6.07723315e-02,\n",
       "         0.00000000e+00,  5.86458417e-02,  0.00000000e+00,\n",
       "         1.40252121e-02, -7.57823612e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.50395961e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.46005965e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.45617726e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.93894657e-02,  0.00000000e+00,  1.33525513e-01,\n",
       "         0.00000000e+00, -1.55872342e-02,  6.43289269e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.33321951e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -5.20304845e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.69044163e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         5.58856279e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -9.78169103e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  6.25324305e-02, -9.29948073e-03,\n",
       "         1.32209266e-01, -2.99396841e-01,  0.00000000e+00,\n",
       "        -3.13979376e-03,  0.00000000e+00, -2.01696701e-01,\n",
       "         5.51097928e-01, -7.81531791e-02,  0.00000000e+00,\n",
       "        -2.01835230e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.36045709e-01, -1.03305465e-01,  1.57369567e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.42350539e-01,  0.00000000e+00, -7.95718062e-02,\n",
       "         5.07768817e-02, -1.07032192e-01,  1.01622969e-01,\n",
       "        -2.90971239e-01, -2.16034803e-01,  2.10369856e-04,\n",
       "        -1.81720493e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.03476930e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.51378170e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.54380272e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.32183629e-01,  4.04021919e-03, -1.50525077e-01,\n",
       "         4.89728140e-03,  7.60672538e-03,  0.00000000e+00,\n",
       "         6.14981096e-02,  1.50493060e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         9.07337712e-02,  0.00000000e+00, -1.29423748e-01,\n",
       "         9.46138806e-02,  2.21805726e-01,  0.00000000e+00,\n",
       "        -1.79991607e-02,  5.48162557e-02,  8.30389403e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.92476911e-01,\n",
       "         0.00000000e+00, -6.81590223e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.18518972e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.13677189e-01,  0.00000000e+00,\n",
       "         2.76090069e-02,  0.00000000e+00,  3.86282272e-02,\n",
       "        -4.43116882e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "        -6.92204906e-02, -6.97973487e-03,  0.00000000e+00,\n",
       "         1.19983198e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.21199936e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -5.27410416e-02, -1.83964722e-01,  0.00000000e+00,\n",
       "         4.28886284e-01, -2.07808087e-02, -1.08358153e-01,\n",
       "        -6.38984977e-02,  1.31485434e-03,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.99560606e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.20711143e-01, -3.34637671e-02,\n",
       "         0.00000000e+00, -6.17670425e-02,  1.77387041e-02,\n",
       "        -1.22097477e-01,  0.00000000e+00, -2.67445311e-01,\n",
       "         1.48351225e-01,  0.00000000e+00, -4.72291595e-01,\n",
       "         1.22907682e-01,  0.00000000e+00,  1.27434877e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  4.39418288e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.09560077e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.09674868e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.39109936e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.86845325e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        -8.32303028e-02, -6.14410175e-02,  2.62806148e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -2.05881275e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         2.57780461e-02,  0.00000000e+00,  6.45128820e-02,\n",
       "         8.47622162e-03,  0.00000000e+00, -1.81778466e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.29305345e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -4.78153999e-02,  0.00000000e+00, -3.13507110e-02,\n",
       "         0.00000000e+00, -1.47625418e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.66388638e-01,\n",
       "         1.03644368e-01,  0.00000000e+00, -6.60158905e-02,\n",
       "         9.05245551e-02,  0.00000000e+00,  8.47230640e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.01290691e-01,\n",
       "         0.00000000e+00, -4.03270862e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.42448834e-01,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.83280009e-02, -2.42787991e-01,\n",
       "        -4.45220225e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.02232056e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  7.67982973e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         9.51618715e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         6.27117449e-02,  2.55115943e-01,  0.00000000e+00,\n",
       "        -2.10563936e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.58482597e-02,  2.01544109e-01,\n",
       "        -9.31669111e-02, -8.92020435e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.40336321e-02,  0.00000000e+00,\n",
       "         4.33388791e-01, -7.67099263e-02,  0.00000000e+00,\n",
       "         1.79505364e-01, -9.02497457e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -9.08577404e-02,\n",
       "        -6.49475222e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  4.63453907e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.07254739e-01,  0.00000000e+00, -6.95745815e-02,\n",
       "        -8.35303987e-04,  0.00000000e+00,  1.56312028e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  3.08107942e-02,\n",
       "        -5.08937326e-02,  0.00000000e+00,  4.40049163e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.71873920e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -3.04063359e-02,  0.00000000e+00,\n",
       "        -2.15319340e-01,  0.00000000e+00,  4.07351743e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -5.62522632e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.12116797e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.14657865e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.92266275e-02,\n",
       "        -2.29337090e-02,  0.00000000e+00, -1.04290840e-01,\n",
       "        -1.60591036e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.23966128e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.66441596e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.20067979e-02,  0.00000000e+00,\n",
       "         1.60876252e-01, -5.48310782e-03, -3.61140524e-02,\n",
       "        -2.01689879e-02,  0.00000000e+00,  3.45461660e-01,\n",
       "         0.00000000e+00,  1.12873309e-01, -1.28900792e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.34128991e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.29734249e-01,  1.29423796e-01,  0.00000000e+00,\n",
       "        -2.73331247e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.50566533e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.66572714e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.44965701e-01,\n",
       "        -3.61628532e-02,  0.00000000e+00, -3.02115300e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -5.58647764e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.21726212e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.97316392e-02,  0.00000000e+00,  1.32241656e-01,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.38918357e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  4.86064030e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -6.77337808e-02,  0.00000000e+00,\n",
       "        -5.78165552e-02,  7.75324449e-03,  1.81029687e-01,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.74412070e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -1.22436741e-01, -1.10151476e-01,\n",
       "         0.00000000e+00, -2.57440830e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.65119675e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -8.78956606e-02,\n",
       "         1.09484475e-01, -1.68711271e-01,  3.28855787e-03,\n",
       "         1.86191713e-01,  0.00000000e+00, -5.37309221e-02,\n",
       "        -4.01534185e-02, -8.87133891e-04,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.19357746e-01,  3.35762432e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         5.76168287e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         4.32239203e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.81152670e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.47462624e-01,\n",
       "         0.00000000e+00,  1.22105651e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.66890703e-02,\n",
       "        -1.59455233e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  2.78740220e-02,  8.58217385e-02,\n",
       "        -4.01274146e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.21791205e-01, -1.56414233e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.34823203e-01,\n",
       "         0.00000000e+00,  8.72105312e-02,  0.00000000e+00,\n",
       "         0.00000000e+00, -9.73106174e-02,  3.19242665e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  8.07950631e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.72328989e-02,\n",
       "        -3.13525028e-03,  0.00000000e+00,  5.91688263e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.03324881e-01,\n",
       "         1.62309731e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.65966973e-01,\n",
       "        -1.76162309e-01, -1.43987258e-02,  0.00000000e+00,\n",
       "         1.63465473e-01, -3.57923219e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -7.57271831e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -1.52659521e-02, -2.52363532e-01,  0.00000000e+00,\n",
       "        -1.70013680e-01, -1.08621279e-01,  0.00000000e+00,\n",
       "         2.30392525e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        -3.73418282e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.45436129e-01, -3.20701972e-02, -6.90689852e-02,\n",
       "         0.00000000e+00, -2.12959564e-02, -1.40849627e-01,\n",
       "         3.47197508e-02,  0.00000000e+00,  2.65457031e-02,\n",
       "        -1.13386473e-01,  6.34321261e-03, -6.47326508e-02,\n",
       "         0.00000000e+00,  0.00000000e+00, -2.59883373e-02,\n",
       "         0.00000000e+00,  0.00000000e+00,  2.31128264e-02,\n",
       "         1.60497632e-02, -1.40538785e-02, -1.39912255e-01,\n",
       "         9.52422755e-03, -3.53229232e-02, -2.76378549e-02,\n",
       "        -7.63697330e-02, -2.04290698e-01,  6.48012857e-03,\n",
       "        -1.06594126e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00, -2.27142245e-01,  0.00000000e+00,\n",
       "         0.00000000e+00, -5.35709908e-02,  1.01703347e-01,\n",
       "         1.28484417e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "         3.24521849e-01,  1.78425321e-01,  3.68498660e-01,\n",
       "        -9.56226286e-02,  0.00000000e+00, -8.02311650e-02,\n",
       "         0.00000000e+00, -2.01557443e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.87361313e-01,\n",
       "         0.00000000e+00, -1.66737285e-01,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.05607960e-01, -2.67113017e-01,\n",
       "         0.00000000e+00,  1.07369208e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  4.87030277e-02,  0.00000000e+00,\n",
       "        -8.03837970e-02,  0.00000000e+00,  1.32120196e-01,\n",
       "        -2.05726832e-01,  2.56078626e-02,  2.41141720e-01,\n",
       "        -1.81680468e-02]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_orig_c.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:32:17.160384Z",
     "start_time": "2020-04-26T03:32:17.157682Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/31029340/\n",
    "#how-to-adjust-scaled-scikit-learn-logicistic-regression-coeffs-to-score-a-non-sc/38836670\n",
    "\n",
    "orig_coefficients = np.exp(np.true_divide(lr_orig_c.coef_,  ss.scale_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:32:18.264246Z",
     "start_time": "2020-04-26T03:32:18.250624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 5.90797412e-01, 1.00000000e+00,\n",
       "        1.54800786e+00, 1.00000000e+00, 9.77239027e-01, 1.00000000e+00,\n",
       "        1.95094338e+00, 1.00000000e+00, 1.00000000e+00, 1.32217667e+00,\n",
       "        2.64782610e-01, 1.00000000e+00, 6.10827965e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.19091325e-01, 5.78789861e+00,\n",
       "        1.00000000e+00, 2.24399020e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.05912035e+00, 4.62336729e-01, 9.16319371e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 3.46687443e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.02641670e+01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 5.07396751e+00, 1.92917731e+00, 1.92767202e+00,\n",
       "        1.42592851e+00, 1.00000000e+00, 1.00000000e+00, 1.01947403e+00,\n",
       "        1.61917815e-01, 1.00000000e+00, 7.86307121e+00, 1.87311204e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.60229484e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.56445852e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.48777160e+00, 1.00000000e+00, 1.00000000e+00, 3.12459460e-01,\n",
       "        3.07140858e-02, 1.82568723e+00, 1.00000000e+00, 1.97445666e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 7.52706577e-01, 2.37106993e-01,\n",
       "        1.03974232e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.03862530e+00, 1.14959469e+00, 1.97563308e+00,\n",
       "        7.85340637e-01, 1.26907901e+00, 4.37700462e-01, 5.33153840e-01,\n",
       "        2.48999518e+00, 1.17822053e-01, 9.99158214e-01, 1.50643672e+00,\n",
       "        1.03364114e+00, 1.00000000e+00, 1.00000000e+00, 1.81479354e-01,\n",
       "        9.48931891e-01, 1.00000000e+00, 1.00000000e+00, 4.81324509e-01,\n",
       "        3.54031998e+00, 1.42875117e+00, 9.17696861e-01, 1.00000000e+00,\n",
       "        1.97814551e-01, 5.33929361e-01, 1.00000000e+00, 9.19856955e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 6.47352138e-01, 1.25140868e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 3.74485248e+00, 1.20552963e+00,\n",
       "        1.00000000e+00, 1.29516603e+00, 1.96692119e-01, 1.03630054e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.06618359e+00, 1.00000000e+00,\n",
       "        1.82845900e-01, 1.00000000e+00, 1.00000000e+00, 1.00297595e+00,\n",
       "        3.43645973e+00, 1.00000000e+00, 1.00000000e+00, 2.35085308e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 9.29366916e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.28435325e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.04605990e+00, 1.00000000e+00, 1.47738704e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 2.75199796e+00, 1.00000000e+00, 6.97362166e-01,\n",
       "        1.00000000e+00, 8.84764424e+00, 2.41444553e-01, 5.73892615e-01,\n",
       "        1.91389303e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.23020786e+00, 4.38145196e-01, 3.84031459e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.21257287e-01, 3.81242325e+00,\n",
       "        1.00000000e+00, 1.96557509e+00, 1.00000000e+00, 2.45123893e+00,\n",
       "        4.15482812e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 5.42772089e-01, 8.72050919e+00, 9.80847198e-02,\n",
       "        4.91994112e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.06702930e+00, 9.61533718e+00, 2.96340343e+00,\n",
       "        1.69075309e+00, 1.00000000e+00, 1.33077203e+00, 4.30072028e+00,\n",
       "        5.26680727e-01, 3.09927756e+00, 1.33211559e+00, 1.00000000e+00,\n",
       "        6.14722821e-01, 7.59199613e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.21687158e-01, 6.25871287e-01, 2.20995666e-01,\n",
       "        1.24212330e+00, 1.00000000e+00, 1.44209601e+01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.03152453e+00, 2.75199796e+00,\n",
       "        1.41899866e+00, 1.00000000e+00, 4.64193063e+00, 1.00000000e+00,\n",
       "        2.00909747e-01, 1.00000000e+00, 2.25657065e-01, 9.30895698e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 9.15547970e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.06516568e-01, 2.63345911e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 5.04031060e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.93567872e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 5.13902470e-01, 3.40498126e-01, 8.55483280e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 3.12877026e-01,\n",
       "        1.00000000e+00, 2.01548890e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.62009949e+00, 8.57908866e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 3.63726142e-01, 1.00000000e+00,\n",
       "        1.74082001e+00, 3.03163271e+00, 1.00000000e+00, 7.70182601e-01,\n",
       "        1.00087269e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.07220447e+00, 4.28748144e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.59074755e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        5.56787420e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.12439737e-01, 3.81628901e-01, 1.00000000e+00, 7.65669724e-01,\n",
       "        1.00000000e+00, 5.79340737e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        7.75472576e-01, 1.07756715e+00, 1.00000000e+00, 1.94119929e+00,\n",
       "        9.40278584e-01, 1.00000000e+00, 1.25152680e-01, 7.02744993e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.07435137e-01, 2.12461468e+01, 1.00000000e+00, 9.38575579e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 7.69127206e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.10050599e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.91056931e+00, 1.00000000e+00, 7.57137427e-02, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.17915208e-01, 4.69278258e+00, 2.51065445e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 5.46648648e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        4.89062143e-01, 1.00000000e+00, 2.23533125e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 2.14684906e-01, 1.00000000e+00, 6.64449847e-01,\n",
       "        1.24428168e+00, 3.06499658e+00, 1.00000000e+00, 1.02270009e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 6.83896801e-01, 1.00000000e+00,\n",
       "        1.30844420e+00, 1.00000000e+00, 1.12545860e+00, 5.89779071e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.81478114e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        2.18602285e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 2.99864480e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 8.97681054e-01, 1.00000000e+00,\n",
       "        5.30475403e+00, 1.00000000e+00, 8.31027100e-01, 2.00603293e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.05747686e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 5.68691255e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 8.01688729e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.94181284e+00, 1.00000000e+00, 1.00000000e+00, 2.49870777e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.03043256e+00,\n",
       "        9.49944814e-01, 4.19621535e+00, 4.33480329e-01, 1.00000000e+00,\n",
       "        9.77144011e-01, 1.00000000e+00, 1.12732793e-01, 1.09867564e+01,\n",
       "        7.00260778e-01, 1.00000000e+00, 9.22614336e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.98566950e-01, 4.66197692e-01, 1.70837100e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.83132738e-01,\n",
       "        1.00000000e+00, 6.21079087e-01, 1.60906655e+00, 5.82655897e-01,\n",
       "        3.33734985e+00, 1.11773790e-01, 1.05463650e-01, 1.00249795e+00,\n",
       "        1.03222065e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.84278376e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.21908351e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 8.03422503e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.45719736e+00, 1.02642710e+00, 1.52065911e-01, 1.02020433e+00,\n",
       "        1.06413310e+00, 1.00000000e+00, 1.58617400e+00, 3.95140145e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 3.61969331e+00, 1.00000000e+00,\n",
       "        1.79599109e-01, 1.64281743e+00, 2.15249655e+00, 1.00000000e+00,\n",
       "        9.13605259e-01, 1.50862894e+00, 2.04767743e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.78060198e-02, 1.00000000e+00, 4.04846873e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.03236030e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 5.49006080e-01,\n",
       "        1.00000000e+00, 1.33228921e+00, 1.00000000e+00, 1.45454753e+00,\n",
       "        9.39109263e-01, 1.00000000e+00, 1.00000000e+00, 3.99185693e-01,\n",
       "        9.66264886e-01, 1.00000000e+00, 1.18543248e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 4.36096442e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        5.50267609e-01, 1.35929739e-01, 1.00000000e+00, 1.08602195e+01,\n",
       "        7.44812706e-01, 2.15184137e-01, 4.28389176e-01, 1.00956925e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 8.97084074e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 2.25781054e-01, 8.00933671e-01, 1.00000000e+00,\n",
       "        7.93671582e-01, 1.22251525e+00, 3.83668317e-01, 1.00000000e+00,\n",
       "        2.87786661e-02, 1.42052279e+00, 1.00000000e+00, 3.69399565e-03,\n",
       "        2.19256716e+00, 1.00000000e+00, 1.17286863e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.20520088e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.95126699e+01, 1.00000000e+00, 1.00000000e+00, 8.64587703e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.85084471e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.81305877e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        4.20375369e-01, 4.64033505e-01, 1.30145016e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.38852433e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.20528435e+00, 1.00000000e+00, 2.07451020e+00,\n",
       "        1.10574801e+00, 1.00000000e+00, 7.59874748e-02, 1.00000000e+00,\n",
       "        1.00000000e+00, 4.88681655e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 5.30278298e-01, 1.00000000e+00, 6.75518333e-01,\n",
       "        1.00000000e+00, 3.49369857e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 2.89220717e-01, 3.42376245e+00, 1.00000000e+00,\n",
       "        7.85085262e-01, 3.09950167e+00, 1.00000000e+00, 1.96996744e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.59334545e+00, 1.00000000e+00,\n",
       "        8.23286183e-01, 1.00000000e+00, 1.00000000e+00, 1.32710392e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 7.72805094e-01, 3.38563504e-01,\n",
       "        1.47353780e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.18074426e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.11667489e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.81640442e+00, 1.00000000e+00, 1.00000000e+00, 1.49667414e+00,\n",
       "        6.56932253e+00, 1.00000000e+00, 3.76849180e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 8.84475268e-01, 1.24511824e+01,\n",
       "        6.78242429e-01, 3.27542218e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.19579758e-01, 1.00000000e+00, 6.98506974e+00, 4.02634594e-01,\n",
       "        1.00000000e+00, 3.04825415e+00, 4.39950531e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 3.40442519e-01, 4.80045049e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.73383386e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.79819102e-01,\n",
       "        1.00000000e+00, 4.38188688e-01, 9.94910371e-01, 1.00000000e+00,\n",
       "        3.99721726e+00, 1.00000000e+00, 1.00000000e+00, 1.41760139e+00,\n",
       "        5.61902045e-01, 1.00000000e+00, 1.58040351e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.29906309e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 9.13683374e-01, 1.00000000e+00,\n",
       "        1.32258352e-01, 1.00000000e+00, 1.24991047e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 7.42453405e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        8.50139218e-01, 1.00000000e+00, 1.00000000e+00, 3.25382189e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.21259008e+00, 7.50543659e-01,\n",
       "        1.00000000e+00, 4.64495316e-01, 2.83654782e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 8.48071379e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 6.06872922e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.16188434e+00, 1.00000000e+00, 2.06576497e+00, 9.42254571e-01,\n",
       "        7.66799101e-01, 8.40563976e-01, 1.00000000e+00, 1.21772894e+01,\n",
       "        1.00000000e+00, 2.12184088e+00, 8.78771048e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        7.46056973e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        2.65083692e+00, 3.37359978e+00, 1.00000000e+00, 7.22837274e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        4.30811108e+00, 1.00000000e+00, 1.00000000e+00, 3.70089472e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.54146447e+00, 7.25425708e-01,\n",
       "        1.00000000e+00, 7.41533102e-02, 1.00000000e+00, 1.00000000e+00,\n",
       "        5.31578875e-01, 1.00000000e+00, 1.00000000e+00, 5.05428104e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 7.91359140e-01,\n",
       "        1.00000000e+00, 2.35291721e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        7.28355378e-01, 1.00000000e+00, 1.00000000e+00, 1.53778799e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        6.12253265e-01, 1.00000000e+00, 5.34895924e-01, 1.09178510e+00,\n",
       "        2.24035816e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        2.01244748e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        5.11230972e-01, 3.96596465e-01, 1.00000000e+00, 3.23221240e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.44107510e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 6.03750110e-01, 1.95626104e+00,\n",
       "        1.72622397e-01, 1.01371009e+00, 2.52396210e+00, 1.00000000e+00,\n",
       "        8.08143242e-01, 8.13955104e-01, 9.95043223e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 6.42670472e-02, 1.30036953e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.45060150e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.39449432e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.26656937e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.59936326e-01, 1.00000000e+00, 5.05279199e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.23222511e+00, 3.94632448e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.11438247e+00, 1.14072838e+00, 7.85729768e-02,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 2.27479161e+00,\n",
       "        9.52850854e-01, 1.00000000e+00, 1.00000000e+00, 1.98060337e+00,\n",
       "        1.00000000e+00, 2.57550234e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        3.76551420e-01, 1.12959876e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        2.25001092e+00, 1.00000000e+00, 1.00000000e+00, 1.04651706e+00,\n",
       "        9.82836269e-01, 1.00000000e+00, 2.19235432e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.10758388e-01, 5.09924639e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 2.39137607e-01, 4.51882616e-01,\n",
       "        8.55712265e-01, 1.00000000e+00, 2.78519123e+00, 6.02028745e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 5.65731523e-01,\n",
       "        1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 8.87118538e-01,\n",
       "        4.39134575e-01, 1.00000000e+00, 1.04818691e-01, 3.23119502e-01,\n",
       "        1.00000000e+00, 1.10108795e+01, 1.00000000e+00, 1.00000000e+00,\n",
       "        7.11425322e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.00000000e+00, 5.19254954e+00, 6.69464236e-01,\n",
       "        6.38870409e-01, 1.00000000e+00, 8.36271750e-01, 2.43630345e-01,\n",
       "        1.50945983e+00, 1.00000000e+00, 1.03047318e+00, 3.20854234e-01,\n",
       "        1.08249444e+00, 5.10041936e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        9.00145315e-01, 1.00000000e+00, 1.00000000e+00, 1.33535698e+00,\n",
       "        1.12558257e+00, 8.53023870e-01, 5.52630142e-01, 1.11374863e+00,\n",
       "        6.06048582e-01, 9.08770876e-01, 3.84591188e-01, 4.98789079e-01,\n",
       "        1.05461104e+00, 7.67349966e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 2.71447318e-01, 1.00000000e+00, 1.00000000e+00,\n",
       "        6.29789684e-01, 1.41925093e+00, 1.09518846e+00, 1.00000000e+00,\n",
       "        1.00000000e+00, 1.78248168e+01, 4.66450964e+00, 1.43860055e+01,\n",
       "        4.28429654e-01, 1.00000000e+00, 6.03591776e-01, 1.00000000e+00,\n",
       "        7.87145390e-01, 1.00000000e+00, 1.00000000e+00, 1.00000000e+00,\n",
       "        7.34229743e-01, 1.00000000e+00, 4.38199677e-01, 1.00000000e+00,\n",
       "        1.00000000e+00, 3.29800732e+00, 1.23266920e-01, 1.00000000e+00,\n",
       "        1.08749174e+00, 1.00000000e+00, 1.00000000e+00, 1.83931902e+00,\n",
       "        1.00000000e+00, 6.27013594e-01, 1.00000000e+00, 2.80872347e+00,\n",
       "        2.94525326e-01, 1.43772018e+00, 4.86167461e+00, 7.72919993e-01]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:32:23.342138Z",
     "start_time": "2020-04-26T03:32:23.338671Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names_orig = np.array(c_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:32:24.313870Z",
     "start_time": "2020-04-26T03:32:24.310476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:32:25.001339Z",
     "start_time": "2020-04-26T03:32:24.998034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_orig_c.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:32:28.293244Z",
     "start_time": "2020-04-26T03:32:28.290063Z"
    }
   },
   "outputs": [],
   "source": [
    "df_orig_coef = pd.DataFrame(orig_coefficients[0], index=feature_names_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:32:29.074868Z",
     "start_time": "2020-04-26T03:32:29.069152Z"
    }
   },
   "outputs": [],
   "source": [
    "df_orig_coef.to_csv(\"../data/orig cvec lasso coefs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try Tfidf Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use pipeline to tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:40:56.451491Z",
     "start_time": "2020-04-26T03:40:56.447976Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer(stop_words=my_stop_words)),\n",
    "    ('ss', StandardScaler(with_mean=False)),\n",
    "    ('lr', LogisticRegression(penalty='l1', solver='saga', random_state=42, max_iter=3000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:40:57.208436Z",
     "start_time": "2020-04-26T03:40:57.205158Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe_tvec_params = {\n",
    "    'tvec__max_features': [1000, 1100, 1250],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)],\n",
    "    'tvec__min_df': [1, 2],\n",
    "    'lr__C': [0.05, 0.1, 0.15]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:41:24.050512Z",
     "start_time": "2020-04-26T03:40:58.503287Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:   24.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tvec',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=['i', 'me',\n",
       "                                                                    'my',...\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l1',\n",
       "                                                           random_state=42,\n",
       "                                                           solver='saga',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'lr__C': [0.05, 0.1, 0.15],\n",
       "                         'tvec__max_features': [1000, 1100, 1250],\n",
       "                         'tvec__min_df': [1, 2],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate grid search\n",
    "gs = GridSearchCV(pipe_tvec, \n",
    "                  param_grid=pipe_tvec_params, \n",
    "                  cv=5,\n",
    "                  n_jobs=-1,\n",
    "                  verbose=1)\n",
    "\n",
    "# Fit grid search to training data\n",
    "gs.fit(clean_train_posts_lem, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:42:11.775085Z",
     "start_time": "2020-04-26T03:42:11.771246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 0.1,\n",
       " 'tvec__max_features': 1250,\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:42:20.046716Z",
     "start_time": "2020-04-26T03:42:19.998604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score 0.6642857142857144\n",
      "Best train score 0.9207142857142857\n",
      "Best test score 0.655\n"
     ]
    }
   ],
   "source": [
    "# Best score\n",
    "print(f'Best CV score', gs.best_score_)\n",
    "print(f'Best train score', gs.score(clean_train_posts_lem, y_train))\n",
    "print(f'Best test score', gs.score(clean_test_posts_lem, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit model again using selected hyperparameters \n",
    "This output will be easier for interpretation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:42:32.717541Z",
     "start_time": "2020-04-26T03:42:32.712943Z"
    }
   },
   "outputs": [],
   "source": [
    "t_vectorizer = TfidfVectorizer(max_features=1250, ngram_range=(1,1), min_df=2, stop_words=my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:42:34.605918Z",
     "start_time": "2020-04-26T03:42:34.551968Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_features_t = t_vectorizer.fit_transform(clean_train_posts_lem)\n",
    "\n",
    "test_data_features_t = t_vectorizer.transform(clean_test_posts_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:42:35.170979Z",
     "start_time": "2020-04-26T03:42:35.166210Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler(with_mean=False)\n",
    "X_train_sc_t = ss.fit_transform(train_data_features_t)\n",
    "X_test_sc_t = ss.transform(test_data_features_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:42:38.862598Z",
     "start_time": "2020-04-26T03:42:37.520945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=3000,\n",
       "                   multi_class='auto', n_jobs=-1, penalty='l1', random_state=42,\n",
       "                   solver='saga', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_t = LogisticRegression(penalty='l1', \n",
    "                          solver='saga', \n",
    "                          C=.1,  \n",
    "                          random_state=42, \n",
    "                          max_iter=3000,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "lr_t.fit(X_train_sc_t, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:42:50.382429Z",
     "start_time": "2020-04-26T03:42:43.490865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180 120]\n",
      " [ 87 213]]\n",
      "Cross-Val Accuracy: 0.6621\n",
      "Train Accuracy: 0.9207\n",
      "Test Accuracy: 0.655\n",
      "Test Sensitivity: 0.71\n",
      "Test Specificity: 0.6\n",
      "Test Precision: 0.6396\n",
      "ROC AUC: 0.2988\n"
     ]
    }
   ],
   "source": [
    "# Compute cv mean score\n",
    "t_cv_mean = cross_val_score(lr_t, X_train_sc_t, y_train, cv=5).mean()\n",
    "\n",
    "# Compute train Accuracy Score\n",
    "train_acc_t = lr_t.score(X_train_sc_t, y_train)\n",
    "\n",
    "# Compute test Accuracy Score\n",
    "test_acc_t = lr_t.score(X_test_sc_t, y_test)\n",
    "\n",
    "# Compute test predictions\n",
    "y_pred_t = lr_t.predict(X_test_sc_t)\n",
    "\n",
    "# Compute test probabilities\n",
    "y_probs_t = lr_t.predict_proba(X_test_sc_t)\n",
    "\n",
    "# Compute confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_t).ravel()\n",
    "print(confusion_matrix(y_test, y_pred_t))\n",
    "\n",
    "# Compute sensitivity & specificity\n",
    "sensitivity_t = tp / (tp + fn)\n",
    "specificity_t = tn / (tn + fp)\n",
    "\n",
    "# Compute precision\n",
    "precision_t = precision_score(y_test, y_pred_t)\n",
    "\n",
    "# Compute roc_auc_score\n",
    "roc_auc_t = roc_auc_score(y_test, y_probs_t[:,0])\n",
    "\n",
    "# Model score summary\n",
    "print(f'Cross-Val Accuracy:', round(t_cv_mean, 4))\n",
    "print(f'Train Accuracy:', round(train_acc_t, 4))\n",
    "print(f'Test Accuracy:', round(test_acc_t, 4))\n",
    "print(f'Test Sensitivity:', round(sensitivity_t, 4))\n",
    "print(f'Test Specificity:', round(specificity_t, 4))\n",
    "print(f'Test Precision:', round(precision_t, 4))\n",
    "print(f'ROC AUC:', round(roc_auc_t, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Count Vectorized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:03.227983Z",
     "start_time": "2020-04-26T03:44:03.225294Z"
    }
   },
   "outputs": [],
   "source": [
    "m_pipe = Pipeline([\n",
    "    ('mnb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:05.109014Z",
     "start_time": "2020-04-26T03:44:05.106518Z"
    }
   },
   "outputs": [],
   "source": [
    "m_pipe_params = {\n",
    "    'mnb__alpha': [.001, .01, .05, .1, .5, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:06.532162Z",
     "start_time": "2020-04-26T03:44:06.458859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('mnb',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'mnb__alpha': [0.001, 0.01, 0.05, 0.1, 0.5, 1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate grid search\n",
    "gs = GridSearchCV(m_pipe, \n",
    "                  param_grid=m_pipe_params, \n",
    "                  cv=5,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "# Fit grid search to training data\n",
    "gs.fit(train_data_features_c, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:12.271006Z",
     "start_time": "2020-04-26T03:44:12.267462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.1}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:25.680301Z",
     "start_time": "2020-04-26T03:44:25.674744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7885714285714286"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best score\n",
    "print(gs.best_score_)\n",
    "\n",
    "gs.score(train_data_features_c, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:27.261391Z",
     "start_time": "2020-04-26T03:44:27.256881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6033333333333334"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(test_data_features_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:28.834242Z",
     "start_time": "2020-04-26T03:44:28.831752Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb_c = MultinomialNB(alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:31.578501Z",
     "start_time": "2020-04-26T03:44:31.562245Z"
    }
   },
   "outputs": [],
   "source": [
    "mc_cv_mean = cross_val_score(mnb_c, train_data_features_c, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:33.106205Z",
     "start_time": "2020-04-26T03:44:33.101146Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb_c.fit(train_data_features_c, y_train)\n",
    "train_acc_mc = mnb_c.score(train_data_features_c, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:34.237282Z",
     "start_time": "2020-04-26T03:44:34.233980Z"
    }
   },
   "outputs": [],
   "source": [
    "test_acc_mc = mnb_c.score(test_data_features_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:35.571997Z",
     "start_time": "2020-04-26T03:44:35.569192Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_mc = mnb_c.predict(test_data_features_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:36.854316Z",
     "start_time": "2020-04-26T03:44:36.851444Z"
    }
   },
   "outputs": [],
   "source": [
    "y_probs_mc = mnb_c.predict_proba(test_data_features_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:37.565091Z",
     "start_time": "2020-04-26T03:44:37.559195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185 115]\n",
      " [123 177]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_mc).ravel()\n",
    "print(confusion_matrix(y_test, y_pred_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:39.013285Z",
     "start_time": "2020-04-26T03:44:39.006745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute Sensitivity & Specificity\n",
    "sensitivity_mc = tp / (tp + fn)\n",
    "specificity_mc = tn / (tn + fp)\n",
    "\n",
    "# Compute precision\n",
    "precision_mc = precision_score(y_test, y_pred_mc)\n",
    "\n",
    "# Compute ROC AUC score\n",
    "roc_auc_mc = roc_auc_score(y_test, y_probs_mc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:44:39.988962Z",
     "start_time": "2020-04-26T03:44:39.983896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Val Accuracy: 0.68\n",
      "Train Accuracy: 0.7886\n",
      "Test Accuracy: 0.6033\n",
      "Test Sensitivity: 0.59\n",
      "Test Specificity: 0.6167\n",
      "Test Precision: 0.6062\n",
      "ROC AUC: 0.3021\n"
     ]
    }
   ],
   "source": [
    "# Model score summary\n",
    "print(f'Cross-Val Accuracy:', round(mc_cv_mean, 4))\n",
    "print(f'Train Accuracy:', round(train_acc_mc, 4))\n",
    "print(f'Test Accuracy:', round(test_acc_mc, 4))\n",
    "print(f'Test Sensitivity:', round(sensitivity_mc, 4))\n",
    "print(f'Test Specificity:', round(specificity_mc, 4))\n",
    "print(f'Test Precision:', round(precision_mc, 4))\n",
    "print(f'ROC AUC:', round(roc_auc_mc, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit single Decision Tree first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:24.828729Z",
     "start_time": "2020-04-26T03:45:24.826399Z"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:26.006198Z",
     "start_time": "2020-04-26T03:45:26.002081Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(dt, \n",
    "                    param_grid = {'max_depth':[2,3,5,7],\n",
    "                                  'min_samples_split':[5,10,15,20],\n",
    "                                  'min_samples_leaf':[2,3,4,5],\n",
    "                                  'ccp_alpha':[0, 0.001, 0.01, 0.1, 1, 10]},\n",
    "                    cv = 5,\n",
    "                    n_jobs=-1,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:29.437669Z",
     "start_time": "2020-04-26T03:45:28.077302Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1128 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1920 out of 1920 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort='deprecated',\n",
       "                                              random_state=42,\n",
       "                                              splitter='best'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'ccp_alpha': [0, 0.001, 0.01, 0.1, 1, 10],\n",
       "                         'max_depth': [2, 3, 5, 7],\n",
       "                         'min_samples_leaf': [2, 3, 4, 5],\n",
       "                         'min_samples_split': [5, 10, 15, 20]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search over parameter space\n",
    "grid.fit(train_data_features_c, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:34.244695Z",
     "start_time": "2020-04-26T03:45:34.241270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.001,\n",
       " 'max_depth': 7,\n",
       " 'min_samples_leaf': 2,\n",
       " 'min_samples_split': 20}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:39.662058Z",
     "start_time": "2020-04-26T03:45:39.658673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5657142857142857"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:40.748222Z",
     "start_time": "2020-04-26T03:45:40.742958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on training set: 0.5964285714285714\n",
      "Score on testing set: 0.5433333333333333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model.\n",
    "print(f'Score on training set: {grid.score(train_data_features_c, y_train)}')\n",
    "print(f'Score on testing set: {grid.score(test_data_features_c, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:41.741377Z",
     "start_time": "2020-04-26T03:45:41.738395Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_preds = grid.predict(test_data_features_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:42.431263Z",
     "start_time": "2020-04-26T03:45:42.425641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[279  21]\n",
      " [253  47]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, dt_preds).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, dt_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:45:43.589514Z",
     "start_time": "2020-04-26T03:45:43.583072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.1567\n",
      "Specificity: 0.93\n",
      "Precison: 0.6912\n",
      "ROC AUC Score: 0.5433\n"
     ]
    }
   ],
   "source": [
    "dt_sensitivity = tp / (tp + fn)\n",
    "dt_specificity = tn / (tn + fp)\n",
    "dt_roc_auc = roc_auc_score(y_test, dt_preds) \n",
    "print(f'Sensitivity: {round(dt_sensitivity, 4)}')\n",
    "print(f'Specificity: {round(dt_specificity, 4)}')\n",
    "print(f'Precison:', round(precision_score(y_test, dt_preds), 4))\n",
    "print(f'ROC AUC Score: {round(dt_roc_auc, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try Bagged Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:46:31.950564Z",
     "start_time": "2020-04-26T03:46:31.884561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5633333333333334"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate BaggingClassifier.\n",
    "bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(min_samples_split=15,\n",
    "                                                              min_samples_leaf=4,\n",
    "                                                              max_depth = 7,\n",
    "                                                              ccp_alpha=0), \n",
    "                        random_state=42)\n",
    "\n",
    "# Fit BaggingClassifier.\n",
    "bag.fit(train_data_features_c, y_train)\n",
    "\n",
    "# Score BaggingClassifier.\n",
    "bag.score(test_data_features_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:46:34.429971Z",
     "start_time": "2020-04-26T03:46:34.200407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6071428571428571"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(bag, train_data_features_c, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:46:35.699619Z",
     "start_time": "2020-04-26T03:46:35.688212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6414285714285715"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.score(train_data_features_c, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:46:36.893602Z",
     "start_time": "2020-04-26T03:46:36.885678Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_preds = bag.predict(test_data_features_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:46:37.905213Z",
     "start_time": "2020-04-26T03:46:37.899359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[263  37]\n",
      " [225  75]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, bag_preds).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, bag_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:46:39.595326Z",
     "start_time": "2020-04-26T03:46:39.587646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity: 0.25\n",
      "Specificity: 0.8767\n",
      "Precision: 0.6696\n",
      "ROC AUC Score: 0.5633\n"
     ]
    }
   ],
   "source": [
    "bag_sensitivity = tp / (tp + fn)\n",
    "bag_specificity = tn / (tn + fp)\n",
    "bag_roc_auc = roc_auc_score(y_test, bag_preds) \n",
    "print(f'Sensitivity: {round(bag_sensitivity, 4)}')\n",
    "print(f'Specificity: {round(bag_specificity, 4)}')\n",
    "print(f'Precision:', round(precision_score(y_test, bag_preds), 4))\n",
    "print(f'ROC AUC Score: {round(bag_roc_auc, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests and Extra Trees Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:47:27.163107Z",
     "start_time": "2020-04-26T03:47:27.160491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "rf = RandomForestClassifier()\n",
    "et = ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:47:28.828100Z",
     "start_time": "2020-04-26T03:47:28.418200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6742857142857143"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, train_data_features_c, y_train, cv=5, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:47:29.972031Z",
     "start_time": "2020-04-26T03:47:29.520516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6699999999999999"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(et, train_data_features_c, y_train, cv=5, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forests** model performs better on cross-validated data, so let's move forward only with that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:48:22.232692Z",
     "start_time": "2020-04-26T03:47:37.485620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:   44.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0,\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Random Forest Classifier in grid search\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': [None, 'auto'],\n",
    "    'max_depth': [None, 2, 3, 4, 5, 6],\n",
    "    'ccp_alpha': [0, 0.1, 0.5, 1]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid=params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs.fit(train_data_features_c, y_train)\n",
    "print(gs.best_score_) # cross-val score\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:49:01.882196Z",
     "start_time": "2020-04-26T03:49:01.751038Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_train_acc_c = gs.score(train_data_features_c, y_train)\n",
    "rf_test_acc_c = gs.score(test_data_features_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:49:02.758442Z",
     "start_time": "2020-04-26T03:49:02.663713Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_preds = gs.predict(test_data_features_c)\n",
    "rf_probs = gs.predict_proba(test_data_features_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:49:03.428685Z",
     "start_time": "2020-04-26T03:49:03.422856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[194 106]\n",
      " [122 178]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, rf_preds).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, rf_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:49:04.648859Z",
     "start_time": "2020-04-26T03:49:04.638515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF CV Accuracy: 0.675\n",
      "RF Train Accuracy: 0.9771\n",
      "RF Test Accuracy: 0.62\n",
      "Sensitivity: 0.5933\n",
      "Specificity: 0.6467\n",
      "Precision: 0.6268\n",
      "ROC AUC Score: 0.6882\n"
     ]
    }
   ],
   "source": [
    "rf_sensitivity = tp / (tp + fn)\n",
    "rf_specificity = tn / (tn + fp)\n",
    "rf_roc_auc = roc_auc_score(y_test, rf_probs[:, 1]) \n",
    "print(f'RF CV Accuracy:', round(gs.best_score_, 4))\n",
    "print(f'RF Train Accuracy: {round(rf_train_acc_c, 4)}')\n",
    "print(f'RF Test Accuracy: {round(rf_test_acc_c, 4)}')\n",
    "print(f'Sensitivity: {round(rf_sensitivity, 4)}')\n",
    "print(f'Specificity: {round(rf_specificity, 4)}')\n",
    "print(f'Precision:', round(precision_score(y_test, rf_preds), 4))\n",
    "print(f'ROC AUC Score: {round(rf_roc_auc, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test AdaBoost vs. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:49:59.168957Z",
     "start_time": "2020-04-26T03:49:59.166145Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "gbm = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:50:00.351000Z",
     "start_time": "2020-04-26T03:50:00.220812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5900000000000001"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(ada, train_data_features_c, y_train, cv=5, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:50:01.460215Z",
     "start_time": "2020-04-26T03:50:01.242297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6399999999999999"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(gbm, train_data_features_c, y_train, cv=5, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceed with Gradient Boosting using Count Vectorized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T03:58:08.092000Z",
     "start_time": "2020-04-26T03:50:40.056352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 232 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 482 tasks      | elapsed:   43.6s\n",
      "[Parallel(n_jobs=-1)]: Done 832 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1282 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1832 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2482 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2700 out of 2700 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6828571428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 6,\n",
       " 'max_features': None,\n",
       " 'n_estimators': 1000,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to add stochastic element to boosting by use of subsample hyperparameter tuning\n",
    "# This approach randomly subsamples the data at each potential node split\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "params = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'subsample': [.2, .4, .6, .8, 1],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6],\n",
    "    'max_features': [None, 'auto']\n",
    "}\n",
    "gs_gbm = GridSearchCV(gbm, param_grid=params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_gbm.fit(train_data_features_c, y_train)\n",
    "print(gs_gbm.best_score_) # cross-val score\n",
    "gs_gbm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:00:48.118315Z",
     "start_time": "2020-04-26T04:00:48.115733Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm_cv_acc = gs_gbm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:00:48.960873Z",
     "start_time": "2020-04-26T04:00:48.900343Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm_train_acc = gs_gbm.score(train_data_features_c, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:00:49.679837Z",
     "start_time": "2020-04-26T04:00:49.649907Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm_test_acc = gs_gbm.score(test_data_features_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:00:50.425083Z",
     "start_time": "2020-04-26T04:00:50.372631Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm_preds = gs_gbm.predict(test_data_features_c)\n",
    "gbm_probs = gs_gbm.predict_proba(test_data_features_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:00:51.309897Z",
     "start_time": "2020-04-26T04:00:51.303784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200 100]\n",
      " [125 175]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, gbm_preds).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, gbm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:00:52.721724Z",
     "start_time": "2020-04-26T04:00:52.711734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF CV Accuracy: 0.6829\n",
      "RF Train Accuracy: 0.9707\n",
      "RF Test Accuracy: 0.625\n",
      "Sensitivity: 0.5833\n",
      "Specificity: 0.6667\n",
      "Precision: 0.6364\n",
      "ROC AUC Score: 0.7263\n"
     ]
    }
   ],
   "source": [
    "gbm_sensitivity = tp / (tp + fn)\n",
    "gbm_specificity = tn / (tn + fp)\n",
    "gbm_roc_auc = roc_auc_score(y_test, gbm_probs[:, 1]) \n",
    "print(f'RF CV Accuracy:', round(gs_gbm.best_score_, 4))\n",
    "print(f'RF Train Accuracy: {round(gbm_train_acc, 4)}')\n",
    "print(f'RF Test Accuracy: {round(gbm_test_acc, 4)}')\n",
    "print(f'Sensitivity: {round(gbm_sensitivity, 4)}')\n",
    "print(f'Specificity: {round(gbm_specificity, 4)}')\n",
    "print(f'Precision:', round(precision_score(y_test, gbm_preds), 4))\n",
    "print(f'ROC AUC Score: {round(gbm_roc_auc, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T21:35:12.565646Z",
     "start_time": "2020-04-22T21:35:12.562638Z"
    }
   },
   "source": [
    "### Try again with TfidfVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:01:45.022993Z",
     "start_time": "2020-04-26T04:01:44.850129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6142857142857143"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(ada, train_data_features_t, y_train, cv=5, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:01:46.144924Z",
     "start_time": "2020-04-26T04:01:45.777343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6642857142857143"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(gbm, train_data_features_t, y_train, cv=5, n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:12:11.644889Z",
     "start_time": "2020-04-26T04:02:01.272980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 450 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=-1)]: Done 800 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1800 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2450 tasks      | elapsed:  8.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2700 out of 2700 | elapsed: 10.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6799999999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'max_depth': 6,\n",
       " 'max_features': None,\n",
       " 'n_estimators': 1000,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "params = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'subsample': [.2, .4, .6, .8, 1],\n",
    "    'max_depth': [1, 2, 3, 4, 5, 6],\n",
    "    'max_features': [None, 'auto']\n",
    "}\n",
    "gs_gbm = GridSearchCV(gbm, param_grid=params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_gbm.fit(train_data_features_t, y_train)\n",
    "print(gs_gbm.best_score_) # cross-val score\n",
    "gs_gbm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:12:51.453275Z",
     "start_time": "2020-04-26T04:12:51.393385Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm_train_acc_t = gs_gbm.score(train_data_features_t, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:12:52.361881Z",
     "start_time": "2020-04-26T04:12:52.331999Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm_test_acc_t = gs_gbm.score(test_data_features_t, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:12:53.455509Z",
     "start_time": "2020-04-26T04:12:53.404387Z"
    }
   },
   "outputs": [],
   "source": [
    "gbm_preds_t = gs_gbm.predict(test_data_features_t)\n",
    "gbm_probs_t = gs_gbm.predict_proba(test_data_features_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:12:58.510315Z",
     "start_time": "2020-04-26T04:12:58.504397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[206  94]\n",
      " [143 157]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, gbm_preds_t).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, gbm_preds_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:12:59.718318Z",
     "start_time": "2020-04-26T04:12:59.707882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF CV Accuracy: 0.68\n",
      "RF Train Accuracy: 0.9443\n",
      "RF Test Accuracy: 0.605\n",
      "Sensitivity: 0.5233\n",
      "Specificity: 0.6867\n",
      "Precision: 0.6255\n",
      "ROC AUC Score: 0.6751\n"
     ]
    }
   ],
   "source": [
    "gbm_sensitivity_t = tp / (tp + fn)\n",
    "gbm_specificity_t = tn / (tn + fp)\n",
    "gbm_roc_auc_t = roc_auc_score(y_test, gbm_probs_t[:, 1]) \n",
    "print(f'RF CV Accuracy:', round(gs_gbm.best_score_, 4))\n",
    "print(f'RF Train Accuracy: {round(gbm_train_acc_t, 4)}')\n",
    "print(f'RF Test Accuracy: {round(gbm_test_acc_t, 4)}')\n",
    "print(f'Sensitivity: {round(gbm_sensitivity_t, 4)}')\n",
    "print(f'Specificity: {round(gbm_specificity_t, 4)}')\n",
    "print(f'Precision:', round(precision_score(y_test, gbm_preds_t), 4))\n",
    "print(f'ROC AUC Score: {round(gbm_roc_auc_t, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First try with Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:13:43.874063Z",
     "start_time": "2020-04-26T04:13:43.870942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "svc_pipe = Pipeline([\n",
    "    ('ss', StandardScaler(with_mean=False)),\n",
    "    ('svc', SVC(random_state=42, probability=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:14:07.186445Z",
     "start_time": "2020-04-26T04:13:47.274091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   19.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6378571428571429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svc__C': 1.0, 'svc__degree': 2, 'svc__kernel': 'rbf'}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_params = {\n",
    "    'svc__C': np.linspace(0.0001, 1, 20),\n",
    "    'svc__kernel': ['rbf', 'poly'],\n",
    "    'svc__degree': [2, 3]\n",
    "}\n",
    "gs_svc = GridSearchCV(svc_pipe, param_grid=svc_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_svc.fit(train_data_features_c, y_train)\n",
    "print(gs_svc.best_score_) # cross-val score\n",
    "gs_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:17:43.914117Z",
     "start_time": "2020-04-26T04:17:43.755193Z"
    }
   },
   "outputs": [],
   "source": [
    "gs_svc_train_acc = gs_svc.score(train_data_features_c, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:17:44.480260Z",
     "start_time": "2020-04-26T04:17:44.409374Z"
    }
   },
   "outputs": [],
   "source": [
    "gs_svc_test_acc = gs_svc.score(test_data_features_c, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:17:45.292608Z",
     "start_time": "2020-04-26T04:17:45.157596Z"
    }
   },
   "outputs": [],
   "source": [
    "svc_preds = gs_svc.predict(test_data_features_c)\n",
    "svc_probs = gs_svc.predict_proba(test_data_features_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:17:46.182076Z",
     "start_time": "2020-04-26T04:17:46.176137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[145 155]\n",
      " [ 60 240]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, svc_preds).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, svc_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:17:47.277109Z",
     "start_time": "2020-04-26T04:17:47.266614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF CV Accuracy: 0.6379\n",
      "RF Train Accuracy: 0.8407\n",
      "RF Test Accuracy: 0.6417\n",
      "Sensitivity: 0.8\n",
      "Specificity: 0.4833\n",
      "Precision: 0.6076\n",
      "ROC AUC Score: 0.6919\n"
     ]
    }
   ],
   "source": [
    "svc_sensitivity = tp / (tp + fn)\n",
    "svc_specificity = tn / (tn + fp)\n",
    "svc_roc_auc = roc_auc_score(y_test, svc_probs[:, 1]) \n",
    "print(f'RF CV Accuracy:', round(gs_svc.best_score_, 4))\n",
    "print(f'RF Train Accuracy: {round(gs_svc_train_acc, 4)}')\n",
    "print(f'RF Test Accuracy: {round(gs_svc_test_acc, 4)}')\n",
    "print(f'Sensitivity: {round(svc_sensitivity, 4)}')\n",
    "print(f'Specificity: {round(svc_specificity, 4)}')\n",
    "print(f'Precision:', round(precision_score(y_test, svc_preds), 4))\n",
    "print(f'ROC AUC Score: {round(svc_roc_auc, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try with TfidfVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:18:38.747283Z",
     "start_time": "2020-04-26T04:18:38.744150Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "svct_pipe = Pipeline([\n",
    "    ('ss', StandardScaler(with_mean=False)),\n",
    "    ('svc', SVC(random_state=42, probability=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:19:03.371635Z",
     "start_time": "2020-04-26T04:18:40.091807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 400 out of 400 | elapsed:   22.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6364285714285713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svc__C': 1.0, 'svc__degree': 2, 'svc__kernel': 'rbf'}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_t_params = {\n",
    "    'svc__C': np.linspace(0.0001, 1, 20),\n",
    "    'svc__kernel': ['rbf', 'poly'],\n",
    "    'svc__degree': [2, 3]\n",
    "}\n",
    "gs_svc_t = GridSearchCV(svct_pipe, param_grid=svc_t_params, cv=5, n_jobs=-1, verbose=1)\n",
    "gs_svc_t.fit(train_data_features_t, y_train)\n",
    "print(gs_svc_t.best_score_) # cross-val score\n",
    "gs_svc_t.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:21:19.790774Z",
     "start_time": "2020-04-26T04:21:19.624345Z"
    }
   },
   "outputs": [],
   "source": [
    "gs_svc_train_acc_t = gs_svc_t.score(train_data_features_t, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:21:20.441452Z",
     "start_time": "2020-04-26T04:21:20.365730Z"
    }
   },
   "outputs": [],
   "source": [
    "gs_svc_test_acc_t = gs_svc_t.score(test_data_features_t, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:21:21.294085Z",
     "start_time": "2020-04-26T04:21:21.151918Z"
    }
   },
   "outputs": [],
   "source": [
    "svc_preds_t = gs_svc_t.predict(test_data_features_t)\n",
    "svc_probs_t = gs_svc_t.predict_proba(test_data_features_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:21:21.811729Z",
     "start_time": "2020-04-26T04:21:21.805940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[207  93]\n",
      " [128 172]]\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, svc_preds_t).ravel()\n",
    "\n",
    "print(confusion_matrix(y_test, svc_preds_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T04:21:23.345971Z",
     "start_time": "2020-04-26T04:21:23.336245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF CV Accuracy: 0.6364\n",
      "RF Train Accuracy: 0.9421\n",
      "RF Test Accuracy: 0.6317\n",
      "Sensitivity: 0.5733\n",
      "Specificity: 0.69\n",
      "Precision: 0.6491\n",
      "ROC AUC Score: 0.6999\n"
     ]
    }
   ],
   "source": [
    "svc_t_sensitivity = tp / (tp + fn)\n",
    "svc_t_specificity = tn / (tn + fp)\n",
    "svc_t_roc_auc = roc_auc_score(y_test, svc_probs_t[:, 1]) \n",
    "print(f'RF CV Accuracy:', round(gs_svc_t.best_score_, 4))\n",
    "print(f'RF Train Accuracy: {round(gs_svc_train_acc_t, 4)}')\n",
    "print(f'RF Test Accuracy: {round(gs_svc_test_acc_t, 4)}')\n",
    "print(f'Sensitivity: {round(svc_t_sensitivity, 4)}')\n",
    "print(f'Specificity: {round(svc_t_specificity, 4)}')\n",
    "print(f'Precision:', round(precision_score(y_test, svc_preds_t), 4))\n",
    "print(f'ROC AUC Score: {round(svc_t_roc_auc, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Summary Table**\n",
    "<br>Sorted in descending order by ROC AUC Score\n",
    "<br>Numbers in **bold** represent the highest score on a given measure\n",
    "\n",
    "|Stopwords Used|Vectorizer/Classifier|CV Accuracy|Train Accuracy|Test Accuracy|Test Sensitivity|Test Specificity|Test Precision|ROC AUC Score|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|Standard English only|CVEC, Logistic Regression with LASSO|**.8157**|.9479|**.8033**|.7800|.8267|**.8182**|**.8958**|\n",
    "|Standard + custom|CVEC, Gradient Boosting Classifier|.6829|.9707|.6250|.5833|.6670|.6364|.7263|\n",
    "|Standard + custom|CVEC, Logistic Regression with LASSO|.6650|.8957|.6533|.7200|.5867|.6353|.7223|\n",
    "|Standard + custom|TfidfVec, Logistic Regression with LASSO|.6621|.9207|.6550|.7100|.6000|.6396|.7012|\n",
    "|Standard + custom|TfidVec, Support Vector Classifier|.6364|.9421|.6317|.5733|.6900|.6491|.6999|\n",
    "|Standard + custom|CVEC, MultinomialBayes Classifier|.6800|.7886|.6033|.5900|.6167|.6062|.6979|\n",
    "|Standard + custom|CVEC, Support Vector Classifier|.6379|.8407|.6417|**.8000**|.4833|.6076|.6919|\n",
    "|Standard + custom|CVEC, Random Forest Classifier|.6750|**.9771**|.6200|.5933|.6467|.6268|.6882|\n",
    "|Standard + custom|TfidfVec, Gradient Boosting Classifier|.6800|.9443|.6050|.5233|.6867|.6255|.6751|\n",
    "|Standard + custom|CVEC, Bagged Decision Tree Classifier|.6071|.6414|.5633|.2500|.8767|.6696|.5633|\n",
    "|Standard + custom|CVEC, Decision Tree Classifier|.5657|.5964|.5433|.1567|**.9300**|.6912|.5433|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the Executive Summary, classifying two NFL team-based subreddits proved to be a challenging task, even without removing obvious-classifying tokens (such as Cowboys, Eagles, Dallas, Philadelphia, etc.).\n",
    "\n",
    "The reason for this is at least two-fold. For one thing, both subreddits consist of fans of NFL football, and therefore use a common vocabulary or terminology that may not differ enough for highly-accurate classification. And for another, I did not scrape a huge sample of posts, so some challenges may be a result of the relatively small sample size used for the analysis.\n",
    "\n",
    "That said, my best models were able to achieve overall classification accuracy of 66% or better, Sensitivity scores over 70% and ROC AUC scores in the 70s. So the models were decent, though I obviously hoped for better performance. Unfortunately, all of the models showed a relatively large amount of overfitting to the training data.\n",
    "\n",
    "Models using a Gradient Boosting Classifier were able to achieve the highest overall accuracy, though at the expense of lower Sensitivity. Multinomial Naive Bayes models also performed similarly.\n",
    "\n",
    "Overall, the models that seemed best able to balance accuracy and Sensitivity were Logistic Regression Classifiers applying L1 (LASSO) regularization.\n",
    "\n",
    "Differences in word usage were revealed, with strong indicators often being references to various media members who cover one or the other team (such as Justin Melo or Todd Archer). Fans may have been reacting in the form of \"did you see what Todd Archer posted?\", for example. r/cowboys posts were more likely to contain references to other teams (like Tampa, Dolphins, Steelers, San Francisco). Certain vulgarities were more associated with one or the other subreddit as well.\n",
    "\n",
    "**Recommendations**\n",
    "\n",
    "For future research, larger and longer-term data pulls could be attempted to see whether increasing the overall sample size leads to less overfitting and better model performance.\n",
    "\n",
    "Many posts only consisted of title text without more substantial selftext entries, so another approach would be to screen for posts that contain both title and selftext content.\n",
    "\n",
    "Lastly, a more rigorous approach to removing stopwords (such as removing ALL current and former player/coach/GM names), and even local sports reporters, might yield more unique topics of interest to each fan base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "368px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
